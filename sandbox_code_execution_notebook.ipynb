{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c0380c",
   "metadata": {},
   "source": [
    "<!-- VISIBLE TITLE BLOCK -->\n",
    "<h1 style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif;\n",
    "            background-color: #f9fbfbff;\n",
    "            color: #38545f;\n",
    "            border-top: 6px solid #ec2625;\n",
    "            padding: 20px 15px 15px 15px;\">\n",
    "    <span style=\"font-size: 32px; font-weight: bold;\">Ho Chi Minh City Temperature Forecasting</span>\n",
    "    <br>\n",
    "    <span style=\"font-size: 20px; color: #6c8794;\">Group 2 - Analysis & Implementation</span>\n",
    "</h1>\n",
    "\n",
    "- This notebook presents a comprehensive, methodologically rigorous approach to forecasting the daily average temperature in Ho Chi Minh City for the next five days\n",
    "- The process documented here follows the project brief from data sourcing and deep exploratory analysis to the final, interpretable champion model\n",
    "- Every decision is justified with data-driven evidence, prioritizing sound methodology to ensure a robust and reliable outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd1188",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"setup\" style=\"display: none;\">0. Setup & Global Configuration</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"setup-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">0. Setup & Global Configuration</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8c39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "# --- Machine Learning ---\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import MultiTaskLassoCV\n",
    "\n",
    "# --- Visualization & Utilities ---\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# --- Custom Project Modules ---\n",
    "import utils.custom_plotly_style_te as cpte\n",
    "import utils.eda_visualizations as eda\n",
    "import utils.analysis_helpers as ah\n",
    "import utils.evaluations as eval\n",
    "import utils.diagnostics as diag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7b872",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"config\" style=\"display: none;\">0.1. Centralized Pipeline Configuration</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"config-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">0.1. Centralized Pipeline Configuration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"\n",
    "    A centralized configuration class for managing paths, parameters, and flags\n",
    "    throughout the notebook. Reproducibility? Yea. Quick and easy key settings\n",
    "    modifications? Also yea:)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Path & File Configuration ---\n",
    "    INPUT_DATA_PATH: str = \"data/weather_hcm_daily.csv\"\n",
    "    # ARTIFACTS_DIR: str = \"champion_model_artifacts\"\n",
    "    UTILS_DIR: str = \"utils\"\n",
    "\n",
    "    # --- Model Training Control ---\n",
    "    # Set to True to retrain and save models.\n",
    "    # Set to False to load pre-trained models for a faster run.\n",
    "    TRAIN_NEW_MODELS: bool = False\n",
    "\n",
    "    # --- General Model Parameters ---\n",
    "    GLOBAL_RANDOM_SEED: int = 105\n",
    "    HORIZONS: List[int] = [1, 2, 3, 4, 5] # Predict from t+1 to t+5\n",
    "    TARGET_VARIABLE: str = \"temp\"\n",
    "\n",
    "    # --- Data Split Parameters ---\n",
    "    # Using an 85/15 split for a substantial test set. We use TimeSeriesSplit CV so can do without three-way holdout\n",
    "    TEST_SIZE: float = 0.15\n",
    "\n",
    "\n",
    "# Instantiate the config class and create necessary directories\n",
    "config = PipelineConfig()\n",
    "# os.makedirs(config.ARTIFACTS_DIR, exist_ok=True)\n",
    "os.makedirs(config.UTILS_DIR, exist_ok=True)\n",
    "\n",
    "# Instantiate console for rich printing\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b8da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Stylistic & Environment Configuration ---\n",
    "\n",
    "# Suppress warnings for a cleaner notebook presentation.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pio.templates[\"te\"] = cpte.te_style_template\n",
    "pio.templates.default = \"te\"\n",
    "pd.options.plotting.backend = \"plotly\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a00469",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"sourcing\" style=\"display: none;\">Q1. Data Sourcing</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"sourcing-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">Q1. Data Sourcing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a590d55",
   "metadata": {},
   "source": [
    "This section details the methodology used to collect the historical weather data for Ho Chi Minh City, which forms the foundation of this forecasting project.\n",
    "\n",
    "*   **Data Source:** [Visual Crossing Weather History API](https://www.visualcrossing.com/weather-query-builder/Hanoi/us/last15days/), per the project requirements.\n",
    "*   **Location specificity:** It was critical to use the exact location string `\"Hồ Chí Minh city\"` to retrieve data for the entire metropolitan area. Similar queries like \"Ho Chi Minh\" or \"Sai Gon\" pointed to more localized or slightly different coordinates, which could introduce inconsistencies.\n",
    "*   **Time period:** We collected daily weather data spanning from **January 1, 2015, to October 1, 2025**. This provides a substantial ~10-year period, capturing a wide range of seasonal variations and long-term trends.\n",
    "*   **Feature selection:** All available weather metrics were selected, with the logical exception of `snow` and `snowdepth`, which are irrelevant for HCMC's tropical climate.\n",
    "*   **Collection strategy:**\n",
    "    *   **Daily data (3,927 records):** Due to the manageable size, the daily dataset was collected using a manual download approach, requiring two accounts to bypass daily limits.\n",
    "    *   **Hourly data (94,248 records):** For the much larger hourly dataset, manual collection was deemed impractical and error-prone. We developed a robust, automated Python script leveraging the Visual Crossing API.\n",
    "    *   **Automated script features:** \n",
    "        *   Designed for resilience and data integrity\n",
    "        *   Featuring automated API key rotation from a pool of 25+ keys from our group members *(employed some techniques to bypass account creation limit)* and automatic retries\n",
    "        *   Auto-backup of the current central downloaded file before performing actions for safety measures\n",
    "        *   Intelligent batch-based downloading that resumed from the last known timestamp in the central downloaded file, for convenience and maximizing the 1000 records/day/free account API key rate limit\n",
    "        *   Built-in data integrity checks to detect duplicates or malformed records\n",
    "\n",
    "$\\implies$ The automated process ensured the final dataset was both complete and of high quality. The data was ready to use instantly for the daily one, and after 4 days for the hourly dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18f9e2",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"eda\" style=\"display: none;\">Q2. Data Understanding & Exploratory Data Analysis (EDA)</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"eda-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">Q2. Data Understanding & Exploratory Data Analysis (EDA)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da448ee3",
   "metadata": {},
   "source": [
    "We must understand the data, deeply, before we can proceed to engineer more features and starts modeling. This EDA phase serves two primary purposes:\n",
    "1.  **Assess Data Quality:** To identify and understand issues like missing values, outliers, and redundancies that must be addressed during preprocessing.\n",
    "2.  **Uncover Patterns & Justify Decisions:** To discover the underlying patterns, seasonality, and relationships within the data. Every significant feature engineering and modeling choice made later in this notebook will be explicitly justified by a finding from this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54783a3d",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"initial-audit\" style=\"display: none;\">Q2.1. Initial Data Load & Quality Audit</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"initial-audit-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.1. Initial Data Load & Quality Audit</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca580482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- <span style=\"font-weight: bold\">DataFrame Info</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- \u001b[1mDataFrame Info\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3927 entries, 2015-01-01 to 2025-10-01\n",
      "Columns: 37 entries, name to source\n",
      "dtypes: float64(25), int64(2), object(10)\n",
      "memory usage: 3.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--- <span style=\"font-weight: bold\">Descriptive Statistics (Numerical Features)</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--- \u001b[1mDescriptive Statistics \u001b[0m\u001b[1m(\u001b[0m\u001b[1mNumerical Features\u001b[0m\u001b[1m)\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "75%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "753a63ae-39a4-4ee8-966b-4174fea261d7",
       "rows": [
        [
         "latitude",
         "3927.0",
         "10.776",
         "0.0",
         "10.776",
         "10.776",
         "10.776",
         "10.776",
         "10.776"
        ],
        [
         "longitude",
         "3927.0",
         "106.70100000000001",
         "1.4212664438827958e-14",
         "106.701",
         "106.701",
         "106.701",
         "106.701",
         "106.701"
        ],
        [
         "tempmax",
         "3927.0",
         "33.07168321874204",
         "1.7917888202133354",
         "24.6",
         "32.0",
         "33.0",
         "34.0",
         "39.0"
        ],
        [
         "tempmin",
         "3927.0",
         "25.136949325184617",
         "1.5865751627481568",
         "18.0",
         "24.0",
         "25.0",
         "26.0",
         "30.0"
        ],
        [
         "temp",
         "3927.0",
         "28.445836516424748",
         "1.3864482587105051",
         "22.8",
         "27.5",
         "28.4",
         "29.3",
         "33.0"
        ],
        [
         "feelslikemax",
         "3927.0",
         "38.46330532212885",
         "3.2474121636781152",
         "24.6",
         "36.4",
         "38.8",
         "40.6",
         "48.7"
        ],
        [
         "feelslikemin",
         "3927.0",
         "25.83073593073593",
         "2.943036202969969",
         "18.0",
         "24.0",
         "25.0",
         "26.0",
         "40.0"
        ],
        [
         "feelslike",
         "3927.0",
         "31.671759612936082",
         "2.9585323107571155",
         "22.8",
         "29.7",
         "31.4",
         "33.5",
         "42.0"
        ],
        [
         "dew",
         "3927.0",
         "23.5038961038961",
         "2.242134062166682",
         "12.7",
         "22.4",
         "24.2",
         "25.0",
         "27.9"
        ],
        [
         "humidity",
         "3927.0",
         "76.57051184110009",
         "9.681564759761343",
         "49.5",
         "69.7",
         "77.5",
         "84.2",
         "99.4"
        ],
        [
         "precip",
         "3927.0",
         "5.208250572956454",
         "12.379588074195532",
         "0.0",
         "0.1",
         "0.8",
         "5.0",
         "227.2"
        ],
        [
         "precipprob",
         "3927.0",
         "75.98675833969952",
         "42.7218113069105",
         "0.0",
         "100.0",
         "100.0",
         "100.0",
         "100.0"
        ],
        [
         "precipcover",
         "3927.0",
         "10.342480264833206",
         "13.218389246600202",
         "0.0",
         "4.17",
         "4.17",
         "12.5",
         "100.0"
        ],
        [
         "windgust",
         "3927.0",
         "31.897428062133947",
         "10.51263974956739",
         "8.3",
         "24.8",
         "29.9",
         "36.7",
         "216.0"
        ],
        [
         "windspeed",
         "3927.0",
         "19.46221033868093",
         "5.504470281095275",
         "7.6",
         "15.8",
         "18.4",
         "22.3",
         "50.0"
        ],
        [
         "windspeedmax",
         "3927.0",
         "19.46221033868093",
         "5.504470281095275",
         "7.6",
         "15.8",
         "18.4",
         "22.3",
         "50.0"
        ],
        [
         "windspeedmean",
         "3927.0",
         "10.029870129870131",
         "3.4009717466691685",
         "3.2",
         "7.4",
         "9.4",
         "12.3",
         "23.0"
        ],
        [
         "windspeedmin",
         "3927.0",
         "3.253603259485612",
         "2.866894608340745",
         "0.0",
         "1.4",
         "2.3",
         "4.9",
         "15.7"
        ],
        [
         "winddir",
         "3927.0",
         "185.3974789915966",
         "80.71267818646015",
         "0.0",
         "127.75",
         "180.9",
         "251.6",
         "359.7"
        ],
        [
         "sealevelpressure",
         "3927.0",
         "1009.1147695441814",
         "2.094072813721863",
         "1001.5",
         "1007.7",
         "1009.0",
         "1010.5",
         "1016.2"
        ],
        [
         "cloudcover",
         "3927.0",
         "55.5943213649096",
         "13.59861028085836",
         "9.7",
         "46.45",
         "55.0",
         "64.8",
         "100.0"
        ],
        [
         "visibility",
         "3927.0",
         "9.614718614718615",
         "0.7783574774663503",
         "5.4",
         "9.2",
         "9.8",
         "10.2",
         "20.0"
        ],
        [
         "solarradiation",
         "3927.0",
         "213.14766997708176",
         "49.69987529896455",
         "0.0",
         "185.8",
         "221.2",
         "250.0",
         "315.8"
        ],
        [
         "solarenergy",
         "3927.0",
         "18.40348866819455",
         "4.296753332638484",
         "0.0",
         "16.1",
         "19.1",
         "21.6",
         "27.2"
        ],
        [
         "uvindex",
         "3927.0",
         "7.80366692131398",
         "1.4949079128629172",
         "0.0",
         "7.0",
         "8.0",
         "9.0",
         "10.0"
        ],
        [
         "severerisk",
         "1196.0",
         "27.604515050167223",
         "17.37145100851382",
         "5.0",
         "10.0",
         "30.0",
         "30.0",
         "75.0"
        ],
        [
         "moonphase",
         "3927.0",
         "0.48399032340208814",
         "0.28875890365530954",
         "0.0",
         "0.25",
         "0.5",
         "0.75",
         "0.98"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 27
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>10.776000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10.776</td>\n",
       "      <td>10.776</td>\n",
       "      <td>10.776</td>\n",
       "      <td>10.776</td>\n",
       "      <td>10.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>106.701000</td>\n",
       "      <td>1.421266e-14</td>\n",
       "      <td>106.701</td>\n",
       "      <td>106.701</td>\n",
       "      <td>106.701</td>\n",
       "      <td>106.701</td>\n",
       "      <td>106.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempmax</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>33.071683</td>\n",
       "      <td>1.791789e+00</td>\n",
       "      <td>24.600</td>\n",
       "      <td>32.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>39.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempmin</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>25.136949</td>\n",
       "      <td>1.586575e+00</td>\n",
       "      <td>18.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>30.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temp</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>28.445837</td>\n",
       "      <td>1.386448e+00</td>\n",
       "      <td>22.800</td>\n",
       "      <td>27.500</td>\n",
       "      <td>28.400</td>\n",
       "      <td>29.300</td>\n",
       "      <td>33.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelslikemax</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>38.463305</td>\n",
       "      <td>3.247412e+00</td>\n",
       "      <td>24.600</td>\n",
       "      <td>36.400</td>\n",
       "      <td>38.800</td>\n",
       "      <td>40.600</td>\n",
       "      <td>48.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelslikemin</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>25.830736</td>\n",
       "      <td>2.943036e+00</td>\n",
       "      <td>18.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>26.000</td>\n",
       "      <td>40.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feelslike</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>31.671760</td>\n",
       "      <td>2.958532e+00</td>\n",
       "      <td>22.800</td>\n",
       "      <td>29.700</td>\n",
       "      <td>31.400</td>\n",
       "      <td>33.500</td>\n",
       "      <td>42.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dew</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>23.503896</td>\n",
       "      <td>2.242134e+00</td>\n",
       "      <td>12.700</td>\n",
       "      <td>22.400</td>\n",
       "      <td>24.200</td>\n",
       "      <td>25.000</td>\n",
       "      <td>27.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>76.570512</td>\n",
       "      <td>9.681565e+00</td>\n",
       "      <td>49.500</td>\n",
       "      <td>69.700</td>\n",
       "      <td>77.500</td>\n",
       "      <td>84.200</td>\n",
       "      <td>99.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precip</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>5.208251</td>\n",
       "      <td>1.237959e+01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.800</td>\n",
       "      <td>5.000</td>\n",
       "      <td>227.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipprob</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>75.986758</td>\n",
       "      <td>4.272181e+01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipcover</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>10.342480</td>\n",
       "      <td>1.321839e+01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.170</td>\n",
       "      <td>4.170</td>\n",
       "      <td>12.500</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windgust</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>31.897428</td>\n",
       "      <td>1.051264e+01</td>\n",
       "      <td>8.300</td>\n",
       "      <td>24.800</td>\n",
       "      <td>29.900</td>\n",
       "      <td>36.700</td>\n",
       "      <td>216.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeed</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>19.462210</td>\n",
       "      <td>5.504470e+00</td>\n",
       "      <td>7.600</td>\n",
       "      <td>15.800</td>\n",
       "      <td>18.400</td>\n",
       "      <td>22.300</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeedmax</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>19.462210</td>\n",
       "      <td>5.504470e+00</td>\n",
       "      <td>7.600</td>\n",
       "      <td>15.800</td>\n",
       "      <td>18.400</td>\n",
       "      <td>22.300</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeedmean</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>10.029870</td>\n",
       "      <td>3.400972e+00</td>\n",
       "      <td>3.200</td>\n",
       "      <td>7.400</td>\n",
       "      <td>9.400</td>\n",
       "      <td>12.300</td>\n",
       "      <td>23.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>windspeedmin</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>3.253603</td>\n",
       "      <td>2.866895e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.400</td>\n",
       "      <td>2.300</td>\n",
       "      <td>4.900</td>\n",
       "      <td>15.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winddir</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>185.397479</td>\n",
       "      <td>8.071268e+01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>127.750</td>\n",
       "      <td>180.900</td>\n",
       "      <td>251.600</td>\n",
       "      <td>359.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sealevelpressure</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>1009.114770</td>\n",
       "      <td>2.094073e+00</td>\n",
       "      <td>1001.500</td>\n",
       "      <td>1007.700</td>\n",
       "      <td>1009.000</td>\n",
       "      <td>1010.500</td>\n",
       "      <td>1016.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cloudcover</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>55.594321</td>\n",
       "      <td>1.359861e+01</td>\n",
       "      <td>9.700</td>\n",
       "      <td>46.450</td>\n",
       "      <td>55.000</td>\n",
       "      <td>64.800</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visibility</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>9.614719</td>\n",
       "      <td>7.783575e-01</td>\n",
       "      <td>5.400</td>\n",
       "      <td>9.200</td>\n",
       "      <td>9.800</td>\n",
       "      <td>10.200</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solarradiation</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>213.147670</td>\n",
       "      <td>4.969988e+01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>185.800</td>\n",
       "      <td>221.200</td>\n",
       "      <td>250.000</td>\n",
       "      <td>315.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solarenergy</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>18.403489</td>\n",
       "      <td>4.296753e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.100</td>\n",
       "      <td>19.100</td>\n",
       "      <td>21.600</td>\n",
       "      <td>27.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uvindex</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>7.803667</td>\n",
       "      <td>1.494908e+00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severerisk</th>\n",
       "      <td>1196.0</td>\n",
       "      <td>27.604515</td>\n",
       "      <td>1.737145e+01</td>\n",
       "      <td>5.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>75.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moonphase</th>\n",
       "      <td>3927.0</td>\n",
       "      <td>0.483990</td>\n",
       "      <td>2.887589e-01</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count         mean           std       min       25%  \\\n",
       "latitude          3927.0    10.776000  0.000000e+00    10.776    10.776   \n",
       "longitude         3927.0   106.701000  1.421266e-14   106.701   106.701   \n",
       "tempmax           3927.0    33.071683  1.791789e+00    24.600    32.000   \n",
       "tempmin           3927.0    25.136949  1.586575e+00    18.000    24.000   \n",
       "temp              3927.0    28.445837  1.386448e+00    22.800    27.500   \n",
       "feelslikemax      3927.0    38.463305  3.247412e+00    24.600    36.400   \n",
       "feelslikemin      3927.0    25.830736  2.943036e+00    18.000    24.000   \n",
       "feelslike         3927.0    31.671760  2.958532e+00    22.800    29.700   \n",
       "dew               3927.0    23.503896  2.242134e+00    12.700    22.400   \n",
       "humidity          3927.0    76.570512  9.681565e+00    49.500    69.700   \n",
       "precip            3927.0     5.208251  1.237959e+01     0.000     0.100   \n",
       "precipprob        3927.0    75.986758  4.272181e+01     0.000   100.000   \n",
       "precipcover       3927.0    10.342480  1.321839e+01     0.000     4.170   \n",
       "windgust          3927.0    31.897428  1.051264e+01     8.300    24.800   \n",
       "windspeed         3927.0    19.462210  5.504470e+00     7.600    15.800   \n",
       "windspeedmax      3927.0    19.462210  5.504470e+00     7.600    15.800   \n",
       "windspeedmean     3927.0    10.029870  3.400972e+00     3.200     7.400   \n",
       "windspeedmin      3927.0     3.253603  2.866895e+00     0.000     1.400   \n",
       "winddir           3927.0   185.397479  8.071268e+01     0.000   127.750   \n",
       "sealevelpressure  3927.0  1009.114770  2.094073e+00  1001.500  1007.700   \n",
       "cloudcover        3927.0    55.594321  1.359861e+01     9.700    46.450   \n",
       "visibility        3927.0     9.614719  7.783575e-01     5.400     9.200   \n",
       "solarradiation    3927.0   213.147670  4.969988e+01     0.000   185.800   \n",
       "solarenergy       3927.0    18.403489  4.296753e+00     0.000    16.100   \n",
       "uvindex           3927.0     7.803667  1.494908e+00     0.000     7.000   \n",
       "severerisk        1196.0    27.604515  1.737145e+01     5.000    10.000   \n",
       "moonphase         3927.0     0.483990  2.887589e-01     0.000     0.250   \n",
       "\n",
       "                       50%       75%       max  \n",
       "latitude            10.776    10.776    10.776  \n",
       "longitude          106.701   106.701   106.701  \n",
       "tempmax             33.000    34.000    39.000  \n",
       "tempmin             25.000    26.000    30.000  \n",
       "temp                28.400    29.300    33.000  \n",
       "feelslikemax        38.800    40.600    48.700  \n",
       "feelslikemin        25.000    26.000    40.000  \n",
       "feelslike           31.400    33.500    42.000  \n",
       "dew                 24.200    25.000    27.900  \n",
       "humidity            77.500    84.200    99.400  \n",
       "precip               0.800     5.000   227.200  \n",
       "precipprob         100.000   100.000   100.000  \n",
       "precipcover          4.170    12.500   100.000  \n",
       "windgust            29.900    36.700   216.000  \n",
       "windspeed           18.400    22.300    50.000  \n",
       "windspeedmax        18.400    22.300    50.000  \n",
       "windspeedmean        9.400    12.300    23.000  \n",
       "windspeedmin         2.300     4.900    15.700  \n",
       "winddir            180.900   251.600   359.700  \n",
       "sealevelpressure  1009.000  1010.500  1016.200  \n",
       "cloudcover          55.000    64.800   100.000  \n",
       "visibility           9.800    10.200    20.000  \n",
       "solarradiation     221.200   250.000   315.800  \n",
       "solarenergy         19.100    21.600    27.200  \n",
       "uvindex              8.000     9.000    10.000  \n",
       "severerisk          30.000    30.000    75.000  \n",
       "moonphase            0.500     0.750     0.980  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--- <span style=\"font-weight: bold\">Missing Value Percentages</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--- \u001b[1mMissing Value Percentages\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">            Missing Count  Missing Percentage\n",
       "severerisk           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2731</span>               <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.54</span>\n",
       "preciptype            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">924</span>               <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.53</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "            Missing Count  Missing Percentage\n",
       "severerisk           \u001b[1;36m2731\u001b[0m               \u001b[1;36m69.54\u001b[0m\n",
       "preciptype            \u001b[1;36m924\u001b[0m               \u001b[1;36m23.53\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset from the configured path\n",
    "df_raw = pd.read_csv(config.INPUT_DATA_PATH)\n",
    "\n",
    "# Convert 'datetime' column to datetime objects and set as the index\n",
    "df_raw[\"datetime\"] = pd.to_datetime(df_raw[\"datetime\"])\n",
    "df_raw.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "# --- Perform a high-level data audit ---\n",
    "console.print(\"--- [bold]DataFrame Info[/bold] ---\")\n",
    "df_raw.info(verbose=False, memory_usage=\"deep\")\n",
    "\n",
    "console.print(\"\\n--- [bold]Descriptive Statistics (Numerical Features)[/bold] ---\")\n",
    "display(df_raw.describe().T)\n",
    "\n",
    "console.print(\"\\n--- [bold]Missing Value Percentages[/bold] ---\")\n",
    "missing_values = df_raw.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_raw) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Count\": missing_values,\n",
    "    \"Missing Percentage\": missing_percentage,\n",
    "}).sort_values(by=\"Missing Percentage\", ascending=False)\n",
    "\n",
    "console.print(missing_df[missing_df[\"Missing Count\"] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8665a",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"feature-defs\" style=\"display: none;\">Q2.2. Feature Definitions & Domain Context</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"feature-defs-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.2. Feature Definitions & Domain Context</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98cd37",
   "metadata": {},
   "source": [
    "| Feature Group          | Feature Name         | Data Type | Value Range (numerical) \\|<br> Example Values (categorical)                               | Definition & How it may helps predicting target variable `temp`                                                                                                          |\n",
    "| :--------------------- | :------------------- | :-------- | :--------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Identifier/Metadata**| `name`, `address`... | `object`  | `['Hồ Chí Minh city']`                                     | Constant metadata identifying the location. **Action:** Will be dropped.                                                               |\n",
    "|                        | `datetime`           | `object`  | `2015-01-01` to `2025-10-01`                               | The unique timestamp for each daily record. **Action:** Will be converted to the DataFrame index.                                        |\n",
    "| **Target & Temp.**     | `temp`               | `float64` | `22.8` - `33.0` (°C)                                       | **Primary Target Variable.** The average daily temperature.                                                                          |\n",
    "|                        | `tempmax`/`tempmin`  | `float64` | `24.6`-`39.0` / `18.0`-`30.0` (°C)                         | The daily maximum and minimum temperatures. Key drivers of the daily average.                                                        |\n",
    "|                        | `feelslike`...       | `float64` | e.g., `22.8` - `42.0` (°C)                                 | The perceived temperature, accounting for humidity and wind. Highly correlated with `temp`.                                         |\n",
    "| **Atmospheric**        | `humidity`           | `float64` | `49.5` - `99.4` (%)                                        | Relative humidity. Crucial in a tropical climate as high humidity traps heat, especially impacting overnight lows.                   |\n",
    "|                        | `dew`                | `float64` | `12.7` - `27.9` (°C)                                       | Dew Point. The temperature at which air becomes saturated. A direct measure of atmospheric moisture.                                   |\n",
    "|                        | `sealevelpressure`   | `float64` | `1001.5` - `1016.2` (hPa)                                   | Air pressure adjusted to sea level. Changes can indicate shifting weather patterns (e.g., low pressure is associated with storms). |\n",
    "|                        | `cloudcover`         | `float64` | `9.7` - `100.0` (%)                                        | The percentage of the sky covered by clouds. Directly impacts solar radiation reaching the surface.                                  |\n",
    "|                        | `visibility`         | `float64` | `5.4` - `20.0` (km)                                        | The distance at which objects can be clearly seen. Often reduced by rain or high humidity.                                       |\n",
    "| **Precipitation**      | `precip`             | `float64` | `0.0` - `227.2` (mm)                                       | Total daily precipitation. Defines the wet vs. dry seasons, which are central to HCMC's climate.                                   |\n",
    "|                        | `precipprob`         | `int64`   | `0` or `100` (%)                                           | Probability of precipitation. In this dataset, there's only 2 values, can be understood as 0 or 1.                                                      |\n",
    "|                        | `preciptype`         | `object`  | `['rain', nan]`                                            | The type of precipitation. `NaN` values deterministically correspond to `precip = 0`. **Action:** Impute `NaN` with `'none'`.          |\n",
    "| **Wind**               | `windspeed`          | `float64` | `7.6` - `50.0` (km/h)                                      | The maximum sustained wind speed for the day.                                                                                        |\n",
    "|                        | `windgust`           | `float64` | `8.3` - `216.0` (km/h)                                     | The highest instantaneous wind speed recorded. Can indicate storm activity.                                                          |\n",
    "|                        | `winddir`            | `float64` | `0.0` - `359.7` (°)                                        | The direction from which the wind originates. Seasonal wind patterns (monsoons) are key climate drivers.                           |\n",
    "| **Solar & Celestial**  | `solarradiation`     | `float64` | `0.0` - `315.8` (W/m²)                                     | The amount of solar energy reaching the surface. The primary driver of daytime temperature.                                          |\n",
    "|                        | `solarenergy`        | `float64` | `0.0` - `27.2` (MJ/m²)                                     | The total solar energy over the day. Highly correlated with `solarradiation`.                                                        |\n",
    "|                        | `uvindex`            | `int64`   | `0` - `10`                                                 | The strength of ultraviolet radiation. A proxy for sun intensity and lack of cloud cover.                                            |\n",
    "|                        | `sunrise`/`sunset`   | `object`  | `2015-01-01T06:11:22`                                      | Timestamps for sunrise and sunset. **Action:** Can be engineered into a `daylight_hours` feature.                                      |\n",
    "|                        | `moonphase`          | `float64` | `0.0` (new) - `0.98` (waning)                              | The fraction of the moon that is illuminated. May have a subtle, long-term cyclical influence.                                       |\n",
    "| **Text & Categorical** | `conditions`         | `object`  | `['Partially cloudy', 'Rain...']`                          | A high-level summary of the day's weather.                                                                                           |\n",
    "|                        | `description`        | `object`  | `['Partly cloudy throughout...']`                          | A more detailed, human-readable description of the weather. **Action:** Will be processed using NLP techniques.                      |\n",
    "|                        | `icon`               | `object`  | `['partly-cloudy-day', 'rain']`                            | A machine-readable categorical summary of conditions.                                                                                |\n",
    "| **Risk**               | `severerisk`         | `float64` | `5.0` - `75.0`                                             | A numerical rating for the risk of severe weather events. `NaN` indicates no recorded risk. **Action:** Impute `NaN` with `0`.      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f35595",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"key-predictors\" style=\"display: none;\">Q2.3. Deep Dive: Key Predictive Feature Groups</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"key-predictors-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', sans-serif; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.3. Deep Dive: Key Predictive Feature Groups</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70282726",
   "metadata": {},
   "source": [
    "### 1. The Solar Radiation & Cloud Cover Nexus\n",
    "*   **Physical Principle:** Solar radiation is the primary engine of Earth's climate, representing the incoming energy that heats the surface. Cloud cover acts as the primary regulator of this energy, reflecting a significant portion back into space before it can be absorbed.\n",
    "*   **Measurement:**\n",
    "    *   `solarradiation` is measured in Watts per square meter (W/m²), quantifying the instantaneous power of solar energy reaching a given area.\n",
    "    *   `cloudcover` is a percentage, often estimated from satellite imagery or ground-based observations.\n",
    "*   **Predictive Hypothesis:**\n",
    "    *   We hypothesize a **strong positive correlation** between `solarradiation` and `tempmax`. Days with high, direct solar radiation will almost certainly be the hottest.\n",
    "    *   Conversely, we expect a **strong negative correlation** between `cloudcover` and `temp`. Higher cloud cover will suppress daytime heating, leading to cooler average temperatures. This relationship is fundamental to HCMC's climate, where the difference between a clear day in the dry season and an overcast day in the rainy season is pronounced.\n",
    "\n",
    "### 2. The Atmospheric Moisture Duo: Humidity & Dew Point\n",
    "*   **Physical Principle:** These two features quantify the amount of water vapor in the atmosphere, which plays a crucial role in temperature regulation via the greenhouse effect. Water vapor traps outgoing longwave radiation, preventing heat from escaping, especially at night.\n",
    "*   **Measurement:**\n",
    "    *   `humidity` is *relative*—it's the percentage of water vapor in the air compared to the maximum amount it *could* hold at that temperature.\n",
    "    *   `dew` (Dew Point) is an *absolute* measure. It is the temperature at which the air would become 100% saturated. A higher dew point means there is more actual moisture in the air, regardless of the current temperature.\n",
    "*   **Predictive Hypothesis:**\n",
    "    *   In HCMC's consistently warm climate, the **Dew Point will be a more robust predictor than Relative Humidity**.\n",
    "    *   We expect a **strong positive correlation between `dew` and `tempmin` (overnight low)**. High atmospheric moisture (high dew point) acts like a blanket, preventing nighttime temperatures from dropping significantly. This is a key reason why humid nights in HCMC feel so warm.\n",
    "\n",
    "### 3. The Precipitation System: A Cooling & Lagged Effect\n",
    "*   **Physical Principle:** Precipitation has multiple effects on temperature. The act of evaporation from wet surfaces actively cools the air (evaporative cooling). Additionally, the cloud systems that produce rain block solar radiation.\n",
    "*   **Measurement:** `precip` is the total volume of rainfall, measured in millimeters (mm).\n",
    "*   **Predictive Hypothesis:**\n",
    "    *   The immediate effect of `precip` on the same day's `temp` will be **negative**. Rainy days are cooler days.\n",
    "    *   More importantly, we hypothesize a **lagged effect**. A day with heavy rainfall will increase ground moisture, leading to higher humidity and dew points on subsequent days. Therefore, `precip` from a few days prior may positively influence the `tempmin` of the current day. This justifies the creation of lagged and rolling-sum precipitation features.\n",
    "\n",
    "### 4. Wind Dynamics: Direction & Speed\n",
    "*   **Physical Principle:** Wind influences temperature through advection (transporting air masses) and evaporative cooling.\n",
    "*   **Measurement:**\n",
    "    *   `windspeed` (km/h) measures the rate of air movement.\n",
    "    *   `winddir` (degrees) indicates the origin direction of the wind.\n",
    "*   **Predictive Hypothesis:**\n",
    "    *   `windspeed` will have a modest cooling effect, especially on the `feelslike` temperature.\n",
    "    *   **`winddir` is potentially a powerful but complex feature.** In HCMC, wind direction is strongly tied to the two monsoon seasons. The Southwest Monsoon (May-Nov) brings moist, rain-bearing winds from the ocean, while the Northeast Monsoon (Dec-Apr) brings drier, cooler air from the continent. We hypothesize that `winddir` can act as a strong seasonal indicator, but its circular nature (359° is close to 0°) means it must be transformed into cyclical (sine/cosine) features to be used effectively by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d35c0",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"target-analysis\" style=\"display: none;\">Q2.4. Target Variable Analysis: Trends & Seasonality</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"target-analysis-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.4. Target Variable Analysis: Trends & Seasonality</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe2d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_target_timeseries = eda.plot_target_variable_time_series(df=df_raw, target_col=config.TARGET_VARIABLE)\n",
    "# fig_target_timeseries.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddaf929",
   "metadata": {},
   "source": [
    "*   **1. Strong & Predictable Annual Seasonality:**\n",
    "    *   The **30-day rolling mean (blue line)** clearly reveals a consistent, repeating annual cycle of peaks and troughs. This strong seasonal pattern is the most dominant signal in the data.\n",
    "    *   This observation confirms our hypothesis that time-based features are essential. The model must be given information about the time of year to capture this cycle.\n",
    "\n",
    "*   **2. Evidence of a Long-Term Upward Trend:**\n",
    "    *   The **365-day rolling mean (red line)** filters out the seasonal fluctuations, exposing a subtle but persistent upward drift in the baseline temperature over the 10-year period.\n",
    "    *   This trend, likely reflecting broader climate change patterns, indicates that the data is **non-stationary**. A simple model that only learns seasonality would degrade over time. Our model must also account for this long-term evolution.\n",
    "\n",
    "*   **3. Fluctuating Volatility (Noise):**\n",
    "    *   The lower panel, showing the rolling standard deviation, quantifies the data's volatility. The temperature is not equally predictable year-round.\n",
    "    *   We can observe periods of higher volatility (wider swings in temperature), particularly during the transitional periods between the dry and wet seasons. This suggests that the model's accuracy may naturally vary depending on the time of year.\n",
    "\n",
    "$\\implies$ Based on these three distinct components (seasonality, trend, noise), we establish the following non-negotiable feature engineering requirements:\n",
    "\n",
    "*   **To capture seasonality:** We will engineer **cyclical features** (e.g., sine/cosine transformations of `day_of_year` and `month`). These allow the model to understand the cyclical nature of time, where December is \"close\" to January.\n",
    "*   **To capture trend:** We will include a **`year` feature** or other trend-aware components to allow the model to account for the gradual increase in baseline temperature over time.\n",
    "*   **To capture recent history & noise:** We will create **lagged features** (e.g., temperature from the previous day) and **rolling window features** (e.g., the average temperature over the last 7 days). These provide the model with a \"memory\" of the recent system state, helping it to predict short-term fluctuations around the seasonal and trend lines.\n",
    "\n",
    "Next, we will perform a deeper dive into the seasonal component to better understand the specific climate dynamics of Ho Chi Minh City throughout the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f18ef",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"seasonal-deep-dive\" style=\"display: none;\">Q2.5. Seasonal Pattern Deep Dive: The Monsoon Cycle</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"seasonal-deep-dive-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.5. Seasonal Pattern Deep Dive: The Monsoon Cycle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc44fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_monthly_seasonality = eda.plot_monthly_seasonality(df=df_raw, target_col=config.TARGET_VARIABLE)\n",
    "# fig_monthly_seasonality.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6faab1",
   "metadata": {},
   "source": [
    "*   **Primary Peak (Pre-monsoon Heat):** The hottest period of the year occurs in **April and May**. This is not mid-summer but rather the end of the dry season, a period characterized by intense, building heat and humidity just before the onset of the rainy Southwest Monsoon.\n",
    "\n",
    "*   **Mid-year Dip (Monsoon Cooling):** A noticeable dip in the median temperature and a compression of the temperature range occurs from **June through August**. This corresponds to the peak of the rainy season. The increased cloud cover and frequent rainfall have a significant cooling effect, suppressing daytime maximum temperatures.\n",
    "\n",
    "*   **Coolest Period (Dry Season):** The coolest months are **December and January**. This coincides with the Northeast Monsoon, which brings comparatively drier and cooler air masses to the region.\n",
    "\n",
    "*   **Outlier Confirmation:** The plot also provides clear visual confirmation of low-temperature outliers, particularly in the cooler months of December and January. These represent unusual cold snaps that a robust model must be able to handle without its feature scaling being skewed.\n",
    "\n",
    "$\\implies$ **Feature Engineering:**\n",
    "1.  This complex, non-sinusoidal pattern suggests creating **cyclical features** (`sin_day_of_year`, `cos_day_of_year`). A simple numerical `month` feature would be insufficient, as it cannot learn that May is climatically closer to June than it is to December.\n",
    "2.  The presence of outliers provides further, conclusive evidence for our decision to use `RobustScaler` during preprocessing.\n",
    "\n",
    "To mathematically isolate these components, we will now perform an STL (Seasonal-Trend-Loess) decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107fb88",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"stl-decomposition\" style=\"display: none;\">Q2.6. STL Decomposition: Isolating Key Signals</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"stl-decomposition-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.6. STL Decomposition: Isolating Key Signals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4c5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_stl = eda.plot_stl_decomposition(df=df_raw, target_col=config.TARGET_VARIABLE)\n",
    "# fig_stl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6ce3b",
   "metadata": {},
   "source": [
    "The STL decomposition provides a mathematical separation of the time series, allowing us to analyze each component in isolation. We use an **additive model** (`Observed = Trend + Seasonal + Residual`) because the magnitude of the seasonal swings does not appear to change proportionally with the long-term trend; the annual temperature variation is relatively constant.\n",
    "\n",
    "*   **Trend Component:**\n",
    "    *   This panel confirms the non-stationary nature of the data with a clear, non-linear trend.\n",
    "    *   The baseline temperature shows a distinct rise and fall over multi-year periods, with a notable acceleration in the upward trend from mid-2023 onwards.\n",
    "    *   **Modeling Implication:** This complex trend confirms that a simple linear `year` feature will be insufficient. Our model must be flexible enough to capture this dynamic baseline, reinforcing the value of tree-based models and rich temporal features.\n",
    "\n",
    "*   **Seasonal Component:**\n",
    "    *   This successfully isolates the strong, repeating annual pattern, which is the dominant driver of temperature variation.\n",
    "    *   As observed, the pattern is highly consistent through the middle of the dataset (approx. 2016-2024). The slight variations in amplitude at the very beginning and end of the series could be attributed to specific climate phenomena in those years or edge effects of the decomposition algorithm.\n",
    "    *   **Modeling Implication:** This component's stability and strength underscore the critical importance of the cyclical features we plan to engineer.\n",
    "\n",
    "*   **Residual Component:**\n",
    "    *   This represents the \"noise\" or randomness left after the predictable trend and seasonal components have been removed.\n",
    "    *   Crucially, the residuals are centered around zero and display no obvious leftover patterns. This indicates that the decomposition has effectively captured the primary signals.\n",
    "    *   **Modeling Implication:** The residuals represent the day-to-day weather fluctuations our model will aim to predict using features like recent lags, rolling windows, and atmospheric conditions (`cloudcover`, `precip`, etc.).\n",
    "\n",
    "$\\implies$ **Conclusion:** The decomposition validates our initial analysis. The temperature is a composite of a dynamic long-term trend, a powerful seasonal cycle, and short-term noise. Our feature engineering strategy must explicitly provide the model with tools to address each of these components individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd54dc1",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"feature-distribution\" style=\"display: none;\">Q2.7. Predictor Analysis: Distribution, Skew, & Outliers</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"feature-distribution-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.7. Predictor Analysis: Distribution, Skew, & Outliers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183dd80",
   "metadata": {},
   "source": [
    "Having thoroughly analyzed the target variable, we now turn to the key predictors. Understanding their statistical distributions is essential for making sound preprocessing decisions. Features with significant skew or extreme outliers can disproportionately influence certain models and require robust handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4512a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a representative subset of key numerical features - a mix of atmospheric, precipitation, and solar variables\n",
    "features_to_plot = [\n",
    "    \"humidity\",\n",
    "    \"dew\",\n",
    "    \"sealevelpressure\",\n",
    "    \"cloudcover\",\n",
    "    \"solarradiation\",\n",
    "    \"uvindex\",\n",
    "    \"precip\",\n",
    "    \"windspeed\",\n",
    "    \"windgust\",\n",
    "]\n",
    "\n",
    "feature_color_map = {\n",
    "    feature: pio.templates[\"te\"].layout.colorway[i % len(pio.templates[\"te\"].layout.colorway)]\n",
    "    for i, feature in enumerate(features_to_plot)\n",
    "}\n",
    "\n",
    "# fig_distributions = eda.plot_feature_distributions(df=df_raw, features=features_to_plot, color_map=feature_color_map)\n",
    "# fig_distributions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543dfd2",
   "metadata": {},
   "source": [
    "We can categorize the features into three distinct groups:\n",
    "\n",
    "*   **1. Approximately Symmetric Distributions:**\n",
    "    *   `Humidity`, `Sealevelpressure`, and `Cloudcover` exhibit largely symmetric, well-behaved distributions. For these features, the mean (dashed line) is very close to the median (solid line), indicating a lack of significant skew. While some have outliers on both ends, they don't appear to heavily distort the central tendency.\n",
    "\n",
    "*   **2. Moderately Skewed Distributions:**\n",
    "    *   `Dew`, `Solarradiation`, and `Windspeed` show a noticeable skew. `Dew` and `Solarradiation` are moderately left-skewed (a longer tail towards lower values), while `Windspeed` is moderately right-skewed. This asymmetry is confirmed by the divergence of their mean and median values.\n",
    "\n",
    "*   **3. Extremely Skewed, Outlier-Dominated Distributions:**\n",
    "    *   `Precip` and `Windgust` represent the most challenging features. The distributions are heavily compressed near zero (for `Precip`), with long, thin tails representing rare but extreme events (heavy rainfall, strong gusts). The vast majority of data points are concentrated in a very small range, with a few extreme outliers dramatically extending the feature's scale. The `uvindex` also shows a strong left skew, compounded by its discrete, integer-based nature.\n",
    "\n",
    "$\\implies$ **Conclusion on Scaling Strategy:**\n",
    "Using a standard `mean-and-variance` scaler (`StandardScaler`) would be methodologically unsound. The extreme outliers in `Precip` and `Windgust` would massively distort the mean and standard deviation, effectively \"squashing\" the informational variance of the other 99% of data points.\n",
    "\n",
    "Therefore, the use of `RobustScaler` is better suited. By scaling data based on the **median and interquartile range (IQR)**, `RobustScaler` is inherently resistant to the influence of extreme outliers, ensuring that all features are scaled on a consistent and meaningful basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71bdb3",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"bivariate-analysis\" style=\"display: none;\">Q2.8. Bivariate & Seasonal Relationship Analysis</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"bivariate-analysis-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q2.8. Bivariate & Seasonal Relationship Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f0749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_features = [\n",
    "    \"temp\",\n",
    "    \"dew\",\n",
    "    \"precip\",\n",
    "    \"humidity\",\n",
    "    \"windspeed\",\n",
    "    \"solarradiation\",\n",
    "    \"cloudcover\",\n",
    "]\n",
    "\n",
    "# fig_pairplot = eda.plot_seasonal_pairplot(df=df_raw, features=pairplot_features)\n",
    "# fig_pairplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562dc7f",
   "metadata": {},
   "source": [
    "*   **1. Diagonals Confirm Strong Seasonal Shifts:**\n",
    "    *   The seasonal density plots on the diagonal clearly show that the distributions of key variables change dramatically between seasons.\n",
    "    *   During the **Rainy Season (blue)**, `Dew`, `Humidity`, and `Cloudcover` are all significantly higher and more concentrated than in the **Dry Season (yellow)**. `Precipitation` shifts from being almost entirely zero to having a long tail of rain events.\n",
    "    *   This confirms these variables are powerful indicators of the prevailing monsoon season and will be critical for the model.\n",
    "\n",
    "*   **2. Solar Radiation & Cloud Cover: The Primary Drivers:**\n",
    "    *   The scatter plots for `Temp` vs. `Solarradiation` and `Temp` vs. `Cloudcover` show the strongest, most consistent relationships.\n",
    "    *   There is a clear positive linear trend between `Solarradiation` and `Temp`: more sun results in higher temperatures.\n",
    "    *   Conversely, there is a strong negative linear trend between `Cloudcover` and `Temp`: more cloud cover leads to cooler temperatures.\n",
    "\n",
    "*   **3. The Less Pronounced Role of Humidity and Dew Point:**\n",
    "    *   The `Temp` vs. `Dew` plot shows a positive relationship; higher absolute moisture is associated with higher temperatures, likely due to the \"blanket effect\" trapping heat.\n",
    "    *   The `Temp` vs. `Humidity` plot reveals a more complex, negative relationship. This seems counter-intuitive but is explained by the seasonal context: the highest humidity occurs during the rainy season, which is also when increased cloud cover and rain suppress daytime temperatures. This highlights that `Humidity`'s effect is often indirect (i.e. non-linear).\n",
    "\n",
    "*   **4. Visual Confirmation of Multicollinearity:**\n",
    "    *   The relationship between `Dew` and `Humidity` is very strong and linear, as is the inverse relationship between `Solarradiation` and `Cloudcover`.\n",
    "    *   This finding is critical for modeling. While a tree-based model can handle this, a linear model would struggle with such highly correlated predictors. We should create distinct feature sets later in the pipeline.\n",
    "\n",
    "$\\implies$ **EDA Conclusion:** Now we understand the data much better, and can make more informed decisions when processing the data, engineering the features, and modeling later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210482f1",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"processing\" style=\"display: none;\">Q3. Data Processing & Correlation Analysis</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"processing-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">Q3. Data Processing & Correlation Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59637048",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"sanitization\" style=\"display: none;\">Q3.1. Column Sanitization</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"sanitization-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q3.1. Column Sanitization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671168b",
   "metadata": {},
   "source": [
    "Our first processing step is to remove all columns that were identified as non-predictive during the EDA. This includes:\n",
    "*   **Constant Identifiers:** Columns like `name`, `address`, `latitude`, etc., which have the same value for every record.\n",
    "*   **Redundant Duplicates:** Columns that are exact copies of others, such as `windspeedmax` being identical to `windspeed`.\n",
    "*   **Purely Descriptive Text:** Columns like `description` that will be handled by more targeted feature engineering later, not kept in their raw form.\n",
    "*   **Time-related Objects:** `sunrise` and `sunset` will be engineered into more useful numerical features (like day length) and are not needed in their raw object format.\n",
    "\n",
    "This step reduces memory usage and focuses our dataset on potentially valuable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f770278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DataFrame shape before sanitization: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3927</span><span style=\"color: #808000; text-decoration-color: #808000\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">37</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DataFrame shape before sanitization: \u001b[1;33m(\u001b[0m\u001b[1;33m3927\u001b[0m\u001b[33m, \u001b[0m\u001b[1;33m37\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DataFrame shape after sanitization:  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">24</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DataFrame shape after sanitization:  \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m24\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Columns removed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">13</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Columns removed: \u001b[1;31m13\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Remaining columns in the processed DataFrame:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mRemaining columns in the processed DataFrame:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tempmax'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tempmin'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'temp'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'feelslikemax'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'feelslikemin'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'feelslike'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'dew'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'humidity'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'precip'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'precipprob'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'precipcover'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'preciptype'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'windgust'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'windspeed'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'windspeedmean'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'windspeedmin'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'winddir'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sealevelpressure'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'cloudcover'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'visibility'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'solarradiation'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'uvindex'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'severerisk'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'moonphase'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'tempmax'\u001b[0m,\n",
       "    \u001b[32m'tempmin'\u001b[0m,\n",
       "    \u001b[32m'temp'\u001b[0m,\n",
       "    \u001b[32m'feelslikemax'\u001b[0m,\n",
       "    \u001b[32m'feelslikemin'\u001b[0m,\n",
       "    \u001b[32m'feelslike'\u001b[0m,\n",
       "    \u001b[32m'dew'\u001b[0m,\n",
       "    \u001b[32m'humidity'\u001b[0m,\n",
       "    \u001b[32m'precip'\u001b[0m,\n",
       "    \u001b[32m'precipprob'\u001b[0m,\n",
       "    \u001b[32m'precipcover'\u001b[0m,\n",
       "    \u001b[32m'preciptype'\u001b[0m,\n",
       "    \u001b[32m'windgust'\u001b[0m,\n",
       "    \u001b[32m'windspeed'\u001b[0m,\n",
       "    \u001b[32m'windspeedmean'\u001b[0m,\n",
       "    \u001b[32m'windspeedmin'\u001b[0m,\n",
       "    \u001b[32m'winddir'\u001b[0m,\n",
       "    \u001b[32m'sealevelpressure'\u001b[0m,\n",
       "    \u001b[32m'cloudcover'\u001b[0m,\n",
       "    \u001b[32m'visibility'\u001b[0m,\n",
       "    \u001b[32m'solarradiation'\u001b[0m,\n",
       "    \u001b[32m'uvindex'\u001b[0m,\n",
       "    \u001b[32m'severerisk'\u001b[0m,\n",
       "    \u001b[32m'moonphase'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_processed = df_raw.copy()\n",
    "\n",
    "cols_to_drop = [\n",
    "    # Constant Identifiers\n",
    "    \"name\",\n",
    "    \"address\",\n",
    "    \"resolvedAddress\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"source\",\n",
    "    # Duplicate\n",
    "    \"windspeedmax\",\n",
    "    # Descriptive/Object columns to be engineered later\n",
    "    \"description\",\n",
    "    \"conditions\",\n",
    "    \"icon\",\n",
    "    \"sunrise\",\n",
    "    \"sunset\",\n",
    "    # Redundant energy column\n",
    "    \"solarenergy\",\n",
    "]\n",
    "\n",
    "shape_before = df_processed.shape\n",
    "\n",
    "df_processed.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "shape_after = df_processed.shape\n",
    "\n",
    "console.print(f\"DataFrame shape before sanitization: [yellow]{shape_before}[/yellow]\")\n",
    "console.print(f\"DataFrame shape after sanitization:  [green]{shape_after}[/green]\")\n",
    "console.print(f\"Columns removed: [red]{shape_before[1] - shape_after[1]}[/red]\")\n",
    "\n",
    "console.print(\"\\n[bold]Remaining columns in the processed DataFrame:[/bold]\")\n",
    "console.print(df_processed.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f900e",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"imputation\" style=\"display: none;\">Q3.2. Missing Value Imputation</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"imputation-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q3.2. Missing Value Imputation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c662d2a",
   "metadata": {},
   "source": [
    "The EDA revealed that missing values are not random but systematic. We address them with deterministic, rule-based imputation that preserves the underlying meaning of the data. Drop is no-no:)\n",
    "\n",
    "*   **`preciptype`:** Missingness in this column occurs exclusively when the `precip` (precipitation amount) is zero. Therefore, we will fill these `NaN` values with the string `'none'`. This transforms the missing data into an explicit, meaningful category for \"no-precipitation days.\"\n",
    "*   **`severerisk`:** Values in this column are only recorded when a certain risk threshold is met. The absence of a value implies the absence of risk. We will fill these `NaN` values with `0`.\n",
    "\n",
    "This approach avoids data deletion and ensures that every value in our dataset is explicitly defined, and potentially providing more predictive signals to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16252754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total remaining null values in the DataFrame: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total remaining null values in the DataFrame: \u001b[1;32m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Value counts for </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'preciptype'</span><span style=\"font-weight: bold\"> after imputation:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mValue counts for \u001b[0m\u001b[1;32m'preciptype'\u001b[0m\u001b[1m after imputation:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preciptype\n",
      "rain    3003\n",
      "none     924\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Value counts for </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'severerisk'</span><span style=\"font-weight: bold\"> after imputation:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mValue counts for \u001b[0m\u001b[1;32m'severerisk'\u001b[0m\u001b[1m after imputation:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severerisk\n",
      "0.0     2731\n",
      "30.0     546\n",
      "10.0     448\n",
      "60.0     195\n",
      "75.0       6\n",
      "5.0        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Fill 'preciptype' based on the deterministic relationship with 'precip'\n",
    "df_processed[\"preciptype\"].fillna(\"none\", inplace=True)\n",
    "\n",
    "# 2. Fill 'severerisk' with 0, as missingness implies no recorded severe risk\n",
    "df_processed[\"severerisk\"].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "remaining_nulls = df_processed.isnull().sum().sum()\n",
    "\n",
    "null_count_color = \"green\" if remaining_nulls == 0 else \"red\"\n",
    "console.print(\n",
    "    f\"Total remaining null values in the DataFrame: [bold {null_count_color}]{remaining_nulls}[/bold {null_count_color}]\"\n",
    ")\n",
    "\n",
    "if remaining_nulls == 0:\n",
    "    console.print(\"\\n[bold]Value counts for 'preciptype' after imputation:[/bold]\")\n",
    "    print(df_processed[\"preciptype\"].value_counts())\n",
    "\n",
    "    console.print(\"\\n[bold]Value counts for 'severerisk' after imputation:[/bold]\")\n",
    "    print(df_processed[\"severerisk\"].value_counts())\n",
    "\n",
    "else:\n",
    "    console.print(\"There are still missing values that need to be addressed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008a2bd",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"correlation-deep-dive\" style=\"display: none;\">Q3.3. Correlations Check</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"correlation-deep-dive-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q3.3. Correlations Check</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b146739",
   "metadata": {},
   "source": [
    "We will employ two different correlation methods to ensure a comprehensive understanding:\n",
    "\n",
    "*   **Pearson Correlation (`r`):**\n",
    "    *   Standard method, measuring the strength of the *linear* relationship between two variables\n",
    "    *   Sensitive to outliers\n",
    "    *   A value of +1 indicates a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
    "\n",
    "*   **Spearman Correlation (`ρ` or `rho`):**\n",
    "    *   Measures the strength of the *monotonic* relationship between two variables\n",
    "    *   Operates on the ranks of the data rather than the raw values $\\rightarrow$ highly robust to outliers, capable of capturing relationships that are consistently increasing or decreasing but not necessarily linear.\n",
    "\n",
    "Our weather data has outliers, and potential non-linearities **$\\implies$** the **Spearman correlation provides a more reliable and robust assessment** of the underlying relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c81910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Target Correlation Summary (Pearson)</span> \n",
       "\n",
       "<span style=\"font-style: italic\">  </span><span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">Most Positively Correlated</span><span style=\"font-style: italic\">  </span>                         <span style=\"font-style: italic\"> </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">Most Negatively Correlated with</span><span style=\"font-style: italic\">  </span>                          \n",
       "<span style=\"font-style: italic\">         </span><span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">with 'temp'</span><span style=\"font-style: italic\">          </span>                         <span style=\"font-style: italic\">              </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">'temp'</span><span style=\"font-style: italic\">              </span>                          \n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                         ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                          \n",
       "┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature      </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃                         ┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature          </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃                          \n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                         ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                          \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike    </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.893 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity         </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.340 </span>│                          \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmax      </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.847 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> cloudcover       </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.308 </span>│                          \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmin      </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.792 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> precip           </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.222 </span>│                          \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemin </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.771 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> sealevelpressure </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.128 </span>│                          \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemax </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.634 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> winddir          </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.087 </span>│                          \n",
       "└──────────────┴─────────────┘                         └──────────────────┴─────────────┘                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30mTarget Correlation Summary (Pearson)\u001b[0m \n",
       "\n",
       "\u001b[3m  \u001b[0m\u001b[1;3;38;2;105;1;32mMost Positively Correlated\u001b[0m\u001b[3m  \u001b[0m                         \u001b[3m \u001b[0m\u001b[1;3;38;2;6;49;99mMost Negatively Correlated with\u001b[0m\u001b[3m  \u001b[0m                          \n",
       "\u001b[3m         \u001b[0m\u001b[1;3;38;2;105;1;32mwith 'temp'\u001b[0m\u001b[3m          \u001b[0m                         \u001b[3m              \u001b[0m\u001b[1;3;38;2;6;49;99m'temp'\u001b[0m\u001b[3m              \u001b[0m                          \n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                         ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                          \n",
       "┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature     \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃                         ┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature         \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃                          \n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                         ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                          \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslike   \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.893\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mhumidity        \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.340\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                          \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmax     \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.847\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mcloudcover      \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.308\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                          \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmin     \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.792\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mprecip          \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.222\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                          \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslikemin\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.771\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36msealevelpressure\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.128\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                          \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslikemax\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.634\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mwinddir         \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.087\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                          \n",
       "└──────────────┴─────────────┘                         └──────────────────┴─────────────┘                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Predictor Multicollinearity Summary (Pearson)</span> \n",
       "\n",
       "<span style=\"font-style: italic\">       </span><span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">Highest Positive Predictor Pairs</span><span style=\"font-style: italic\">        </span>            <span style=\"font-style: italic\">       </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">Highest Negative Predictor Pairs</span><span style=\"font-style: italic\">        </span>         \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓            ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓         \n",
       "┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 1      </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 2    </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃            ┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 1  </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 2        </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃         \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩            ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmin        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemin </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.911 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> dew        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> sealevelpressure </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.652 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> uvindex      </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.911 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> cloudcover </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation   </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.601 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> temp           </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike    </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.893 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation   </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.567 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmax        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> temp         </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.847 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> cloudcover </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> uvindex          </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.557 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemax   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike    </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.833 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> sealevelpressure </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.554 </span>│         \n",
       "└────────────────┴──────────────┴─────────────┘            └────────────┴──────────────────┴─────────────┘         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;30mPredictor Multicollinearity Summary (Pearson)\u001b[0m \n",
       "\n",
       "\u001b[3m       \u001b[0m\u001b[1;3;38;2;105;1;32mHighest Positive Predictor Pairs\u001b[0m\u001b[3m        \u001b[0m            \u001b[3m       \u001b[0m\u001b[1;3;38;2;6;49;99mHighest Negative Predictor Pairs\u001b[0m\u001b[3m        \u001b[0m         \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓            ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓         \n",
       "┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 1     \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 2   \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃            ┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 1 \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 2       \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃         \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩            ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmin       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslikemin\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.911\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mdew       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msealevelpressure\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.652\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36msolarradiation\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36muvindex     \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.911\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mcloudcover\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msolarradiation  \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.601\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtemp          \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslike   \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.893\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mhumidity  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msolarradiation  \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.567\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmax       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mtemp        \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.847\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mcloudcover\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36muvindex         \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.557\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslikemax  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslike   \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.833\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mhumidity  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msealevelpressure\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.554\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "└────────────────┴──────────────┴─────────────┘            └────────────┴──────────────────┴─────────────┘         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_numerical = df_processed.select_dtypes(include=np.number)\n",
    "\n",
    "# fig_pearson = eda.plot_correlation_heatmap_enhanced(df=df_numerical, method=\"pearson\")\n",
    "# fig_pearson.show()\n",
    "\n",
    "ah.display_correlation_summary(df=df_numerical, target=config.TARGET_VARIABLE, method=\"pearson\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f165eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Target Correlation Summary (Spearman)</span> \n",
       "\n",
       "<span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">Most Positively Correlated with</span><span style=\"font-style: italic\"> </span>                         <span style=\"font-style: italic\"> </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">Most Negatively Correlated with</span><span style=\"font-style: italic\">  </span>                        \n",
       "<span style=\"font-style: italic\">             </span><span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">'temp'</span><span style=\"font-style: italic\">             </span>                         <span style=\"font-style: italic\">              </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">'temp'</span><span style=\"font-style: italic\">              </span>                        \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                         ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                        \n",
       "┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature        </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃                         ┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature          </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃                        \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                         ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                        \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike      </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.874 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity         </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.393 </span>│                        \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmax        </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.840 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> cloudcover       </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.335 </span>│                        \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmin        </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.757 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> precip           </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.160 </span>│                        \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemin   </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.756 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> winddir          </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.101 </span>│                        \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.599 </span>│                         │<span style=\"color: #008080; text-decoration-color: #008080\"> sealevelpressure </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.091 </span>│                        \n",
       "└────────────────┴─────────────┘                         └──────────────────┴─────────────┘                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;30mTarget Correlation Summary (Spearman)\u001b[0m \n",
       "\n",
       "\u001b[1;3;38;2;105;1;32mMost Positively Correlated with\u001b[0m\u001b[3m \u001b[0m                         \u001b[3m \u001b[0m\u001b[1;3;38;2;6;49;99mMost Negatively Correlated with\u001b[0m\u001b[3m  \u001b[0m                        \n",
       "\u001b[3m             \u001b[0m\u001b[1;3;38;2;105;1;32m'temp'\u001b[0m\u001b[3m             \u001b[0m                         \u001b[3m              \u001b[0m\u001b[1;3;38;2;6;49;99m'temp'\u001b[0m\u001b[3m              \u001b[0m                        \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                         ┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓                        \n",
       "┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature       \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃                         ┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature         \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃                        \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                         ┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩                        \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslike     \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.874\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mhumidity        \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.393\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                        \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmax       \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.840\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mcloudcover      \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.335\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                        \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmin       \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.757\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mprecip          \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.160\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                        \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslikemin  \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.756\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36mwinddir         \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.101\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                        \n",
       "│\u001b[36m \u001b[0m\u001b[36msolarradiation\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.599\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│                         │\u001b[36m \u001b[0m\u001b[36msealevelpressure\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.091\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│                        \n",
       "└────────────────┴─────────────┘                         └──────────────────┴─────────────┘                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Predictor Multicollinearity Summary (Spearman)</span> \n",
       "\n",
       "<span style=\"font-style: italic\">       </span><span style=\"color: #690120; text-decoration-color: #690120; font-weight: bold; font-style: italic\">Highest Positive Predictor Pairs</span><span style=\"font-style: italic\">        </span>            <span style=\"font-style: italic\">       </span><span style=\"color: #063163; text-decoration-color: #063163; font-weight: bold; font-style: italic\">Highest Negative Predictor Pairs</span><span style=\"font-style: italic\">        </span>         \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓            ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓         \n",
       "┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 1      </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 2    </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃            ┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 1  </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Feature 2        </span>┃<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> Coefficient </span>┃         \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩            ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmin        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemin </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.999 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> dew        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> sealevelpressure </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.610 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> uvindex      </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.881 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation   </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.600 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> temp           </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike    </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.874 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> cloudcover </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> solarradiation   </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.584 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> tempmax        </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> temp         </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.840 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> visibility       </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.566 </span>│         \n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslikemax   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> feelslike    </span>│<span style=\"color: #690120; text-decoration-color: #690120\">       0.823 </span>│            │<span style=\"color: #008080; text-decoration-color: #008080\"> humidity   </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> uvindex          </span>│<span style=\"color: #063163; text-decoration-color: #063163\">      -0.559 </span>│         \n",
       "└────────────────┴──────────────┴─────────────┘            └────────────┴──────────────────┴─────────────┘         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;30mPredictor Multicollinearity Summary (Spearman)\u001b[0m \n",
       "\n",
       "\u001b[3m       \u001b[0m\u001b[1;3;38;2;105;1;32mHighest Positive Predictor Pairs\u001b[0m\u001b[3m        \u001b[0m            \u001b[3m       \u001b[0m\u001b[1;3;38;2;6;49;99mHighest Negative Predictor Pairs\u001b[0m\u001b[3m        \u001b[0m         \n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓            ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓         \n",
       "┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 1     \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 2   \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃            ┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 1 \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mFeature 2       \u001b[0m\u001b[1;30m \u001b[0m┃\u001b[1;30m \u001b[0m\u001b[1;30mCoefficient\u001b[0m\u001b[1;30m \u001b[0m┃         \n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩            ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmin       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslikemin\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.999\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mdew       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msealevelpressure\u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.610\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36msolarradiation\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36muvindex     \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.881\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mhumidity  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msolarradiation  \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.600\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtemp          \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslike   \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.874\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mcloudcover\u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36msolarradiation  \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.584\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mtempmax       \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mtemp        \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.840\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mhumidity  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mvisibility      \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.566\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "│\u001b[36m \u001b[0m\u001b[36mfeelslikemax  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36mfeelslike   \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;105;1;32m \u001b[0m\u001b[38;2;105;1;32m      0.823\u001b[0m\u001b[38;2;105;1;32m \u001b[0m│            │\u001b[36m \u001b[0m\u001b[36mhumidity  \u001b[0m\u001b[36m \u001b[0m│\u001b[36m \u001b[0m\u001b[36muvindex         \u001b[0m\u001b[36m \u001b[0m│\u001b[38;2;6;49;99m \u001b[0m\u001b[38;2;6;49;99m     -0.559\u001b[0m\u001b[38;2;6;49;99m \u001b[0m│         \n",
       "└────────────────┴──────────────┴─────────────┘            └────────────┴──────────────────┴─────────────┘         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig_spearman = eda.plot_correlation_heatmap_enhanced(df=df_numerical, method=\"spearman\")\n",
    "# fig_spearman.show()\n",
    "\n",
    "ah.display_correlation_summary(df=df_numerical, target=config.TARGET_VARIABLE, method=\"spearman\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59fa54",
   "metadata": {},
   "source": [
    "*   **1. Key drivers:**\n",
    "    *   Both matrices confirm that `temp` is most strongly correlated with its variants (`feelslike`, `tempmax`, `tempmin`), which is expected.\n",
    "    *   More importantly, they both confirm our hypotheses about the primary physical drivers: `solarradiation` has a strong positive correlation with temperature, while `humidity` and `cloudcover` have the strongest negative correlations.\n",
    "\n",
    "*   **2. Severe multicollinearity:** Both methods reveal two critical clusters of extreme multicollinearity, which is a concern for linear models:\n",
    "    *   **Temperature cluster:** Features like `temp`, `feelslike`, `tempmin`, and `feelslikemin` are all highly inter-correlated (many coefficients > 0.80).\n",
    "    *   **Humidity cluster:** `dew` and `humidity` show a very strong positive relationship (`r` = 0.82, `ρ` = 0.80).\n",
    "    *   **Solar cluster:** `solarradiation` and `uvindex` are also highly correlated, effectively measuring the same underlying phenomenon of sun intensity.\n",
    "\n",
    "*   **3. Spearman's robustness:**\n",
    "    *   The correlation between `tempmin` and `feelslikemin` is **`ρ` = 0.999**. This indicates they are functionally identical in terms of their rank order and are perfectly redundant predictors.\n",
    "    *   The negative correlations for `humidity` and `cloudcover` with `temp` are **stronger** in the Spearman results $\\rightarrow$ increases confidence in their importance, as their relationship is consistently monotonic,  not just an artifact of a few outliers.\n",
    "\n",
    "$\\implies$ Attempting to fit a standard linear model with all of these features would result in **unstable and uninterpretable** coefficients. We need another approach to manage this multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad339c72",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"multicollinearity-strategy\" style=\"display: none;\">Q3.4. Strategy for Multicollinearity</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"multicollinearity-strategy-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q3.4. Strategy for Multicollinearity</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b62ca",
   "metadata": {},
   "source": [
    "We have confirmed the presence of severe multicollinearity - this occurs when two or more predictor variables are highly correlated, making it difficult for a model to disentangle their individual effects on the target variable. For standard **linear models**, this can lead to **unstable, unreliable, and uninterpretable coefficient** estimates.\n",
    "\n",
    "*   **Standard diagnostic tool: Variance Inflation Factor (VIF)**\n",
    "    *   We can use this common method to diagnose, which calculates how much the variance of a regression coefficient is inflated due to its correlation with other predictors.\n",
    "    *   A high VIF (usually > 5 or 10) for a feature indicates that it is highly collinear and a candidate for removal.\n",
    "\n",
    "*   **Our choice: Regularization**\n",
    "    *   While a manual, VIF-based feature pruning is a valid approach, we will instead use a more integrated strategy by leveraging models with **built-in regularization**.\n",
    "    *   Algorithms like `RidgeCV`, `ElasticNetCV`, and `MultiTaskLassoCV` (which we will use for feature selection) are specifically designed to handle multicollinearity. They add a penalty term to the loss function that shrinks the coefficients of correlated predictors.\n",
    "    *   **In practice, this means the model itself will learn to \"de-emphasize\" redundant features.** For example, when faced with the highly correlated `tempmin` and `feelslikemin`, a regularized model will likely assign a significant coefficient to one while shrinking the coefficient of the other towards zero. This achieves the same goal as manual VIF-based removal but in a data-driven, automated fashion that is integrated directly into the model training process.\n",
    "\n",
    "$\\implies$ **Justification:** By choosing regularized models and embedded feature selection, we have a more robust and sophisticated approach to managing multicollinearity. This allows the model to make the optimal data-driven trade-offs during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e09c3",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"finalizing-df\" style=\"display: none;\">Q3.5. Final Processed DataFrame before Feature Engineering</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"finalizing-df-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q3.5. Final Processed DataFrame before Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe1d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Final Shape of Processed DataFrame:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">24</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span> \n",
       "<span style=\"font-weight: bold\">Data Types and Memory Usage:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mFinal Shape of Processed DataFrame:\u001b[0m \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m24\u001b[0m\u001b[1;32m)\u001b[0m \n",
       "\u001b[1mData Types and Memory Usage:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3927 entries, 2015-01-01 to 2025-10-01\n",
      "Data columns (total 24 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   tempmax           3927 non-null   float64\n",
      " 1   tempmin           3927 non-null   float64\n",
      " 2   temp              3927 non-null   float64\n",
      " 3   feelslikemax      3927 non-null   float64\n",
      " 4   feelslikemin      3927 non-null   float64\n",
      " 5   feelslike         3927 non-null   float64\n",
      " 6   dew               3927 non-null   float64\n",
      " 7   humidity          3927 non-null   float64\n",
      " 8   precip            3927 non-null   float64\n",
      " 9   precipprob        3927 non-null   int64  \n",
      " 10  precipcover       3927 non-null   float64\n",
      " 11  preciptype        3927 non-null   object \n",
      " 12  windgust          3927 non-null   float64\n",
      " 13  windspeed         3927 non-null   float64\n",
      " 14  windspeedmean     3927 non-null   float64\n",
      " 15  windspeedmin      3927 non-null   float64\n",
      " 16  winddir           3927 non-null   float64\n",
      " 17  sealevelpressure  3927 non-null   float64\n",
      " 18  cloudcover        3927 non-null   float64\n",
      " 19  visibility        3927 non-null   float64\n",
      " 20  solarradiation    3927 non-null   float64\n",
      " 21  uvindex           3927 non-null   int64  \n",
      " 22  severerisk        3927 non-null   float64\n",
      " 23  moonphase         3927 non-null   float64\n",
      "dtypes: float64(21), int64(2), object(1)\n",
      "memory usage: 939.6 KB\n"
     ]
    }
   ],
   "source": [
    "console.print(\n",
    "    f\"[bold]Final Shape of Processed DataFrame:[/bold] [green]{df_processed.shape}[/green]\",\n",
    "    \"\\n[bold]Data Types and Memory Usage:[/bold]\",\n",
    ")\n",
    "df_processed.info(memory_usage=\"deep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77eb9b",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"feature-engineering\" style=\"display: none;\">Q4. Feature Engineering</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"feature-engineering-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">Q4. Feature Engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d769f",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-temporal\" style=\"display: none;\">Q4.1. Foundational Temporal Features</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-temporal-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.1. Foundational Temporal Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7870bb",
   "metadata": {},
   "source": [
    "One of the most critical stages of the project - we need to transform the clean, processed data into an information-rich feature set that makes the underlying patterns we discovered in the EDA explicit for our machine learning models. We will engineer features to encode four key dimensions of the time-series data:\n",
    "\n",
    "1.  **Basic timestamps:** Discrete markers of time (`year`, `month`).\n",
    "2.  **Cyclical seasonality:** Continuous representations of the annual cycle.\n",
    "3.  **Complex seasonality (Fourier):** Features to capture multi-frequency patterns like the bimodal climate cycle.\n",
    "4.  **Domain-specific indicators:** Meteorological features that make physical relationships explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cdc6971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Temporal features created.</span>\n",
       "  - Number of new features: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">12</span>\n",
       "  - DataFrame shape is now: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">36</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTemporal features created.\u001b[0m\n",
       "  - Number of new features: \u001b[1;32m12\u001b[0m\n",
       "  - DataFrame shape is now: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m36\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Sample of new temporal features:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mSample of new temporal features:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "week_of_year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "sin_day_of_year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cos_day_of_year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sin_month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cos_month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_sin_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_cos_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_sin_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_cos_3",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1c84200e-5768-49a4-ad64-f9361a54f7cb",
       "rows": [
        [
         "2015-01-01 00:00:00",
         "2015",
         "1",
         "1",
         "1",
         "0.01716632975470737",
         "0.9998526477050269",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.034327600513243496",
         "0.9994106342455052",
         "0.05147875477034653",
         "0.9986740898848305"
        ],
        [
         "2015-01-02 00:00:00",
         "2015",
         "1",
         "1",
         "2",
         "0.034327600513243496",
         "0.9994106342455052",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.06861473800213404",
         "0.9976432316860063",
         "0.1028209971373604",
         "0.9946998756145891"
        ],
        [
         "2015-01-03 00:00:00",
         "2015",
         "1",
         "1",
         "3",
         "0.05147875477034653",
         "0.9986740898848305",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.1028209971373604",
         "0.9946998756145891",
         "0.15389057670406178",
         "0.9880878960910772"
        ],
        [
         "2015-01-04 00:00:00",
         "2015",
         "1",
         "1",
         "4",
         "0.06861473800213404",
         "0.9976432316860063",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.13690605792347524",
         "0.990584035457797",
         "0.2045520661262008",
         "0.9788556850953578"
        ],
        [
         "2015-01-05 00:00:00",
         "2015",
         "1",
         "2",
         "5",
         "0.08573050015569435",
         "0.9963183634476755",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.17082974322534417",
         "0.985300562686109",
         "0.2546711202412287",
         "0.9670277247913204"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>sin_day_of_year</th>\n",
       "      <th>cos_day_of_year</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>fourier_sin_2</th>\n",
       "      <th>fourier_cos_2</th>\n",
       "      <th>fourier_sin_3</th>\n",
       "      <th>fourier_cos_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.034328</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.998674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034328</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.068615</td>\n",
       "      <td>0.997643</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>0.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.998674</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.102821</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.153891</td>\n",
       "      <td>0.988088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.068615</td>\n",
       "      <td>0.997643</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.136906</td>\n",
       "      <td>0.990584</td>\n",
       "      <td>0.204552</td>\n",
       "      <td>0.978856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.085731</td>\n",
       "      <td>0.996318</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>0.985301</td>\n",
       "      <td>0.254671</td>\n",
       "      <td>0.967028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            year  month  week_of_year  day_of_year  sin_day_of_year  \\\n",
       "datetime                                                              \n",
       "2015-01-01  2015      1             1            1         0.017166   \n",
       "2015-01-02  2015      1             1            2         0.034328   \n",
       "2015-01-03  2015      1             1            3         0.051479   \n",
       "2015-01-04  2015      1             1            4         0.068615   \n",
       "2015-01-05  2015      1             2            5         0.085731   \n",
       "\n",
       "            cos_day_of_year  sin_month  cos_month  fourier_sin_2  \\\n",
       "datetime                                                           \n",
       "2015-01-01         0.999853        0.5   0.866025       0.034328   \n",
       "2015-01-02         0.999411        0.5   0.866025       0.068615   \n",
       "2015-01-03         0.998674        0.5   0.866025       0.102821   \n",
       "2015-01-04         0.997643        0.5   0.866025       0.136906   \n",
       "2015-01-05         0.996318        0.5   0.866025       0.170830   \n",
       "\n",
       "            fourier_cos_2  fourier_sin_3  fourier_cos_3  \n",
       "datetime                                                 \n",
       "2015-01-01       0.999411       0.051479       0.998674  \n",
       "2015-01-02       0.997643       0.102821       0.994700  \n",
       "2015-01-03       0.994700       0.153891       0.988088  \n",
       "2015-01-04       0.990584       0.204552       0.978856  \n",
       "2015-01-05       0.985301       0.254671       0.967028  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_engineered = df_processed.copy()\n",
    "\n",
    "# --- 4.1.2: Basic Temporal Features ---\n",
    "# These features help the model understand the long-term trend and basic monthly groupings.\n",
    "df_engineered[\"year\"] = df_engineered.index.year\n",
    "df_engineered[\"month\"] = df_engineered.index.month\n",
    "df_engineered[\"week_of_year\"] = df_engineered.index.isocalendar().week.astype(int)\n",
    "df_engineered[\"day_of_year\"] = df_engineered.index.dayofyear\n",
    "\n",
    "# --- 4.1.3: Cyclical Features ---\n",
    "# Sine/cosine transformations are essential for cyclical data. They encode the\n",
    "# \"closeness\" of time points (e.g., Dec 31 is close to Jan 1) in a way that\n",
    "# linear models can understand.\n",
    "day_of_year_max = 366.0  # Account for leap years\n",
    "month_max = 12.0\n",
    "\n",
    "df_engineered[\"sin_day_of_year\"] = np.sin(2 * np.pi * df_engineered[\"day_of_year\"] / day_of_year_max)\n",
    "df_engineered[\"cos_day_of_year\"] = np.cos(2 * np.pi * df_engineered[\"day_of_year\"] / day_of_year_max)\n",
    "df_engineered[\"sin_month\"] = np.sin(2 * np.pi * df_engineered[\"month\"] / month_max)\n",
    "df_engineered[\"cos_month\"] = np.cos(2 * np.pi * df_engineered[\"month\"] / month_max)\n",
    "\n",
    "# --- 4.1.4: Fourier Series Features ---\n",
    "# Our EDA revealed a bimodal seasonal pattern. While the annual sin/cos\n",
    "# captures the main cycle, higher-frequency Fourier terms can help model this\n",
    "# more complex, multi-frequency seasonality (e.g., a semi-annual cycle). Just maybe, but hey it doesn't hurt to try.\n",
    "for k in range(2, 4):  # Add k=2 (semi-annual) and k=3 (quarterly) terms\n",
    "    df_engineered[f\"fourier_sin_{k}\"] = np.sin(2 * np.pi * k * df_engineered[\"day_of_year\"] / day_of_year_max)\n",
    "    df_engineered[f\"fourier_cos_{k}\"] = np.cos(2 * np.pi * k * df_engineered[\"day_of_year\"] / day_of_year_max)\n",
    "\n",
    "\n",
    "temporal_features_added = [\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"week_of_year\",\n",
    "    \"day_of_year\",\n",
    "    \"sin_day_of_year\",\n",
    "    \"cos_day_of_year\",\n",
    "    \"sin_month\",\n",
    "    \"cos_month\",\n",
    "    \"fourier_sin_2\",\n",
    "    \"fourier_cos_2\",\n",
    "    \"fourier_sin_3\",\n",
    "    \"fourier_cos_3\",\n",
    "]\n",
    "console.print(\n",
    "    \"[bold]Temporal features created.[/bold]\\n\",\n",
    "    f\" - Number of new features: [green]{len(temporal_features_added)}[/green]\\n\",\n",
    "    f\" - DataFrame shape is now: [green]{df_engineered.shape}[/green]\",\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Sample of new temporal features:[/bold]\")\n",
    "display(df_engineered[temporal_features_added].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778210d7",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-domain\" style=\"display: none;\">Q4.2. Domain-Specific Feature Creation</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-domain-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.2. Domain-Specific Feature Creation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f36b4",
   "metadata": {},
   "source": [
    "Beyond generic time-based signals, we can engineer features that make the physical relationships we hypothesized in our EDA explicit. By calculating these values, we provide the model with pre-computed, high-value predictors that directly relate to the drivers of temperature in a tropical climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf95149d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Domain-specific features created.</span>\n",
       "  - Number of new features: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span>\n",
       "  - DataFrame shape is now: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">39</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mDomain-specific features created.\u001b[0m\n",
       "  - Number of new features: \u001b[1;32m3\u001b[0m\n",
       "  - DataFrame shape is now: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m39\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Sample of new domain-specific features:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mSample of new domain-specific features:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "daylight_hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_range",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_point_deficit",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3a2260c3-4631-4214-bade-0e09d90fd128",
       "rows": [
        [
         "2015-01-01 00:00:00",
         "11.504722222222222",
         "8.0",
         "8.700000000000003"
        ],
        [
         "2015-01-02 00:00:00",
         "11.507222222222222",
         "10.0",
         "9.2"
        ],
        [
         "2015-01-03 00:00:00",
         "11.509722222222223",
         "9.0",
         "7.800000000000001"
        ],
        [
         "2015-01-04 00:00:00",
         "11.5125",
         "8.0",
         "5.800000000000001"
        ],
        [
         "2015-01-05 00:00:00",
         "11.515555555555556",
         "5.700000000000003",
         "4.599999999999998"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daylight_hours</th>\n",
       "      <th>temp_range</th>\n",
       "      <th>dew_point_deficit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>11.504722</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>11.507222</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>11.509722</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>11.512500</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>11.515556</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            daylight_hours  temp_range  dew_point_deficit\n",
       "datetime                                                 \n",
       "2015-01-01       11.504722         8.0                8.7\n",
       "2015-01-02       11.507222        10.0                9.2\n",
       "2015-01-03       11.509722         9.0                7.8\n",
       "2015-01-04       11.512500         8.0                5.8\n",
       "2015-01-05       11.515556         5.7                4.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4.2.1: Daylight Hours ---\n",
    "# This is a more direct measure of solar energy potential than day_of_year.\n",
    "sunrise_sunset = pd.read_csv(\n",
    "    config.INPUT_DATA_PATH, usecols=[\"datetime\", \"sunrise\", \"sunset\"], parse_dates=[\"datetime\", \"sunrise\", \"sunset\"]\n",
    ").set_index(\"datetime\")\n",
    "\n",
    "df_engineered[\"daylight_hours\"] = (sunrise_sunset[\"sunset\"] - sunrise_sunset[\"sunrise\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# --- 4.2.2: Daily Temperature Range ---\n",
    "# This feature quantifies daily temperature volatility. A low range often indicates\n",
    "# a cloudy or rainy day, while a high range suggests a clear, sunny day.\n",
    "df_engineered[\"temp_range\"] = df_engineered[\"tempmax\"] - df_engineered[\"tempmin\"]\n",
    "\n",
    "# --- 4.2.3: Dew Point Deficit ---\n",
    "# This measures how close the air is to saturation. A smaller deficit means\n",
    "# higher relative humidity and can inhibit nighttime cooling.\n",
    "df_engineered[\"dew_point_deficit\"] = df_engineered[\"temp\"] - df_engineered[\"dew\"]\n",
    "\n",
    "\n",
    "domain_features_added = [\"daylight_hours\", \"temp_range\", \"dew_point_deficit\"]\n",
    "console.print(\n",
    "    \"[bold]Domain-specific features created.[/bold]\\n\",\n",
    "    f\" - Number of new features: [green]{len(domain_features_added)}[/green]\\n\",\n",
    "    f\" - DataFrame shape is now: [green]{df_engineered.shape}[/green]\",\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Sample of new domain-specific features:[/bold]\")\n",
    "display(df_engineered[domain_features_added].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913c899",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-lag-rolling\" style=\"display: none;\">Q4.3. Lag & Rolling Window Features</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-lag-rolling-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.3. Lag & Rolling Window Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5485a0f",
   "metadata": {},
   "source": [
    "These features are the cornerstone of time-series forecasting, allowing the model to learn from the recent past.\n",
    "\n",
    "*   **Lag features** provide a direct \"memory\" of a variable's value from previous days. For example, the temperature from yesterday (`temp_lag_1`) is one of the single best predictors for the temperature today. We will test just how much it is with a naive baseline (persistence) model later.\n",
    "\n",
    "*   **Rolling window features** provide \"context\" by summarizing a variable's behavior over a recent period. This captures momentum and volatility. For example, the 7-day average temperature (`temp_roll_7d_mean`) tells the model if the past week has been generally hotter or cooler than the seasonal average.\n",
    "\n",
    "**Preventing data leakage:**\n",
    "* To calculate rolling window features for a given day `t`, we **only use data from `t-1` and earlier**\n",
    "* We prevent including the value from day `t` in its own rolling calculation by applying `.shift(1)` to the data *before* the `.rolling()` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed081a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Lag and rolling features created.</span>\n",
       "  - Number of new lag/rolling features: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">93</span>\n",
       "  - DataFrame shape is now: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">132</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mLag and rolling features created.\u001b[0m\n",
       "  - Number of new lag/rolling features: \u001b[1;32m93\u001b[0m\n",
       "  - DataFrame shape is now: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m132\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4.3.1: Lag Features ---\n",
    "# Define the features to lag and the time periods for the lookback.\n",
    "# We choose key dynamic variables that characterize the daily weather system.\n",
    "features_to_lag = [\n",
    "    \"temp\",\n",
    "    \"humidity\",\n",
    "    \"solarradiation\",\n",
    "    \"precip\",\n",
    "    \"windspeed\",\n",
    "    \"cloudcover\",\n",
    "    \"dew\",\n",
    "    \"sealevelpressure\",\n",
    "    \"windgust\",\n",
    "]\n",
    "lag_periods = [1, 2, 3, 7, 14, 28]\n",
    "\n",
    "for feature in features_to_lag:\n",
    "    for lag in lag_periods:\n",
    "        df_engineered[f\"{feature}_lag_{lag}\"] = df_engineered[feature].shift(lag)\n",
    "\n",
    "# --- 4.3.2: Rolling Window Features ---\n",
    "# Define features and windows for rolling statistics.\n",
    "features_for_rolling = [\n",
    "    \"temp\",\n",
    "    \"humidity\",\n",
    "    \"windspeed\",\n",
    "    \"solarradiation\",\n",
    "    \"dew\",\n",
    "    \"cloudcover\",  # Precip has its own logic\n",
    "]\n",
    "window_sizes = [7, 14, 28]\n",
    "rolling_stats = [\"mean\", \"std\"]\n",
    "\n",
    "for feature in features_for_rolling:\n",
    "    # Shift the series by 1 to prevent data leakage.\n",
    "    series_shifted = df_engineered[feature].shift(1)\n",
    "    for window in window_sizes:\n",
    "        for stat in rolling_stats:\n",
    "            col_name = f\"{feature}_roll_{window}d_{stat}\"\n",
    "            df_engineered[col_name] = series_shifted.rolling(window=window, min_periods=1).agg(stat)\n",
    "\n",
    "# Specific logic for precipitation: sum is more meaningful than mean.\n",
    "precip_shifted = df_engineered[\"precip\"].shift(1)\n",
    "for window in window_sizes:\n",
    "    df_engineered[f\"precip_roll_{window}d_sum\"] = precip_shifted.rolling(window=window, min_periods=1).sum()\n",
    "\n",
    "\n",
    "shape_before_fe = df_processed.shape[1]\n",
    "shape_after_fe = df_engineered.shape[1]\n",
    "num_new_features = shape_after_fe - shape_before_fe - len(temporal_features_added) - len(domain_features_added)\n",
    "\n",
    "console.print(\n",
    "    \"[bold]Lag and rolling features created.[/bold]\\n\",\n",
    "    f\" - Number of new lag/rolling features: [green]{num_new_features}[/green]\\n\",\n",
    "    f\" - DataFrame shape is now: [green]{df_engineered.shape}[/green]\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8c780",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-categorical\" style=\"display: none;\">Q4.4. Categorical Feature Encoding</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-categorical-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.4. Categorical Feature Encoding</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bf60a",
   "metadata": {},
   "source": [
    "The dataset contains one structured categorical feature, `preciptype`, which currently holds the values `'rain'` and `'none'`. We must convert it into a numerical format so the model can understand it.\n",
    "\n",
    "*   **Chosen method: One-hot Encoding**\n",
    "    *   We will use One-hot Encoding to convert the single `preciptype` column into two new binary (0/1) columns: `preciptype_rain` and `preciptype_none`.\n",
    "*   **Justification:**\n",
    "    *   We didn't choose Dummy Encoding (`k-1` columns) because it preserves the explicit interpretability of each category.\n",
    "    *   While creating `k` columns can introduce perfect multicollinearity (since one column can be perfectly predicted by the others), this is not a concern for the tree-based models. For linear models, the built-in regularization will effectively handle this redundancy. Therefore, prioritizing clarity and interpretability is the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acb98077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Original </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'preciptype'</span><span style=\"font-weight: bold\"> column removed.</span>\n",
       "  - New binary columns added: `<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'preciptype_none'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'preciptype_rain'</span><span style=\"font-weight: bold\">]</span>`\n",
       "  - DataFrame shape is now: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">133</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mOriginal \u001b[0m\u001b[1;32m'preciptype'\u001b[0m\u001b[1m column removed.\u001b[0m\n",
       "  - New binary columns added: `\u001b[1m[\u001b[0m\u001b[32m'preciptype_none'\u001b[0m, \u001b[32m'preciptype_rain'\u001b[0m\u001b[1m]\u001b[0m`\n",
       "  - DataFrame shape is now: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m133\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Sample of new encoded features:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mSample of new encoded features:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "preciptype_none",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "preciptype_rain",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b481f2a0-13a7-4fc7-b6e9-6d4c0ba9bcfa",
       "rows": [
        [
         "2015-01-01 00:00:00",
         "1",
         "0"
        ],
        [
         "2015-01-02 00:00:00",
         "1",
         "0"
        ],
        [
         "2015-01-03 00:00:00",
         "0",
         "1"
        ],
        [
         "2015-01-04 00:00:00",
         "1",
         "0"
        ],
        [
         "2015-01-05 00:00:00",
         "0",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preciptype_none</th>\n",
       "      <th>preciptype_rain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            preciptype_none  preciptype_rain\n",
       "datetime                                    \n",
       "2015-01-01                1                0\n",
       "2015-01-02                1                0\n",
       "2015-01-03                0                1\n",
       "2015-01-04                1                0\n",
       "2015-01-05                0                1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform One-Hot Encoding on the 'preciptype' column\n",
    "df_engineered = pd.get_dummies(\n",
    "    df_engineered,\n",
    "    columns=[\"preciptype\"],\n",
    "    prefix=\"preciptype\",\n",
    "    dtype=int,\n",
    ")\n",
    "\n",
    "\n",
    "encoded_cols = [col for col in df_engineered.columns if \"preciptype_\" in col]\n",
    "console.print(\n",
    "    \"[bold]Original 'preciptype' column removed.[/bold]\\n\",\n",
    "    f\" - New binary columns added: `{encoded_cols}`\\n\",\n",
    "    f\" - DataFrame shape is now: [green]{df_engineered.shape}[/green]\",\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Sample of new encoded features:[/bold]\")\n",
    "display(df_engineered[encoded_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9937a",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-text\" style=\"display: none;\">Q4.5. Unstructured Text Feature Engineering</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-text-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.5. Unstructured Text Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1c3f8",
   "metadata": {},
   "source": [
    "* The dataset contains a `description` column with human-readable weather summaries (e.g., \"Partly cloudy throughout the day with rain in the morning.\"). While our numerical features capture quantitative data, this text may contain valuable qualitative nuances.\n",
    "* To leverage this information, we will employ a standard and robust Natural Language Processing (NLP) pipeline to convert the text into a set of numerical features.\n",
    "*   **Chosen method: TF-IDF + Truncated SVD**\n",
    "    1.  **TF-IDF (Term Frequency-Inverse Document Frequency):** converts the text into a high-dimensional matrix, where each value represents the importance of a word to a specific day's description.\n",
    "    2.  **Truncated SVD (Singular Value Decomposition):** The TF-IDF matrix is very sparse and has too many dimensions (one for each unique word) $\\rightarrow$ Truncated SVD will reduce this matrix into a small number of dense components (5 in our case). In other words, we can understand it as capturing the core \"topics\" or \"concepts\" within the weather descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7106beb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> - Number of new text-based features <span style=\"font-weight: bold\">(</span>SVD components<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7</span>\n",
       " - DataFrame shape is now: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">140</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " - Number of new text-based features \u001b[1m(\u001b[0mSVD components\u001b[1m)\u001b[0m: \u001b[1;32m7\u001b[0m\n",
       " - DataFrame shape is now: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m140\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\">Sample of new text features:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1mSample of new text features:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "text_svd_0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_6",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "cf0f18bd-65f0-49bf-96bb-7ea2c0e3883a",
       "rows": [
        [
         "2015-01-01 00:00:00",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-02 00:00:00",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-03 00:00:00",
         "0.5557979358888956",
         "-0.04911324846334773",
         "0.5853668401226898",
         "0.2807709705081909",
         "-0.24092880961679536",
         "0.06135609242029562",
         "0.4487695354349877"
        ],
        [
         "2015-01-04 00:00:00",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-05 00:00:00",
         "0.40711299084196034",
         "-0.08116996417314216",
         "0.08757481913643936",
         "0.08928190872177197",
         "-0.4937693315397312",
         "-0.18606798327226198",
         "-0.15248596831779207"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_svd_0</th>\n",
       "      <th>text_svd_1</th>\n",
       "      <th>text_svd_2</th>\n",
       "      <th>text_svd_3</th>\n",
       "      <th>text_svd_4</th>\n",
       "      <th>text_svd_5</th>\n",
       "      <th>text_svd_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>0.555798</td>\n",
       "      <td>-0.049113</td>\n",
       "      <td>0.585367</td>\n",
       "      <td>0.280771</td>\n",
       "      <td>-0.240929</td>\n",
       "      <td>0.061356</td>\n",
       "      <td>0.448770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>0.407113</td>\n",
       "      <td>-0.081170</td>\n",
       "      <td>0.087575</td>\n",
       "      <td>0.089282</td>\n",
       "      <td>-0.493769</td>\n",
       "      <td>-0.186068</td>\n",
       "      <td>-0.152486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_svd_0  text_svd_1  text_svd_2  text_svd_3  text_svd_4  \\\n",
       "datetime                                                                 \n",
       "2015-01-01    0.879053   -0.176581   -0.320034   -0.237530   -0.112032   \n",
       "2015-01-02    0.879053   -0.176581   -0.320034   -0.237530   -0.112032   \n",
       "2015-01-03    0.555798   -0.049113    0.585367    0.280771   -0.240929   \n",
       "2015-01-04    0.879053   -0.176581   -0.320034   -0.237530   -0.112032   \n",
       "2015-01-05    0.407113   -0.081170    0.087575    0.089282   -0.493769   \n",
       "\n",
       "            text_svd_5  text_svd_6  \n",
       "datetime                            \n",
       "2015-01-01   -0.016034    0.155117  \n",
       "2015-01-02   -0.016034    0.155117  \n",
       "2015-01-03    0.061356    0.448770  \n",
       "2015-01-04   -0.016034    0.155117  \n",
       "2015-01-05   -0.186068   -0.152486  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_data = df_raw[[\"description\"]].copy()\n",
    "df_engineered = df_engineered.join(text_data)\n",
    "\n",
    "# --- Define the NLP processing pipeline ---\n",
    "n_svd_components = 7  # The `description` field has 26 distinct features, mostly repetitive, so 7 is robust for our case\n",
    "nlp_pipeline = Pipeline([\n",
    "    (\n",
    "        \"tfidf\",\n",
    "        TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 2),  # Capture both single words and two-word phrases\n",
    "            max_features=1000,  # Limit vocabulary to the top 1000 terms\n",
    "        ),\n",
    "    ),\n",
    "    (\"svd\", TruncatedSVD(n_components=n_svd_components, random_state=config.GLOBAL_RANDOM_SEED)),\n",
    "])\n",
    "\n",
    "text_features = nlp_pipeline.fit_transform(df_engineered[\"description\"])\n",
    "\n",
    "text_feature_names = [f\"text_svd_{i}\" for i in range(n_svd_components)]\n",
    "df_text_features = pd.DataFrame(text_features, index=df_engineered.index, columns=text_feature_names)\n",
    "\n",
    "df_engineered = pd.concat([df_engineered, df_text_features], axis=1)\n",
    "\n",
    "df_engineered.drop(columns=[\"description\"], inplace=True)\n",
    "\n",
    "\n",
    "console.print(\n",
    "    f\" - Number of new text-based features (SVD components): [green]{n_svd_components}[/green]\\n\",\n",
    "    f\" - DataFrame shape is now: [green]{df_engineered.shape}[/green]\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Sample of new text features:[/bold]\")\n",
    "display(df_engineered[text_feature_names].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5607ad8b",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"fe-final-verification\" style=\"display: none;\">Q4.6. Final Verification & NaN Handling</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"fe-final-verification-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q4.6. Final Verification & NaN Handling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265010e",
   "metadata": {},
   "source": [
    "Now we verify the structure and acknowledge the presence of `NaN` values. These `NaN`s are an expected and correct outcome of the lag and rolling window operations. The initial rows of the DataFrame lack sufficient historical data for these calculations (e.g., since the first day has no \"day before\" to lag from).\n",
    "\n",
    "The number of rows containing these `NaN`s is determined by our longest lookback period. These rows will be dropped in the next section, **after** we create our target variables. This ensures a perfect, leak-free alignment between our features (`X`) and the values we aim to predict (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9598392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Longest lag period: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> days\n",
       "Longest rolling window: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> days\n",
       "==&gt; Maximum lookback period: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">28</span> days\n",
       "\n",
       "This means the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> rows will contain NaN values and will be dropped during the modeling prep stage.\n",
       "<span style=\"font-weight: bold\">Final Shape of DataFrame:</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">140</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Longest lag period: \u001b[1;36m28\u001b[0m days\n",
       "Longest rolling window: \u001b[1;36m28\u001b[0m days\n",
       "==> Maximum lookback period: \u001b[1;33m28\u001b[0m days\n",
       "\n",
       "This means the first \u001b[1;36m28\u001b[0m rows will contain NaN values and will be dropped during the modeling prep stage.\n",
       "\u001b[1mFinal Shape of DataFrame:\u001b[0m \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m140\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">First </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\"> Rows (showing initial NaNs):</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mFirst \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m Rows \u001b[0m\u001b[1m(\u001b[0m\u001b[1mshowing initial NaNs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "datetime",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "tempmax",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tempmin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feelslikemax",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feelslikemin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feelslike",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precipprob",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "precipcover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeedmean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeedmin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "winddir",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "visibility",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "uvindex",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "severerisk",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "moonphase",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "month",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "week_of_year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "day_of_year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "sin_day_of_year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cos_day_of_year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sin_month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cos_month",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_sin_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_cos_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_sin_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fourier_cos_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "daylight_hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_range",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_point_deficit",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sealevelpressure_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windgust_lag_28",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "humidity_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "windspeed_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "solarradiation_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dew_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_7d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_7d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_14d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_14d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_28d_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cloudcover_roll_28d_std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_roll_7d_sum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_roll_14d_sum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_roll_28d_sum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "preciptype_none",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "preciptype_rain",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_svd_0",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text_svd_6",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d0c2221f-21b3-43d7-b17a-15f7c5fed09a",
       "rows": [
        [
         "2015-01-01 00:00:00",
         "31.0",
         "23.0",
         "26.6",
         "31.4",
         "23.0",
         "26.8",
         "17.9",
         "60.3",
         "0.0",
         "0",
         "0.0",
         "20.5",
         "15.4",
         "7.9",
         "2.9",
         "27.3",
         "1012.5",
         "53.8",
         "8.9",
         "230.6",
         "8",
         "0.0",
         "0.36",
         "2015",
         "1",
         "1",
         "1",
         "0.01716632975470737",
         "0.9998526477050269",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.034327600513243496",
         "0.9994106342455052",
         "0.05147875477034653",
         "0.9986740898848305",
         "11.504722222222222",
         "8.0",
         "8.700000000000003",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "1",
         "0",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-02 00:00:00",
         "30.0",
         "20.0",
         "25.0",
         "30.4",
         "20.0",
         "25.1",
         "15.8",
         "57.2",
         "0.0",
         "0",
         "0.0",
         "21.6",
         "13.0",
         "7.2",
         "2.9",
         "331.6",
         "1013.0",
         "66.3",
         "10.2",
         "186.2",
         "7",
         "0.0",
         "0.39",
         "2015",
         "1",
         "1",
         "2",
         "0.034327600513243496",
         "0.9994106342455052",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.06861473800213404",
         "0.9976432316860063",
         "0.1028209971373604",
         "0.9946998756145891",
         "11.507222222222222",
         "10.0",
         "9.2",
         "26.6",
         null,
         null,
         null,
         null,
         null,
         "60.3",
         null,
         null,
         null,
         null,
         null,
         "230.6",
         null,
         null,
         null,
         null,
         null,
         "0.0",
         null,
         null,
         null,
         null,
         null,
         "15.4",
         null,
         null,
         null,
         null,
         null,
         "53.8",
         null,
         null,
         null,
         null,
         null,
         "17.9",
         null,
         null,
         null,
         null,
         null,
         "1012.5",
         null,
         null,
         null,
         null,
         null,
         "20.5",
         null,
         null,
         null,
         null,
         null,
         "26.6",
         null,
         "26.6",
         null,
         "26.6",
         null,
         "60.3",
         null,
         "60.3",
         null,
         "60.3",
         null,
         "15.4",
         null,
         "15.4",
         null,
         "15.4",
         null,
         "230.6",
         null,
         "230.6",
         null,
         "230.6",
         null,
         "17.9",
         null,
         "17.9",
         null,
         "17.9",
         null,
         "53.8",
         null,
         "53.8",
         null,
         "53.8",
         null,
         "0.0",
         "0.0",
         "0.0",
         "1",
         "0",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-03 00:00:00",
         "32.0",
         "23.0",
         "26.8",
         "33.5",
         "23.0",
         "27.4",
         "19.0",
         "63.7",
         "0.2",
         "100",
         "8.33",
         "23.8",
         "11.2",
         "7.8",
         "1.8",
         "9.5",
         "1012.0",
         "52.7",
         "9.8",
         "185.8",
         "8",
         "0.0",
         "0.43",
         "2015",
         "1",
         "1",
         "3",
         "0.05147875477034653",
         "0.9986740898848305",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.1028209971373604",
         "0.9946998756145891",
         "0.15389057670406178",
         "0.9880878960910772",
         "11.509722222222223",
         "9.0",
         "7.800000000000001",
         "25.0",
         "26.6",
         null,
         null,
         null,
         null,
         "57.2",
         "60.3",
         null,
         null,
         null,
         null,
         "186.2",
         "230.6",
         null,
         null,
         null,
         null,
         "0.0",
         "0.0",
         null,
         null,
         null,
         null,
         "13.0",
         "15.4",
         null,
         null,
         null,
         null,
         "66.3",
         "53.8",
         null,
         null,
         null,
         null,
         "15.8",
         "17.9",
         null,
         null,
         null,
         null,
         "1013.0",
         "1012.5",
         null,
         null,
         null,
         null,
         "21.6",
         "20.5",
         null,
         null,
         null,
         null,
         "25.8",
         "1.1313708498984771",
         "25.8",
         "1.1313708498984771",
         "25.8",
         "1.1313708498984771",
         "58.75",
         "2.192031021678293",
         "58.75",
         "2.192031021678293",
         "58.75",
         "2.192031021678293",
         "14.2",
         "1.6970562748477136",
         "14.2",
         "1.6970562748477136",
         "14.2",
         "1.6970562748477136",
         "208.39999999999998",
         "31.395541084682705",
         "208.39999999999998",
         "31.395541084682705",
         "208.39999999999998",
         "31.395541084682705",
         "16.85",
         "1.4849242404917495",
         "16.85",
         "1.4849242404917495",
         "16.85",
         "1.4849242404917495",
         "60.05",
         "8.838834764831844",
         "60.05",
         "8.838834764831844",
         "60.05",
         "8.838834764831844",
         "0.0",
         "0.0",
         "0.0",
         "0",
         "1",
         "0.5557979358888956",
         "-0.04911324846334773",
         "0.5853668401226898",
         "0.2807709705081909",
         "-0.24092880961679536",
         "0.06135609242029562",
         "0.4487695354349877"
        ],
        [
         "2015-01-04 00:00:00",
         "32.0",
         "24.0",
         "27.1",
         "34.8",
         "24.0",
         "28.3",
         "21.3",
         "71.7",
         "0.0",
         "0",
         "0.0",
         "22.7",
         "10.8",
         "6.3",
         "2.9",
         "237.4",
         "1010.2",
         "59.3",
         "8.4",
         "125.8",
         "4",
         "0.0",
         "0.46",
         "2015",
         "1",
         "1",
         "4",
         "0.06861473800213404",
         "0.9976432316860063",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.13690605792347524",
         "0.990584035457797",
         "0.2045520661262008",
         "0.9788556850953578",
         "11.5125",
         "8.0",
         "5.800000000000001",
         "26.8",
         "25.0",
         "26.6",
         null,
         null,
         null,
         "63.7",
         "57.2",
         "60.3",
         null,
         null,
         null,
         "185.8",
         "186.2",
         "230.6",
         null,
         null,
         null,
         "0.2",
         "0.0",
         "0.0",
         null,
         null,
         null,
         "11.2",
         "13.0",
         "15.4",
         null,
         null,
         null,
         "52.7",
         "66.3",
         "53.8",
         null,
         null,
         null,
         "19.0",
         "15.8",
         "17.9",
         null,
         null,
         null,
         "1012.0",
         "1013.0",
         "1012.5",
         null,
         null,
         null,
         "23.8",
         "21.6",
         "20.5",
         null,
         null,
         null,
         "26.133333333333336",
         "0.9865765724632504",
         "26.133333333333336",
         "0.9865765724632504",
         "26.133333333333336",
         "0.9865765724632504",
         "60.4",
         "3.2511536414017725",
         "60.4",
         "3.2511536414017725",
         "60.4",
         "3.2511536414017725",
         "13.200000000000001",
         "2.1071307505705477",
         "13.200000000000001",
         "2.1071307505705477",
         "13.200000000000001",
         "2.1071307505705477",
         "200.86666666666667",
         "25.750598698541605",
         "200.86666666666667",
         "25.750598698541605",
         "200.86666666666667",
         "25.750598698541605",
         "17.566666666666666",
         "1.6258331197676261",
         "17.566666666666666",
         "1.6258331197676261",
         "17.566666666666666",
         "1.6258331197676261",
         "57.6",
         "7.554468876102408",
         "57.6",
         "7.554468876102408",
         "57.6",
         "7.554468876102408",
         "0.2",
         "0.2",
         "0.2",
         "1",
         "0",
         "0.8790533296084557",
         "-0.1765811233047667",
         "-0.3200342674104343",
         "-0.23752952737489877",
         "-0.11203227267624385",
         "-0.016033608113933895",
         "0.1551168024980476"
        ],
        [
         "2015-01-05 00:00:00",
         "30.6",
         "24.9",
         "26.7",
         "33.2",
         "24.9",
         "27.7",
         "22.1",
         "76.4",
         "6.0",
         "100",
         "4.17",
         "24.8",
         "11.6",
         "5.5",
         "1.1",
         "168.5",
         "1010.0",
         "78.0",
         "9.1",
         "163.4",
         "8",
         "0.0",
         "0.5",
         "2015",
         "1",
         "2",
         "5",
         "0.08573050015569435",
         "0.9963183634476755",
         "0.49999999999999994",
         "0.8660254037844387",
         "0.17082974322534417",
         "0.985300562686109",
         "0.2546711202412287",
         "0.9670277247913204",
         "11.515555555555556",
         "5.700000000000003",
         "4.599999999999998",
         "27.1",
         "26.8",
         "25.0",
         null,
         null,
         null,
         "71.7",
         "63.7",
         "57.2",
         null,
         null,
         null,
         "125.8",
         "185.8",
         "186.2",
         null,
         null,
         null,
         "0.0",
         "0.2",
         "0.0",
         null,
         null,
         null,
         "10.8",
         "11.2",
         "13.0",
         null,
         null,
         null,
         "59.3",
         "52.7",
         "66.3",
         null,
         null,
         null,
         "21.3",
         "19.0",
         "15.8",
         null,
         null,
         null,
         "1010.2",
         "1012.0",
         "1013.0",
         null,
         null,
         null,
         "22.7",
         "23.8",
         "21.6",
         null,
         null,
         null,
         "26.375",
         "0.9394147114027979",
         "26.375",
         "0.9394147114027979",
         "26.375",
         "0.9394147114027979",
         "63.225",
         "6.242528867908156",
         "63.225",
         "6.242528867908156",
         "63.225",
         "6.242528867908156",
         "12.6",
         "2.0976176963403024",
         "12.6",
         "2.0976176963403024",
         "12.6",
         "2.0976176963403024",
         "182.1",
         "43.021080104215564",
         "182.1",
         "43.021080104215564",
         "182.1",
         "43.021080104215564",
         "18.5",
         "2.2905603390145974",
         "18.5",
         "2.2905603390145974",
         "18.5",
         "2.2905603390145974",
         "58.025",
         "6.226489112386422",
         "58.025",
         "6.226489112386422",
         "58.025",
         "6.226489112386422",
         "0.2",
         "0.2",
         "0.2",
         "0",
         "1",
         "0.40711299084196034",
         "-0.08116996417314216",
         "0.08757481913643936",
         "0.08928190872177197",
         "-0.4937693315397312",
         "-0.18606798327226198",
         "-0.15248596831779207"
        ]
       ],
       "shape": {
        "columns": 140,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempmax</th>\n",
       "      <th>tempmin</th>\n",
       "      <th>temp</th>\n",
       "      <th>feelslikemax</th>\n",
       "      <th>feelslikemin</th>\n",
       "      <th>feelslike</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precip</th>\n",
       "      <th>precipprob</th>\n",
       "      <th>...</th>\n",
       "      <th>precip_roll_28d_sum</th>\n",
       "      <th>preciptype_none</th>\n",
       "      <th>preciptype_rain</th>\n",
       "      <th>text_svd_0</th>\n",
       "      <th>text_svd_1</th>\n",
       "      <th>text_svd_2</th>\n",
       "      <th>text_svd_3</th>\n",
       "      <th>text_svd_4</th>\n",
       "      <th>text_svd_5</th>\n",
       "      <th>text_svd_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>31.4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>17.9</td>\n",
       "      <td>60.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>57.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>32.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>33.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>63.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555798</td>\n",
       "      <td>-0.049113</td>\n",
       "      <td>0.585367</td>\n",
       "      <td>0.280771</td>\n",
       "      <td>-0.240929</td>\n",
       "      <td>0.061356</td>\n",
       "      <td>0.448770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>32.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>34.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>21.3</td>\n",
       "      <td>71.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879053</td>\n",
       "      <td>-0.176581</td>\n",
       "      <td>-0.320034</td>\n",
       "      <td>-0.237530</td>\n",
       "      <td>-0.112032</td>\n",
       "      <td>-0.016034</td>\n",
       "      <td>0.155117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>30.6</td>\n",
       "      <td>24.9</td>\n",
       "      <td>26.7</td>\n",
       "      <td>33.2</td>\n",
       "      <td>24.9</td>\n",
       "      <td>27.7</td>\n",
       "      <td>22.1</td>\n",
       "      <td>76.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.407113</td>\n",
       "      <td>-0.081170</td>\n",
       "      <td>0.087575</td>\n",
       "      <td>0.089282</td>\n",
       "      <td>-0.493769</td>\n",
       "      <td>-0.186068</td>\n",
       "      <td>-0.152486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tempmax  tempmin  temp  feelslikemax  feelslikemin  feelslike  \\\n",
       "datetime                                                                    \n",
       "2015-01-01     31.0     23.0  26.6          31.4          23.0       26.8   \n",
       "2015-01-02     30.0     20.0  25.0          30.4          20.0       25.1   \n",
       "2015-01-03     32.0     23.0  26.8          33.5          23.0       27.4   \n",
       "2015-01-04     32.0     24.0  27.1          34.8          24.0       28.3   \n",
       "2015-01-05     30.6     24.9  26.7          33.2          24.9       27.7   \n",
       "\n",
       "             dew  humidity  precip  precipprob  ...  precip_roll_28d_sum  \\\n",
       "datetime                                        ...                        \n",
       "2015-01-01  17.9      60.3     0.0           0  ...                  NaN   \n",
       "2015-01-02  15.8      57.2     0.0           0  ...                  0.0   \n",
       "2015-01-03  19.0      63.7     0.2         100  ...                  0.0   \n",
       "2015-01-04  21.3      71.7     0.0           0  ...                  0.2   \n",
       "2015-01-05  22.1      76.4     6.0         100  ...                  0.2   \n",
       "\n",
       "            preciptype_none  preciptype_rain  text_svd_0  text_svd_1  \\\n",
       "datetime                                                               \n",
       "2015-01-01                1                0    0.879053   -0.176581   \n",
       "2015-01-02                1                0    0.879053   -0.176581   \n",
       "2015-01-03                0                1    0.555798   -0.049113   \n",
       "2015-01-04                1                0    0.879053   -0.176581   \n",
       "2015-01-05                0                1    0.407113   -0.081170   \n",
       "\n",
       "            text_svd_2  text_svd_3  text_svd_4  text_svd_5  text_svd_6  \n",
       "datetime                                                                \n",
       "2015-01-01   -0.320034   -0.237530   -0.112032   -0.016034    0.155117  \n",
       "2015-01-02   -0.320034   -0.237530   -0.112032   -0.016034    0.155117  \n",
       "2015-01-03    0.585367    0.280771   -0.240929    0.061356    0.448770  \n",
       "2015-01-04   -0.320034   -0.237530   -0.112032   -0.016034    0.155117  \n",
       "2015-01-05    0.087575    0.089282   -0.493769   -0.186068   -0.152486  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_lag = max(lag_periods)\n",
    "max_rolling_window = max(window_sizes)\n",
    "max_lookback = max(max_lag, max_rolling_window)\n",
    "\n",
    "console.print(\n",
    "    f\"Longest lag period: [cyan]{max_lag}[/cyan] days\\n\",\n",
    "    f\"Longest rolling window: [cyan]{max_rolling_window}[/cyan] days\\n\",\n",
    "    f\"==> Maximum lookback period: [bold yellow]{max_lookback}[/bold yellow] days\\n\",\n",
    "    f\"\\nThis means the first {max_lookback} rows will contain NaN values and will be dropped during the modeling prep stage.\\n\",\n",
    "    f\"[bold]Final Shape of DataFrame:[/bold] [green]{df_engineered.shape}[/green]\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "console.print(\"[bold]First 5 Rows (showing initial NaNs):[/bold]\")\n",
    "display(df_engineered.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bc8ba",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H1 FOR OUTLINE VIEW -->\n",
    "<h1 id=\"modeling\" style=\"display: none;\">Q5. Modeling Strategy & Evaluation</h1>\n",
    "<!-- VISIBLE H1 -->\n",
    "<h1 id=\"modeling-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: white; font-size: 22px; font-weight: bold; background-color: #0771A4; border-radius: 4px; padding: 12px 0px 12px 15px; margin-top: 20px;\">Q5. Modeling Strategy & Evaluation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94524b1f",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"modeling-foundational-setup\" style=\"display: none;\">Q5.1. Forecasting Strategies, Target Definition & Data Splitting</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"modeling-foundational-setup-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.1. Forecasting Strategies, Target Definition & Data Splitting</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed15a4",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"modeling-strategy\" style=\"display: none;\">Q5.1.1. Defining the Forecasting Strategy</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"modeling-strategy-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.1.1. Defining the Forecasting Strategy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f72ec",
   "metadata": {},
   "source": [
    "### Forecasting Architectures\n",
    "\n",
    "**1. Recursive strategy:**\n",
    "*   **Method:** A single model is trained to predict only one step ahead (`t+1`). To forecast `t+2`, the model's own prediction for `t+1` is fed back into its input features. This process is repeated for each subsequent step.\n",
    "*   **Drawback:** This approach is highly susceptible to **error accumulation**. Any small error in the `t+1` forecast becomes part of the input for the `t+2` forecast, and these errors compound, often leading to a rapid degradation in performance for longer horizons.\n",
    "\n",
    "**2. Multi-output strategy:**\n",
    "*   **Method:** A single, more complex model is trained to predict all five forecast horizons (`t+1` through `t+5`) simultaneously in one forward pass.\n",
    "*   **Drawback:** This model is a \"generalist.\" While computationally efficient, it often struggles to optimize for any single horizon, resulting in performance that is good on average but rarely the best for any specific day.\n",
    "\n",
    "**3. Direct strategy (We choose this):**\n",
    "*   **Method:** We train a separate, independent, specialist model for each forecast horizon. The model for `t+3`, for example, is trained exclusively on data pairs of `(Features at time t, Target at time t+3)`.\n",
    "*   **Justification:** This is the most robust and potentially highest-performing strategy for this problem. It avoids error accumulation and allows each model to become an expert at its specific task, learning the unique feature relationships that are most important for that particular forecast distance. While it requires training multiple models, the gain in accuracy and reliability is a worthwhile trade-off, especially since the amount of training data for the daily dataset is relatively small, and  only have to predict 5 models (5 horizons).\n",
    "\n",
    "### What to Predict?\n",
    "\n",
    "**1. Predicting the *direct value* (We choose this):** The most straightforward and robust approach is to predict the final temperature value directly. The previous feature engineering step (lags, rolling windows, cyclical features) has already provided the model with most necessary information about trends, seasonality, and recent changes, allowing the model to learn the complete mapping from features to the final target value end-to-end.\n",
    "\n",
    "**2. Predicting the *delta*:** We could predict the *change* in temperature from day `t` to `t+h`. The final forecast is then `Temp_t + Predicted_Delta`. This can be effective if the series is strongly trending, but it can be unstable. For temperature, especially for a relatively \"stable\" area like Ho Chi Minh city, the deltas wouldn't be clear, doesn't have strong strends (it has some, as noted in EDA above, but not clear enough of a signal), which makes it hard for the model to reliably predict the change in temperature from day to day with great accuracy.\n",
    "\n",
    "**3. Predicting the *residual*:** After de-trending and de-seasonalizing (as in our STL decomposition), we could model the remaining \"noise\" or residual. It's more common technique in classical time-series analysis but often more complex to implement in ML effectively, and same with predicting the delta approach above, the residual don't have any strong, clear trends or big enough of a difference for the model to reliably learn the pattern and predict.\n",
    "\n",
    "$\\implies$ We will implement a **direct forecasting strategy** where we train **five specialist models**, each tasked with predicting the **absolute temperature value** for its assigned horizon (`t+1` through `t+5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a935c8b",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H3 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"modeling-target-creation\" style=\"display: none;\">Q5.1.2. Target Variable Creation & Final Alignment</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"modeling-target-creation-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.1.2. Target Variable Creation & Final Alignment</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343e3885",
   "metadata": {},
   "source": [
    "Since we're using the direct forecasting strategy, we need to create the five distinct target variables, done by shifting the `temp` column backwards in time. For example, the target for day `t`'s features will be the actual temperature from day `t+1`, `t+2`, and so on.\n",
    "\n",
    "This will introduce `NaN` values at the end of the DataFrame (as there is no future data for the last few days) and at the beginning (from the lag/rolling features). We will then need to drop all rows containing any `NaN` values, ensuring every row in the final DataFrame is complete and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6d76ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Shape before final alignment: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3927</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">145</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "Shape after final alignment:  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3894</span><span style=\"color: #008000; text-decoration-color: #008000\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">145</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "Rows dropped due to lookback/lookahead: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">33</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Shape before final alignment: \u001b[1;32m(\u001b[0m\u001b[1;32m3927\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m145\u001b[0m\u001b[1;32m)\u001b[0m\n",
       "Shape after final alignment:  \u001b[1;32m(\u001b[0m\u001b[1;32m3894\u001b[0m\u001b[32m, \u001b[0m\u001b[1;32m145\u001b[0m\u001b[1;32m)\u001b[0m\n",
       "Rows dropped due to lookback/lookahead: \u001b[1;31m33\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_model_ready = df_engineered.copy()\n",
    "\n",
    "# --- 1. Create the Multi-Horizon Target Variables ---\n",
    "for h in config.HORIZONS:\n",
    "    df_model_ready[f\"target_temp_t+{h}\"] = df_model_ready[\"temp\"].shift(-h)\n",
    "\n",
    "# --- 2. Final Alignment via NaN Removal ---\n",
    "shape_before_align = df_model_ready.shape\n",
    "df_model_ready.dropna(inplace=True)\n",
    "shape_after_align = df_model_ready.shape\n",
    "\n",
    "rows_dropped = shape_before_align[0] - shape_after_align[0]\n",
    "\n",
    "console.print(\n",
    "    f\"Shape before final alignment: [green]{shape_before_align}[/green]\\n\",\n",
    "    f\"Shape after final alignment:  [green]{shape_after_align}[/green]\\n\",\n",
    "    f\"Rows dropped due to lookback/lookahead: [red]{rows_dropped}[/red]\",\n",
    "    sep=\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa274da2",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H3 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"modeling-final-split\" style=\"display: none;\">Q5.1.3. The Final Train-Test Split</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"modeling-final-split-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.1.3. The Final Train-Test Split</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef7821",
   "metadata": {},
   "source": [
    "*   **Training Set (`_train`):** This data (the first 85%) is used for all model training, cross-validation, and hyperparameter tuning.\n",
    "*   **Test Set (`_test`):** This data (the final 15%) is held out and completely untouched until the very end of the project. The set simulates the future, unseen datawhich the top model model will be evaluated on this set *once* to produce its final, unbiased performance score.\n",
    "\n",
    "We adopt this **Train/Test with Cross-Validation** approach over a fixed Train/Validation/Test split:\n",
    "* A fixed validation set can be small and idiosyncratic, leading a model to overfit to its specific patterns\n",
    "* By using `TimeSeriesSplit` cross-validation on the larger training set, we evaluate our models across multiple, diverse time periods, leading to a much more robust and reliable estimate of their true generalization performance.\n",
    "* After evaluating the models to choose the most suitable one, we will train that model once on the entire training set, providing it with more recent data (closer to the test set). This is even more important because weather/climate is non-stationary, we may have data drift and/or concept drift, so the model may perform differently (usually worse) as its training data and test data grows further apart. More on that in Q7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10dc34da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> - Total samples:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3894</span>\n",
       " - Training set size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3310</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">85</span>%<span style=\"font-weight: bold\">)</span>\n",
       " - Test set size:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">584</span> <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " - Total samples:     \u001b[1;36m3894\u001b[0m\n",
       " - Training set size: \u001b[1;36m3310\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m85\u001b[0m%\u001b[1m)\u001b[0m\n",
       " - Test set size:     \u001b[1;36m584\u001b[0m \u001b[1m(\u001b[0m\u001b[1;36m15\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">───────────────────────────────────────────── </span><span style=\"font-weight: bold\">Time Range of the sets:</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m───────────────────────────────────────────── \u001b[0m\u001b[1mTime Range of the sets:\u001b[0m\u001b[92m ─────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training Set Time Range:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>\n",
       "Test Set Time Range:      <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training Set Time Range:  \u001b[1;36m2015\u001b[0m-\u001b[1;36m01\u001b[0m-\u001b[1;36m29\u001b[0m to \u001b[1;36m2024\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m20\u001b[0m\n",
       "Test Set Time Range:      \u001b[1;36m2024\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m21\u001b[0m to \u001b[1;36m2025\u001b[0m-\u001b[1;36m09\u001b[0m-\u001b[1;36m26\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Define Feature and Target Columns ---\n",
    "target_cols = [f\"target_temp_t+{h}\" for h in config.HORIZONS]\n",
    "\n",
    "# We must drop the original 'temp' column as is now a source of data leakage (because it's\n",
    "# from the same day as the target values we are trying to predict)\n",
    "features_to_drop = target_cols + [\"temp\"]\n",
    "\n",
    "# All remaining columns are predictors.\n",
    "feature_cols = [col for col in df_model_ready.columns if col not in features_to_drop]\n",
    "\n",
    "X = df_model_ready[feature_cols]\n",
    "y = df_model_ready[target_cols]\n",
    "\n",
    "# --- 2. Perform the Chronological Split ---\n",
    "n_total = len(X)\n",
    "n_test = int(n_total * config.TEST_SIZE)\n",
    "n_train = n_total - n_test\n",
    "\n",
    "X_train_full = X.iloc[:n_train]\n",
    "y_train = y.iloc[:n_train]\n",
    "\n",
    "X_test_full = X.iloc[n_train:]\n",
    "y_test = y.iloc[n_train:]\n",
    "\n",
    "\n",
    "console.print(\n",
    "    f\" - Total samples:     {n_total}\\n\",\n",
    "    f\" - Training set size: {n_train} ({1 - config.TEST_SIZE:.0%})\\n\",\n",
    "    f\" - Test set size:     {n_test} ({config.TEST_SIZE:.0%})\",\n",
    "    sep=\"\",\n",
    ")\n",
    "\n",
    "console.rule(\"[bold]Time Range of the sets:[/bold]\", align=\"center\")\n",
    "console.print(\n",
    "    f\"Training Set Time Range:  {X_train_full.index.min().date()} to {X_train_full.index.max().date()}\\n\",\n",
    "    f\"Test Set Time Range:      {X_test_full.index.min().date()} to {X_test_full.index.max().date()}\",\n",
    "    sep=\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ec6e0",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H3 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"modeling-feature-subsets\" style=\"display: none;\">Q5.1.4. Creating Specialized Feature Subsets</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"modeling-feature-subsets-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.1.4. Creating Specialized Feature Subsets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b354cc",
   "metadata": {},
   "source": [
    "As established in our EDA, severe multicollinearity poses a significant challenge for linear models. To address this, we now create two distinct, purpose-built feature sets for our upcoming model bake-off:\n",
    "\n",
    "1.  **`X_train_tree` / `X_test_tree`:** A comprehensive (full) feature set containing all engineered predictors. This set is designed for tree-based models (like LightGBM, XGBoost) and other non-linear architectures which are inherently robust to multicollinearity.\n",
    "\n",
    "2.  **`X_train_linear` / `X_test_linear`:** A carefully curated, parsimonious feature set for our linear models (`RidgeCV`, `ElasticNetCV`, etc.). This subset will be created with data-driven methodology that penalizes feature redundancy.\n",
    "\n",
    "**We choose: `MultiTaskLassoCV`**\n",
    "\n",
    "*   **What it is:** Lasso (Least Absolute Shrinkage and Selection Operator) is a form of linear regression that includes an $L_1$ penalty term. This penalty forces the coefficients of less important or redundant features to shrink to exactly zero, effectively performing automated feature selection.\n",
    "*   **Why `MultiTask`?** Standard Lasso works on a single target. Since we have five forecast horizons, `MultiTaskLassoCV` jointly fits models for all five targets simultaneously. It applies a group penalty that encourages the selection of a common set of features that are useful *across all horizons*. This is ideal for finding a single, robust feature set for our entire direct forecasting system. We could use `LassoCV` for each of the horizon, but that increases uncessary complexity, reduce interpretability, all without much (if any) meaningful gains.\n",
    "*   **Why `CV`?** The `CV` (Cross-Validation) component automatically finds the optimal strength of the regularization penalty (`alpha`) using our `TimeSeriesSplit` cross-validation strategy, ensuring the feature selection process is itself robust and leak-free.\n",
    "\n",
    "In essence, by fitting `MultiTaskLassoCV` on the full training set, we are asking the data itself to identify the most potent and non-redundant subset of predictors, creating a lean, powerful feature set perfectly tailored for our linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ab49fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> - Tree-based model feature count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">139</span>\n",
       " - Linear model feature count:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41</span> <span style=\"font-weight: bold\">(</span>selected by LASSO<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       " - Tree-based model feature count: \u001b[1;36m139\u001b[0m\n",
       " - Linear model feature count:     \u001b[1;36m41\u001b[0m \u001b[1m(\u001b[0mselected by LASSO\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_tree = X_train_full\n",
    "X_test_tree = X_test_full\n",
    "\n",
    "# --- Use MultiTaskLassoCV to select a low-collinearity subset for linear models ---\n",
    "# Fit only on the training data\n",
    "lasso_pipeline = Pipeline([\n",
    "    (\"scaler\", RobustScaler()),\n",
    "    (\n",
    "        \"lasso\",\n",
    "        MultiTaskLassoCV(\n",
    "            cv=TimeSeriesSplit(n_splits=5),\n",
    "            random_state=config.GLOBAL_RANDOM_SEED,\n",
    "            n_jobs=-1,\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train_full, y_train)\n",
    "\n",
    "# Identify features with non-zero coefficients.\n",
    "coefficients = lasso_pipeline.named_steps[\"lasso\"].coef_\n",
    "selected_features_mask = np.abs(coefficients).sum(axis=0) > 0\n",
    "linear_feature_cols = X_train_full.columns[selected_features_mask].tolist()\n",
    "\n",
    "X_train_linear = X_train_full[linear_feature_cols]\n",
    "X_test_linear = X_test_full[linear_feature_cols]\n",
    "\n",
    "\n",
    "console.print(\n",
    "    f\" - Tree-based model feature count: [cyan]{X_train_tree.shape[1]}[/cyan]\\n\",\n",
    "    f\" - Linear model feature count:     [cyan]{X_train_linear.shape[1]}[/cyan] (selected by LASSO)\",\n",
    "    sep=\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca043e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           LASSO Feature Selection Summary           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Feature Category         </span>┃<span style=\"font-weight: bold\"> Kept </span>┃<span style=\"font-weight: bold\"> Dropped </span>┃<span style=\"font-weight: bold\"> Total </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Raw Predictor            </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  13  </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    9    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">  22   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Temporal (Cyclical)      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  4   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   8   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Text (SVD)               </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  3   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   7   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Humidity)           </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  1   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    5    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Dew)                </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  2   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Cloudcover)         </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  1   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    5    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Dew)            </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    6    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Cloudcover)     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  2   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Precip)             </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  6   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    0    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Sealevelpressure)   </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  2   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Windgust)           </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    6    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Windspeed)          </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    6    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Temp)               </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  3   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    3    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Lag (Solarradiation)     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    6    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Windspeed)      </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  1   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    5    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Temp)           </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  2   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Humidity)       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    6    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Solarradiation) </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  1   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    5    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   6   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Temporal (Basic)         </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    4    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   4   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Domain-Specific          </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    3    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   3   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Rolling (Precip)         </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    3    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   3   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Categorical              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  0   </span>│<span style=\"color: #800000; text-decoration-color: #800000\">    2    </span>│<span style=\"color: #000080; text-decoration-color: #000080\">   2   </span>│\n",
       "└──────────────────────────┴──────┴─────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           LASSO Feature Selection Summary           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFeature Category        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKept\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDropped\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTotal\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mRaw Predictor           \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 13 \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   9   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m 22  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTemporal (Cyclical)     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 4  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  8  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mText (SVD)              \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 3  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  7  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Humidity)          \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 1  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   5   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Dew)               \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 2  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Cloudcover)        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 1  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   5   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Dew)           \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   6   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Cloudcover)    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 2  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Precip)            \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 6  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   0   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Sealevelpressure)  \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 2  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Windgust)          \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   6   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Windspeed)         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   6   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Temp)              \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 3  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   3   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mLag (Solarradiation)    \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   6   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Windspeed)     \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 1  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   5   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Temp)          \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 2  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Humidity)      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   6   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Solarradiation)\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 1  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   5   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  6  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTemporal (Basic)        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   4   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  4  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDomain-Specific         \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   3   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  3  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mRolling (Precip)        \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   3   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  3  \u001b[0m\u001b[34m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCategorical             \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m 0  \u001b[0m\u001b[32m \u001b[0m│\u001b[31m \u001b[0m\u001b[31m   2   \u001b[0m\u001b[31m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m  2  \u001b[0m\u001b[34m \u001b[0m│\n",
       "└──────────────────────────┴──────┴─────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ah.display_lasso_summary_table(\n",
    "    all_features=X_train_tree.columns.tolist(),\n",
    "    selected_features=X_train_linear.columns.tolist(),\n",
    ")\n",
    "\n",
    "# fig_sunburst_final = ah.plot_lasso_summary_sunburst(\n",
    "#     all_features=X_train_tree.columns.tolist(),\n",
    "#     selected_features=X_train_linear.columns.tolist(),\n",
    "# )\n",
    "# fig_sunburst_final.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b7450",
   "metadata": {},
   "source": [
    "LASSO provided insights into which features are most valuable for a *linear* forecasting model.\n",
    "\n",
    "*   **Aggressive pruning of redundancy:** LASSO reduced the feature set by 70%, from 139 down to 41. It heavily pruned the highly collinear **Lag** and **Rolling Window** features, which made up the bulk of the original set.\n",
    "\n",
    "*   **Preference for recent & long-term history:**\n",
    "    *   For the primary `temp` variable, LASSO kept the most recent history (`temp_lag_1`, `_2`, `_3`), confirming that yesterday's temperature is a powerful predictor.\n",
    "    *   Interestingly, for other atmospheric variables like `dew` and `cloudcover`, it discarded recent lags but kept the **long-term lags (14 and 28 days)**, *consistently*. This suggests that for a linear model, the broader climate context from 2-4 weeks ago is more valuable than the noisy data from 1-2 days ago.\n",
    "\n",
    "*   **Value of raw predictors:** LASSO retained a core set of 13 raw predictors, including fundamental drivers like `solarradiation` and `cloudcover`, after deeming many of their derivative lag/rolling features redundant.\n",
    "\n",
    "*   **Rejection of complex/derived Features:**\n",
    "    *   The model dropped **all domain-specific features** (`daylight_hours`, `temp_range`, etc.) and **all categorical flags** (`preciptype_none/rain`).\n",
    "    *   Perhaps, for a linear model, the information contained in these features was likely already captured more effectively by the combination of raw predictors and selected lags. For example, the information in `temp_range` is already present in the raw `tempmax` and `tempmin` features, which LASSO chose to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887917e4",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"evaluation-framework\" style=\"display: none;\">Q5.2. Evaluation Framework</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"evaluation-framework-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.2. Evaluation Framework</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2214ec",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H3 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"evaluation-metrics\" style=\"display: none;\">Q5.2.1. Evaluation Metrics</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"evaluation-metrics-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.2.1. Evaluation Metrics</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a71ff3",
   "metadata": {},
   "source": [
    "Before training any models, we must define how we will measure \"good.\" A single metric is rarely sufficient; a holistic view requires assessing performance from multiple perspectives. The following table defines the standard regression metrics we will consider.\n",
    "\n",
    "| METRIC         | FORMULA                                                                                             | CORE QUESTION                                                                    | INTEPRETATIONS                                                                                                                  |\n",
    "| :-------------- | :---------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **R-squared ($R^2$)** | $1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$                        | \"How much of the temperature's variance is explained by our model?\"              | A scale-free score from $(-\\infty, 1]$. A score of 0.7 means our model explains 70% of the variance. **Primary metric for model comparison.** |\n",
    "| **Adjusted $R^2$** | $1 - \\frac{(1-R^2)(n-1)}{n-p-1}$                                                                       | \"What is the $R^2$, after penalizing for adding more features?\"                | Useful for explanatory linear modeling to avoid overfitting by adding useless predictors. **Not used in our final reports.**                    |\n",
    "| **RMSE**       | $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$                                                  | \"What is the typical magnitude of our error, penalizing larger errors more?\"     | Also in degrees Celsius. Because errors are squared, RMSE is more sensitive to large forecast misses than MAE. A key indicator of error size. |\n",
    "| **MAE**        | $\\frac{1}{n} \\sum_{i=1}^{n} \\|y_i - \\hat{y}_i\\|$                                                            | \"On average, what is the absolute error of our forecast in degrees?\"             | Highly interpretable. An MAE of 0.5 means our forecasts are off by an average of 0.5°C. Robust to outliers.                                    |\n",
    "| **MAPE**       | $\\frac{100}{n} \\sum_{i=1}^{n} \\|\\frac{y_i - \\hat{y}_i}{y_i}\\|$                                                | \"On average, what is the percentage error of our forecast?\"                      | A relative error metric. A MAPE of 2% means our forecasts are off by an average of 2% of the actual temperature.                               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e5d48",
   "metadata": {},
   "source": [
    "Based on the definitions above, we will adopt the following reporting hierarchy:\n",
    "1.  **Primary metric for selection:** **R-squared ($R^2$)** will be the primary metric used in our bake-off leaderboards to compare the relative performance of different model architectures.\n",
    "2.  **Key error metrics for interpretation:** **RMSE** and **MAE** will be reported for our finalist and champion models. They provide a direct, interpretable measure of the forecast error in degrees Celsius.\n",
    "3.  **Secondary relative metric:** **MAPE** will be reported for the final champion model to provide a sense of percentage error.\n",
    "4.  **Excluded metric:** We will **not** report **Adjusted $R^2$**. For our predictive task, the use of a robust cross-validation strategy on held-out data (`TimeSeriesSplit`) is a more direct and applicable method for penalizing model complexity and assessing true generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cca84d",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H3 FOR OUTLINE VIEW -->\n",
    "<h3 id=\"cv-strategy\" style=\"display: none;\">Q5.2.2. Cross-Validation Strategy</h3>\n",
    "<!-- VISIBLE H3 -->\n",
    "<h3 id=\"cv-strategy-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 16px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.2.2. Cross-Validation Strategy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce256052",
   "metadata": {},
   "source": [
    "To reliably estimate a model's performance on unseen data, we must use a cross-validation strategy that respects the temporal nature of our data.\n",
    "\n",
    "*   **Invalid method: standard K-Fold CV**\n",
    "    *   Standard K-Fold randomly shuffles and splits the data into `k` folds.\n",
    "    *   For time-series data, this is a catastrophic error. It allows the model to train on data from the future (e.g., 2023) to make predictions on data from the past (e.g., 2021), which is **data leakage**, leading to wildly optimistic and completely invalid performance estimates.\n",
    "\n",
    "*   **Chosen method: `TimeSeriesSplit`**\n",
    "    *   `TimeSeriesSplit` creates a series of expanding or sliding window splits. In each fold, the training set always consists of data that occurred *before* the validation set.\n",
    "\n",
    "    *   **How it Works (for 5 splits):** Split the train set into 6 blocks\n",
    "        *   **Fold 1:** Train on `[Block 1]`, validate on `[Block 2]`\n",
    "        *   **Fold 2:** Train on `[Block 1, Block 2]`, validate on `[Block 3]`\n",
    "        *   **Fold 3:** Train on `[Block 1, 2, 3]`, validate on `[Block 4]`\n",
    "        *   ...and so on.\n",
    "\n",
    "$\\implies$ This process perfectly simulates a real-world production scenario where a model is periodically retrained on all available historical data to make forecasts for the immediate future. The average score across these folds provides a robust, realistic, and leak-free estimate of our model's generalization ability. All model selection decisions in this notebook will be based on performance using this `TimeSeriesSplit` strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c4420",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"baseline-model\" style=\"display: none;\">Q5.3. The Baseline Model: Establishing the Performance Floor</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"baseline-model-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.3. The Baseline Model: Establishing the Performance Floor</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec19f6",
   "metadata": {},
   "source": [
    "A baseline provides the \"performance floor\"—the minimum score that any sophisticated model must convincingly beat to be considered useful. Without a baseline, a model's performance score lacks context.\n",
    "\n",
    "Several options for a baseline exist (for the current daily dataset):\n",
    "\n",
    "*   **Historical average:** Predict every day with the overall average temperature from the training set. This is the simplest possible baseline, but it is too naive for our data as it completely ignores the powerful seasonal cycle we identified.\n",
    "\n",
    "*   **Seasonal naive:** Predict the temperature for a given day using the temperature from the same day one year prior (e.g., $\\hat{y}_t = y_{t-365}$). This is a stronger baseline that accounts for seasonality.\n",
    "\n",
    "*   **Persistence model (We choose this):** This model operates on a simple, powerful heuristic: \"the best guess for the near future is the recent past.\" It predicts the temperature for a future day `t+h` using the temperature from day `t`.\n",
    "    *   **Formula:** $\\hat{y}_{t+h} = y_t$\n",
    "    *   **Justification:** We choose the persistence model as our primary baseline because it represents the most fundamental temporal relationship in the data. For short horizons like `t+1`, it can be surprisingly difficult to beat. Any model we build must first demonstrate that it can leverage our rich feature set to understand the weather dynamics *better* than this simple persistence rule. It provides the true, lowest bar for a time-aware forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba454f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_cv_scores = eval.evaluate_persistence_model(\n",
    "#     df_model_ready=df_model_ready,\n",
    "#     X_train=X_train_full,\n",
    "#     y_train_all_horizons=y_train,\n",
    "#     horizons=config.HORIZONS,\n",
    "# )\n",
    "# eval.display_baseline_results(baseline_cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe2f18",
   "metadata": {},
   "source": [
    "*   **Decent short-term performance:** A mean CV R² of **0.5455 for the `t+1` horizon** is decent for a naive model. It confirms that in Ho Chi Minh's stable weather system, today's temperature is a strong predictor of tomorrow's. Our `t+1` models must convincingly beat this high bar.\n",
    "\n",
    "*   **Rapid performance decay:** As expected, the model's performance collapses as the forecast horizon increases, becoming worse than a simple historical average by `t+5` (negative R² score). This highlights the core challenge of our project: predicting temperature several days in advance, where simple persistence fails completely.\n",
    "\n",
    "*   **The importance of a robust CV score:**\n",
    "    *   The average CV score (e.g., 0.5455 for `t+1`) is a conservative but realistic estimate of the model's performance over time.\n",
    "    *   It is expected to be lower than a one-off score on the final test set (which for `t+1` is ~0.685) because the `TimeSeriesSplit` forces the model to make predictions across various time periods, including older data where patterns might be different. A model that performs well across all CV folds is more robust and reliable than one that only performs well on the most recent data.\n",
    "\n",
    "$\\implies$ **Goal:** Our models must demonstrate their value by not only outperforming this baseline on average, but especially by providing a significant performance lift on the challenging **`t+3` to `t+5` horizons**, where the naive model provides no value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adf8ee",
   "metadata": {},
   "source": [
    "<!-- HIDDEN H2 FOR OUTLINE VIEW -->\n",
    "<h2 id=\"bakeoff\" style=\"display: none;\">Q5.4. Model Bake-Off</h2>\n",
    "<!-- VISIBLE H2 -->\n",
    "<h2 id=\"bakeoff-visible\" style=\"font-family: 'Roboto Condensed', 'Arial Narrow', 'sans-serif'; color: #38545f; font-size: 18px; font-weight: 500; background-color: #f9fbfb; border-top: 4px solid #0c75ab; border-radius: 2px; border-bottom: 1px solid #D9D9D9; padding: 10px 0px 10px 15px; margin-top: 15px;\">Q5.4. Model Bake-Off</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbfe3c",
   "metadata": {},
   "source": [
    "We will now systematically evaluate a diverse range of model architectures to identify the most promising candidate for our champion model. This process is conducted with the following protocol:\n",
    "\n",
    "*   **Comprehensive saves:** We use the `run_bakeoff` utility function to generate and save detailed result files (`.pkl`). These files contain not just the final scores, but the scores and predictions from *every fold*, allowing for deep diagnostic analysis.\n",
    "*   **Presentation reporting:** We then load those pre-computed artifacts to generate summary tables and visualizations. The `config.TRAIN_NEW_MODELS` flag allows us to optionally re-run the training process if needed, but load the pre-computed ones by default to save time.\n",
    "\n",
    "We will evaluate four main families of models:\n",
    "1.  **Standard ML models:** Linear and tree-based models that serve as strong, interpretable baselines.\n",
    "2.  **Advanced time-series models:** Architectures like SARIMAX and Prophet that are specifically designed for time-series data.\n",
    "3.  **Deep learning models**: Models like LSTM, CNN, Seq2Seq, with attention strategy may capture the pattern better, especially with longer horizons.\n",
    "4.  **Ensemble models:** Hybrid models that combine the predictions of multiple base models to improve robustness and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaa696fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Bake-Off Script\n",
    "\n",
    "# --- 1. Imports & Setup ---\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Assuming all utility functions and the ChampionForecastStack class are in utils/\n",
    "import utils.training as train\n",
    "\n",
    "# Create a directory for experiment artifacts if it doesn't exist\n",
    "os.makedirs(\"experiments\", exist_ok=True)\n",
    "\n",
    "# --- 2. Load Data (Assuming this is already done in your sandbox) ---\n",
    "# You should have:\n",
    "# X_train_linear, X_train_tree\n",
    "# y_train\n",
    "# config (with HORIZONS and GLOBAL_RANDOM_SEED)\n",
    "\n",
    "# --- 3. Define Models for Bake-Off ---\n",
    "# Naming convention is key for clarity in results.\n",
    "linear_models = {\n",
    "    \"LinearRegression\": LinearRegression(n_jobs=-1),\n",
    "    \"RidgeCV\": RidgeCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "    \"ElasticNetCV\": ElasticNetCV(\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        n_jobs=-1,\n",
    "        random_state=config.GLOBAL_RANDOM_SEED,\n",
    "    ),\n",
    "}\n",
    "\n",
    "tree_models = {\n",
    "    \"LGBMRegressor\": LGBMRegressor(\n",
    "        random_state=config.GLOBAL_RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1,\n",
    "    ),\n",
    "    \"XGBRegressor\": XGBRegressor(\n",
    "        random_state=config.GLOBAL_RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "    ),\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(\n",
    "        random_state=config.GLOBAL_RANDOM_SEED,\n",
    "        verbose=0,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# --- 4. Define CV Splitter ---\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160de750",
   "metadata": {},
   "source": [
    "# **> Run from above to here <**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa882678",
   "metadata": {},
   "source": [
    "### **Bake-off for the single models and ensembles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e88c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Run Bake-Offs and Save Artifacts ---\n",
    "\n",
    "# Run for Linear Models\n",
    "print(\"--- Running Bake-Off for Linear Models ---\")\n",
    "linear_results = train.run_bakeoff(\n",
    "    models_to_run=linear_models,\n",
    "    X_train=X_train_linear,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(linear_results, \"experiments/bakeoff_results_linear.pkl\")\n",
    "print(\"Linear model results saved to 'experiments/bakeoff_results_linear.pkl'\")\n",
    "\n",
    "# Run for Tree-Based Models\n",
    "print(\"\\n--- Running Bake-Off for Tree-Based Models ---\")\n",
    "tree_results = train.run_bakeoff(\n",
    "    models_to_run=tree_models,\n",
    "    X_train=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(tree_results, \"experiments/bakeoff_results_tree.pkl\")\n",
    "print(\"Tree-based model results saved to 'experiments/bakeoff_results_tree.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2317ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Advanced TS Bake-Off Script\n",
    "\n",
    "# --- 1. Imports for Advanced Models ---\n",
    "# Make sure you have these installed: pip install statsmodels prophet\n",
    "import statsmodels.api as sm\n",
    "from prophet import Prophet\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  # Suppress Prophet warnings\n",
    "\n",
    "# (Existing imports like os, joblib, pandas, etc., are assumed to be present)\n",
    "# import utils.training as train (if in a new session)\n",
    "\n",
    "# --- 2. Define Custom Wrapper for Prophet ---\n",
    "# Prophet has a different API, so we wrap it in a scikit-learn compatible class.\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "def _calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Helper to calculate MAPE safely.\"\"\"\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n",
    "\n",
    "\n",
    "class ProphetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, seasonality_mode=\"additive\"):\n",
    "        self.seasonality_mode = seasonality_mode\n",
    "        self.model = None\n",
    "        self.regressor_names = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # --- FIX: Add robust imputation as a safeguard ---\n",
    "        X_imputed = X.ffill().bfill()\n",
    "\n",
    "        # --- FIX: Correct DataFrame construction to prevent index misalignment ---\n",
    "        # Start with the features and add 'ds' and 'y' columns.\n",
    "        df_prophet_input = X_imputed.copy()\n",
    "        df_prophet_input[\"ds\"] = df_prophet_input.index\n",
    "        df_prophet_input[\"y\"] = y.values  # Use .values to ignore index and align by position\n",
    "\n",
    "        self.regressor_names = [col for col in X.columns]\n",
    "        self.model = Prophet(seasonality_mode=self.seasonality_mode)\n",
    "        for regressor in self.regressor_names:\n",
    "            self.model.add_regressor(regressor)\n",
    "\n",
    "        # Suppress verbose output from Prophet\n",
    "        with open(os.devnull, \"w\") as devnull:\n",
    "            import sys\n",
    "\n",
    "            old_stdout = sys.stdout\n",
    "            sys.stdout = devnull\n",
    "            try:\n",
    "                # Use the correctly constructed DataFrame\n",
    "                self.model.fit(df_prophet_input)\n",
    "            finally:\n",
    "                sys.stdout = old_stdout\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # --- FIX: Ensure prediction data is also imputed ---\n",
    "        X_imputed = X.ffill().bfill()\n",
    "\n",
    "        # --- FIX: Correct DataFrame construction for prediction ---\n",
    "        df_future = X_imputed.copy()\n",
    "        df_future[\"ds\"] = df_future.index\n",
    "\n",
    "        forecast = self.model.predict(df_future)\n",
    "        return forecast[\"yhat\"].values\n",
    "\n",
    "\n",
    "# --- 3. Define Advanced Models ---\n",
    "# Note: SARIMAX is extremely slow and might not be practical for a full CV bake-off\n",
    "# on a large feature set. We include it for completeness but be aware of runtime.\n",
    "# For SARIMAX, we will use a reduced feature set for tractability.\n",
    "\n",
    "# A smaller, highly relevant feature set for SARIMAX\n",
    "sarimax_features = [\"sin_day_of_year\", \"cos_day_of_year\", \"temp_lag_1\", \"temp_roll_7d_mean\", \"solarradiation_lag_1\"]\n",
    "# X_train_sarimax = X_train_tree[sarimax_features]\n",
    "X_train_sarimax = X_train_linear\n",
    "\n",
    "advanced_ts_models = {\n",
    "    \"Prophet\": ProphetWrapper(),\n",
    "    # SARIMAX(order=(p,d,q), seasonal_order=(P,D,Q,s))\n",
    "    # We use a common simple order. Finding the best order is a project in itself.\n",
    "    # We use the full feature set as exogenous variables ('exog').\n",
    "    \"SARIMAX\": sm.tsa.SARIMAX(\n",
    "        endog=y_train[\"target_temp_t+1\"],  # Will be replaced in loop\n",
    "        exog=X_train_sarimax,  # Will be replaced in loop\n",
    "        order=(1, 1, 1),\n",
    "        seasonal_order=(1, 1, 1, 7),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# --- 4. Special Bake-Off Loop for Advanced Models ---\n",
    "# We need a custom loop because the statsmodels API is different.\n",
    "advanced_ts_results = {}\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for model_name, model_obj in advanced_ts_models.items():\n",
    "    print(f\"\\n--- Running Bake-Off for {model_name} ---\")\n",
    "    model_results = {}\n",
    "\n",
    "    for h in tqdm(config.HORIZONS, desc=f\"Training {model_name}\"):\n",
    "        target_col = f\"target_temp_t+{h}\"\n",
    "        y_train_horizon = y_train[target_col]\n",
    "\n",
    "        fold_scores = {\"r2\": [], \"rmse\": [], \"mae\": [], \"mape\": []}\n",
    "\n",
    "        for train_idx, val_idx in tscv_splitter.split(X_train_tree):\n",
    "            # Use X_train_tree for Prophet, X_train_sarimax for SARIMAX\n",
    "            X_train_to_use = X_train_sarimax if model_name == \"SARIMAX\" else X_train_tree\n",
    "\n",
    "            X_train_fold, X_val_fold = X_train_to_use.iloc[train_idx], X_train_to_use.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train_horizon.iloc[train_idx], y_train_horizon.iloc[val_idx]\n",
    "\n",
    "            if model_name == \"SARIMAX\":\n",
    "                # statsmodels needs to be re-initialized for each fold with the correct data\n",
    "                model_instance = sm.tsa.SARIMAX(\n",
    "                    endog=y_train_fold, exog=X_train_fold, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)\n",
    "                ).fit(disp=False)\n",
    "                y_pred_fold = model_instance.predict(\n",
    "                    start=X_val_fold.index[0], end=X_val_fold.index[-1], exog=X_val_fold\n",
    "                )\n",
    "            else:  # For ProphetWrapper and other scikit-learn compatible models\n",
    "                model_obj.fit(X_train_fold, y_train_fold)\n",
    "                y_pred_fold = model_obj.predict(X_val_fold)\n",
    "\n",
    "            fold_scores[\"r2\"].append(r2_score(y_val_fold, y_pred_fold))\n",
    "            fold_scores[\"rmse\"].append(np.sqrt(mean_squared_error(y_val_fold, y_pred_fold)))\n",
    "            fold_scores[\"mae\"].append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "            fold_scores[\"mape\"].append(_calculate_mape(y_val_fold.values, y_pred_fold))\n",
    "\n",
    "        model_results[f\"t+{h}\"] = {\"scores\": fold_scores}\n",
    "\n",
    "    advanced_ts_results[model_name] = model_results\n",
    "\n",
    "joblib.dump(advanced_ts_results, \"experiments/bakeoff_results_advanced_ts.pkl\")\n",
    "print(\"Advanced time-series model results saved to 'experiments/bakeoff_results_advanced_ts.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Deep Learning (LSTM) Bake-Off Script\n",
    "\n",
    "# --- 1. Imports for Deep Learning ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import gc  # Garbage Collector\n",
    "\n",
    "# (Assumes other necessary imports like os, joblib, pandas, numpy are present)\n",
    "\n",
    "\n",
    "# --- 2. Define a scikit-learn compatible Wrapper for the Keras LSTM Model ---\n",
    "class LSTMWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_timesteps=7, n_features=0, epochs=50, batch_size=32, validation_split=0.1, verbose=0):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.scaler = RobustScaler()  # Each model needs its own scaler fit on its training data\n",
    "\n",
    "    def _create_sequences(self, X, y):\n",
    "        \"\"\"Helper to transform tabular data into 3D sequences for LSTM.\"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(X) - self.n_timesteps):\n",
    "            X_seq.append(X[i : (i + self.n_timesteps)])\n",
    "            y_seq.append(y[i + self.n_timesteps - 1])  # Target corresponds to the last day in the sequence\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        # 1. Scale the data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # 2. Reshape into 3D sequences\n",
    "        X_seq, y_seq = self._create_sequences(X_scaled, y.values)\n",
    "\n",
    "        if len(X_seq) == 0:\n",
    "            print(\"Warning: Not enough data to create sequences with the given timesteps. Skipping fit.\")\n",
    "            return self\n",
    "\n",
    "        # 3. Build the LSTM model architecture\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.model = Sequential([\n",
    "            LSTM(64, activation=\"relu\", input_shape=(self.n_timesteps, self.n_features), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation=\"relu\"),\n",
    "            Dense(1),  # Output layer\n",
    "        ])\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "        # 4. Train the model\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "        self.model.fit(\n",
    "            X_seq,\n",
    "            y_seq,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            return np.zeros(len(X))\n",
    "\n",
    "        # Scale and reshape the prediction data\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        # We need to create sequences, but only predict for the valid range\n",
    "        X_seq, _ = self._create_sequences(X_scaled, np.zeros(len(X_scaled)))  # Dummy y\n",
    "\n",
    "        if len(X_seq) == 0:\n",
    "            # Handle cases where X is too short to form a full sequence\n",
    "            return np.full(len(X), np.nan)\n",
    "\n",
    "        predictions = self.model.predict(X_seq, verbose=self.verbose).flatten()\n",
    "\n",
    "        # The predictions align with the END of each sequence. We need to pad the start.\n",
    "        padding = np.full(self.n_timesteps, np.nan)\n",
    "        return np.concatenate([padding, predictions])\n",
    "\n",
    "\n",
    "# --- 3. Run the Bake-Off for the LSTM ---\n",
    "print(\"\\n--- Running Bake-Off for LSTM Model ---\")\n",
    "# Use a smaller subset of features for LSTM to keep training time manageable\n",
    "# and focus on the most impactful variables.\n",
    "lstm_features = [\n",
    "    \"temp_lag_1\",\n",
    "    \"temp_lag_2\",\n",
    "    \"temp_lag_3\",\n",
    "    \"temp_lag_7\",\n",
    "    \"sin_day_of_year\",\n",
    "    \"cos_day_of_year\",\n",
    "    \"solarradiation\",\n",
    "    \"cloudcover\",\n",
    "    \"humidity\",\n",
    "    \"precip\",\n",
    "    \"windspeed\",\n",
    "    \"dew\",\n",
    "]\n",
    "X_train_lstm = X_train_tree[lstm_features]\n",
    "\n",
    "dl_models = {\n",
    "    # Using a 7-day lookback window (timesteps) for the LSTM.\n",
    "    \"LSTM\": LSTMWrapper(n_timesteps=7)\n",
    "}\n",
    "dl_results = {}\n",
    "\n",
    "# We need another custom loop because the data needs to be reshaped\n",
    "# and predictions need to be handled carefully.\n",
    "for model_name, model_obj in dl_models.items():\n",
    "    model_results = {}\n",
    "    for h in tqdm(config.HORIZONS, desc=f\"Training {model_name}\"):\n",
    "        target_col = f\"target_temp_t+{h}\"\n",
    "        y_train_horizon = y_train[target_col]\n",
    "        fold_scores = {\"r2\": [], \"rmse\": [], \"mae\": [], \"mape\": []}\n",
    "\n",
    "        for train_idx, val_idx in tscv_splitter.split(X_train_lstm):\n",
    "            X_train_fold, X_val_fold = X_train_lstm.iloc[train_idx], X_train_lstm.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train_horizon.iloc[train_idx], y_train_horizon.iloc[val_idx]\n",
    "\n",
    "            model_instance = LSTMWrapper(n_timesteps=7)\n",
    "            model_instance.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_fold_raw = model_instance.predict(X_val_fold)\n",
    "\n",
    "            # Important: The wrapper returns NaNs at the start. We must align predictions with true values.\n",
    "            valid_preds_mask = ~np.isnan(y_pred_fold_raw)\n",
    "            y_pred_fold_aligned = y_pred_fold_raw[valid_preds_mask]\n",
    "            y_val_fold_aligned = y_val_fold.values[valid_preds_mask]\n",
    "\n",
    "            if len(y_pred_fold_aligned) > 0:\n",
    "                fold_scores[\"r2\"].append(r2_score(y_val_fold_aligned, y_pred_fold_aligned))\n",
    "                fold_scores[\"rmse\"].append(np.sqrt(mean_squared_error(y_val_fold_aligned, y_pred_fold_aligned)))\n",
    "                fold_scores[\"mae\"].append(mean_absolute_error(y_val_fold_aligned, y_pred_fold_aligned))\n",
    "                fold_scores[\"mape\"].append(_calculate_mape(y_val_fold_aligned, y_pred_fold_aligned))\n",
    "\n",
    "            # Clean up memory\n",
    "            del model_instance\n",
    "            gc.collect()\n",
    "\n",
    "        model_results[f\"t+{h}\"] = {\"scores\": fold_scores}\n",
    "    dl_results[model_name] = model_results\n",
    "\n",
    "joblib.dump(dl_results, \"experiments/bakeoff_results_dl.pkl\")\n",
    "print(\"Deep Learning model results saved to 'experiments/bakeoff_results_dl.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Ensemble Bake-Off Script\n",
    "\n",
    "# --- 1. Imports ---\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 2. Define Models for Ensemble Bake-Off ---\n",
    "# The SimpleAveragingEnsemble is handled by a special case in the runner function,\n",
    "# so we pass its components in a dictionary.\n",
    "ensemble_models = {\n",
    "    \"SimpleAveragingEnsemble\": {\n",
    "        \"linear\": RidgeCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        \"tree\": CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "    },\n",
    "    \"RidgeCV_CatBoost_with_RidgeCV\": train.StackingEnsemble(\n",
    "        base_model_linear=RidgeCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        base_model_tree=CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "    ),\n",
    "    \"RidgeCV_CatBoost_with_ElasticNeCV\": train.StackingEnsemble(\n",
    "        base_model_linear=RidgeCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        base_model_tree=CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "        meta_learner=ElasticNetCV(cv=TimeSeriesSplit(n_splits=3)),\n",
    "    ),\n",
    "    \"ElasticNetCV_CatBoost_with_RidgeCV\": train.StackingEnsemble(\n",
    "        base_model_linear=ElasticNetCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        base_model_tree=CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "    ),\n",
    "    \"ElasticNet_CatBoost_with_ElasticNetCV\": train.StackingEnsemble(\n",
    "        base_model_linear=ElasticNetCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        base_model_tree=CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "        meta_learner=ElasticNetCV(cv=TimeSeriesSplit(n_splits=3)),\n",
    "    ),\n",
    "    \"ElasticNetCV_LGBM_with_RidgeCV\": train.StackingEnsemble(\n",
    "        base_model_linear=ElasticNetCV(cv=TimeSeriesSplit(n_splits=5)),\n",
    "        base_model_tree=LGBMRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=-1),\n",
    "    ),\n",
    "}\n",
    "\n",
    "# --- 3. Run Ensemble Bake-Off ---\n",
    "print(\"\\n--- Running Bake-Off for Ensemble Models ---\")\n",
    "# Assuming tscv_splitter is already defined from the previous bake-off cell\n",
    "ensemble_results = train.run_ensemble_bakeoff(\n",
    "    ensembles_to_run=ensemble_models,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(ensemble_results, \"experiments/bakeoff_results_ensemble.pkl\")\n",
    "print(\"Ensemble model results saved to 'experiments/bakeoff_results_ensemble.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aeb6fb",
   "metadata": {},
   "source": [
    "### **Diagnostic scripts - Train full then test, no mean CV evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Diagnostic Script for Full Train/Test Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "# --- Helper Functions (copied from utils/evaluation.py for self-containment) ---\n",
    "def _calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / np.maximum(np.abs(y_true), 1e-8))) * 100\n",
    "\n",
    "\n",
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"R²\": r2_score(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MAPE\": _calculate_mape(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 1. Define Finalist Models ---\n",
    "# Using the same definitions as the bake-off for consistency\n",
    "finalist_linear_models = {\n",
    "    \"LinearRegression\": LinearRegression(n_jobs=-1),\n",
    "    \"RidgeCV\": RidgeCV(),  # CV is not used here, but we keep the object for consistency\n",
    "    \"ElasticNetCV\": ElasticNetCV(random_state=config.GLOBAL_RANDOM_SEED, n_jobs=-1),\n",
    "}\n",
    "\n",
    "finalist_tree_models = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(random_state=config.GLOBAL_RANDOM_SEED, verbose=0),\n",
    "    \"LGBMRegressor\": LGBMRegressor(random_state=config.GLOBAL_RANDOM_SEED, n_jobs=-1, verbosity=-1),\n",
    "}\n",
    "\n",
    "all_finalists = {**finalist_linear_models, **finalist_tree_models}\n",
    "\n",
    "# --- 2. Run Full Evaluation Loop ---\n",
    "print(\"--- RUNNING DIAGNOSTIC: EVALUATION ON FULL TRAIN/TEST SPLIT ---\\n\")\n",
    "\n",
    "all_results_summary = []\n",
    "\n",
    "for model_name, model_obj in all_finalists.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\")\n",
    "\n",
    "    # Select the correct feature set\n",
    "    is_linear = model_name in finalist_linear_models\n",
    "    X_train_to_use = X_train_linear if is_linear else X_train_tree\n",
    "    X_test_to_use = X_test_linear if is_linear else X_test_tree\n",
    "\n",
    "    horizon_metrics = []\n",
    "\n",
    "    for h in config.HORIZONS:\n",
    "        target_col = f\"target_temp_t+{h}\"\n",
    "\n",
    "        # 1. Fit model on the ENTIRE training set\n",
    "        model_obj.fit(X_train_to_use, y_train[target_col])\n",
    "\n",
    "        # 2. Predict ONCE on the test set\n",
    "        y_pred_test = model_obj.predict(X_test_to_use)\n",
    "\n",
    "        # 3. Calculate all metrics\n",
    "        metrics = calculate_all_metrics(y_test[target_col], y_pred_test)\n",
    "        metrics[\"Horizon\"] = f\"t+{h}\"\n",
    "        horizon_metrics.append(metrics)\n",
    "\n",
    "    # --- 3. Display Results for the current model ---\n",
    "    df_model_results = pd.DataFrame(horizon_metrics)\n",
    "\n",
    "    # Calculate average and add it as a new row\n",
    "    avg_metrics = df_model_results.mean(numeric_only=True)\n",
    "    avg_metrics[\"Horizon\"] = \"Average\"\n",
    "    # pd.concat is deprecated, use _append\n",
    "    df_model_results = df_model_results._append(avg_metrics, ignore_index=True)\n",
    "\n",
    "    df_model_results = df_model_results.set_index(\"Horizon\")\n",
    "\n",
    "    print(df_model_results.to_string(float_format=\"%.4f\"))\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "    # Store for final summary if needed\n",
    "    all_results_summary.append({\"model\": model_name, \"results\": df_model_results})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Diagnostic Script for Tuned Trees & Ensembles\n",
    "from sklearn import clone\n",
    "\n",
    "\n",
    "# --- 1. Define Sensible Heuristic Defaults for Tree Models ---\n",
    "# These are more generous than the library defaults to give them a better chance.\n",
    "# Based on common practices for similar datasets.\n",
    "sensible_lgbm_params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": 7,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": config.GLOBAL_RANDOM_SEED,\n",
    "    \"verbosity\": -1,\n",
    "}\n",
    "\n",
    "sensible_catboost_params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 7,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"verbose\": 0,\n",
    "    \"random_state\": config.GLOBAL_RANDOM_SEED,\n",
    "}\n",
    "\n",
    "tuned_tree_models = {\n",
    "    \"LGBMRegressor_tuned\": LGBMRegressor(**sensible_lgbm_params),\n",
    "    \"CatBoostRegressor_tuned\": CatBoostRegressor(**sensible_catboost_params),\n",
    "}\n",
    "\n",
    "# --- 2. Define Base Models for Ensembling ---\n",
    "# We will use the best linear and the new tuned tree model.\n",
    "base_models_for_ensemble = {\n",
    "    \"RidgeCV\": finalist_linear_models[\"RidgeCV\"],\n",
    "    \"LGBMRegressor_tuned\": tuned_tree_models[\"LGBMRegressor_tuned\"],\n",
    "}\n",
    "\n",
    "# --- 3. Create Ensemble Models to Test ---\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "ensemble_models = {}\n",
    "\n",
    "\n",
    "# Simple Averaging Ensemble\n",
    "class AveragingEnsemble(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        self.fitted_models_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.fitted_models_ = [clone(model).fit(X, y) for name, model in self.models.items()]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.fitted_models_])\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "\n",
    "ensemble_models[\"AvgEnsemble(Ridge+LGBM)\"] = AveragingEnsemble(models=base_models_for_ensemble)\n",
    "\n",
    "# Stacking Ensemble\n",
    "# The ChampionForecastStack is designed for this, but for a quick diagnostic,\n",
    "# sklearn's StackingRegressor is sufficient and clear.\n",
    "stacking_ensemble = StackingRegressor(\n",
    "    estimators=list(base_models_for_ensemble.items()), final_estimator=RidgeCV(), n_jobs=-1\n",
    ")\n",
    "ensemble_models[\"StackingEnsemble(Ridge+LGBM)\"] = stacking_ensemble\n",
    "\n",
    "\n",
    "# --- 4. Combine Models and Run Evaluation ---\n",
    "models_to_evaluate = {**tuned_tree_models, **ensemble_models}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"--- RUNNING DIAGNOSTIC: EVALUATION FOR TUNED TREES & ENSEMBLES ---\\n\")\n",
    "\n",
    "for model_name, model_obj in models_to_evaluate.items():\n",
    "    print(f\"--- Evaluating Model: {model_name} ---\")\n",
    "\n",
    "    # All these models use the full tree feature set\n",
    "    X_train_to_use = X_train_tree\n",
    "    X_test_to_use = X_test_tree\n",
    "\n",
    "    horizon_metrics = []\n",
    "\n",
    "    for h in config.HORIZONS:\n",
    "        target_col = f\"target_temp_t+{h}\"\n",
    "\n",
    "        # Fit model on the ENTIRE training set\n",
    "        model_obj.fit(X_train_to_use, y_train[target_col])\n",
    "\n",
    "        # Predict ONCE on the test set\n",
    "        y_pred_test = model_obj.predict(X_test_to_use)\n",
    "\n",
    "        # Calculate all metrics\n",
    "        metrics = calculate_all_metrics(y_test[target_col], y_pred_test)\n",
    "        metrics[\"Horizon\"] = f\"t+{h}\"\n",
    "        horizon_metrics.append(metrics)\n",
    "\n",
    "    # Display Results for the current model\n",
    "    df_model_results = pd.DataFrame(horizon_metrics).set_index(\"Horizon\")\n",
    "    avg_metrics = df_model_results.mean(numeric_only=True)\n",
    "    avg_metrics.name = \"Average\"\n",
    "    df_model_results = df_model_results._append(avg_metrics)\n",
    "\n",
    "    print(df_model_results.to_string(float_format=\"%.4f\"))\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c4ec6",
   "metadata": {},
   "source": [
    "### **HPOs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdbab3",
   "metadata": {},
   "source": [
    "#### **CatBoost - OLD AVERAGE ONE, discarded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b49d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sandbox_notebook.ipynb - HPO Study 1: CatBoostRegressor \n",
    "# This was the old average (one set of params for all horizons) tuning\n",
    "\n",
    "# # --- 1. Imports ---\n",
    "# import optuna\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# # Assuming all utility functions and data are loaded\n",
    "# import utils.training as train\n",
    "\n",
    "# # --- 2. Setup Study ---\n",
    "# STUDY_NAME_CATBOOST = \"hpo_catboost_regressor\"\n",
    "# N_TRIALS_CATBOOST = 175  # Adjust as needed based on time constraints\n",
    "\n",
    "# # --- 3. Define the Objective Function ---\n",
    "# # We create a dictionary for the feature sets, even though this model only uses one.\n",
    "# # This keeps the function signature consistent for our objective factory.\n",
    "# X_train_for_tuning = {\n",
    "#     \"linear\": X_train_linear,  # Placeholder, not used by this model\n",
    "#     \"tree\": X_train_tree,\n",
    "# }\n",
    "\n",
    "# catboost_objective = train.create_optuna_objective(\n",
    "#     model_name=\"CatBoostRegressor\",\n",
    "#     X_train_dict=X_train_for_tuning,\n",
    "#     y_train=y_train,\n",
    "#     cv_splitter=TimeSeriesSplit(n_splits=5),\n",
    "#     horizons=config.HORIZONS,\n",
    "# )\n",
    "\n",
    "# # --- 4. Run the Study ---\n",
    "# print(f\"--- Running Optuna Study for CatBoostRegressor ({N_TRIALS_CATBOOST} trials) ---\")\n",
    "# # To monitor in real-time, run this in your terminal:\n",
    "# # optuna-dashboard sqlite:///experiments/hpo_catboost_regressor.db\n",
    "# catboost_study = train.run_optuna_study(\n",
    "#     study_name=STUDY_NAME_CATBOOST, objective_func=catboost_objective, n_trials=N_TRIALS_CATBOOST\n",
    "# )\n",
    "\n",
    "# # --- 5. Save the Best Parameters ---\n",
    "# train.save_tuned_params(\n",
    "#     study=catboost_study, horizons=config.HORIZONS, file_path=\"experiments/tuned_params_CatBoostRegressor.pkl\"\n",
    "# )\n",
    "\n",
    "# print(\"\\n--- CatBoostRegressor Tuning Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8971a5",
   "metadata": {},
   "source": [
    "#### **LGBM - Per-horizon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - HPO Study 2: LGBM\n",
    "\n",
    "# --- HPO CONFIGURATION ---\n",
    "TUNE_PER_HORIZON = True\n",
    "N_TRIALS = 250  # Total trials if False, trials per horizon if True\n",
    "\n",
    "# --- 2. Data Setup ---\n",
    "X_train_for_tuning = {\"linear\": X_train_linear, \"tree\": X_train_tree}\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --- 3. Run Study for LGBMRegressor ---\n",
    "STUDY_NAME_LGBM = \"hpo_lgbm_regressor\"\n",
    "lgbm_custom_space = {\n",
    "    # --- Core Learning Parameters (Narrowed) ---\n",
    "    \"learning_rate\": {\"type\": \"float\", \"low\": 0.005, \"high\": 0.04, \"log\": True},\n",
    "    \"n_estimators\": {\"type\": \"int\", \"low\": 300, \"high\": 2000},\n",
    "    # --- Tree Structure Parameters (Focused) ---\n",
    "    \"num_leaves\": {\"type\": \"int\", \"low\": 20, \"high\": 60},  # Kept the same, good range\n",
    "    \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 6},  # Narrowed significantly\n",
    "    # --- Regularization Parameters (Constrained) ---\n",
    "    \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 1.0, \"log\": True},  # Reduced upper bound\n",
    "    \"lambda_l2\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 1.0, \"log\": True},  # Reduced upper bound\n",
    "    # --- Bagging/Subsampling Parameters (Kept for robustness) ---\n",
    "    \"feature_fraction\": {\"type\": \"float\", \"low\": 0.5, \"high\": 0.9},\n",
    "    \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.5, \"high\": 0.9},\n",
    "    \"bagging_freq\": {\"type\": \"int\", \"low\": 1, \"high\": 7},\n",
    "    # --- Other Structural Parameters ---\n",
    "    \"min_child_samples\": {\"type\": \"int\", \"low\": 5, \"high\": 50},\n",
    "}\n",
    "if TUNE_PER_HORIZON:\n",
    "    per_horizon_studies = {}\n",
    "    for h in config.HORIZONS:\n",
    "        print(f\"--- Tuning LGBMRegressor for HORIZON t+{h} ({N_TRIALS} trials) ---\")\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"LGBMRegressor\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            custom_search_space=lgbm_custom_space,\n",
    "        )\n",
    "        study = train.run_optuna_study(f\"{STUDY_NAME_LGBM}_h{h}\", objective, n_trials=N_TRIALS)\n",
    "        per_horizon_studies[h] = study\n",
    "    train.save_tuned_params(None, \"experiments/tuned_params_LGBMRegressor.pkl\", per_horizon_studies)\n",
    "else:\n",
    "    print(f\"--- Tuning LGBMRegressor for AVERAGE PERFORMANCE ({N_TRIALS} trials) ---\")\n",
    "    objective = train.create_optuna_objective(\n",
    "        model_name=\"LGBMRegressor\",\n",
    "        X_train_dict=X_train_for_tuning,\n",
    "        y_train=y_train,\n",
    "        cv_splitter=tscv_splitter,\n",
    "        horizon_to_tune=None,\n",
    "        custom_search_space=lgbm_custom_space,\n",
    "    )\n",
    "    study = train.run_optuna_study(STUDY_NAME_LGBM, objective, n_trials=N_TRIALS)\n",
    "    train.save_tuned_params(study, \"experiments/tuned_params_LGBMRegressor.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe699db",
   "metadata": {},
   "source": [
    "#### **CatBoost - Per-horizon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_catboost_targeted_spaces(base_params: dict) -> list:\n",
    "    \"\"\"\n",
    "    Generates a list of five targeted hyperparameter search spaces for CatBoost,\n",
    "    one for each forecast horizon, based on a set of well-performing base parameters.\n",
    "\n",
    "    The strategy is to start with a complex search space for t+1 and progressively\n",
    "    simplify it for longer horizons to combat overfitting on weaker signals.\n",
    "    \"\"\"\n",
    "    targeted_spaces = []\n",
    "\n",
    "    # Base values from our best \"average\" trial\n",
    "    base_lr = base_params[\"learning_rate\"]\n",
    "    base_iter = base_params[\"iterations\"]\n",
    "    base_depth = base_params[\"depth\"]\n",
    "\n",
    "    for h in range(1, 6):  # For horizons t+1 through t+5\n",
    "        # Principle: Complexity and precision decrease as horizon increases\n",
    "\n",
    "        # Learning Rate: Allow slightly lower for t+1, slightly higher for t+5\n",
    "        lr_low = max(0.005, base_lr - 0.007)\n",
    "        lr_high = min(0.05, base_lr + 0.01 * h)\n",
    "\n",
    "        # Iterations: High for t+1, decreasing for longer horizons\n",
    "        iter_low = max(200, base_iter - 100 * h)\n",
    "        iter_high = max(400, base_iter + 400 - 150 * (h - 1))\n",
    "\n",
    "        # Depth: Higher for t+1, strictly decreasing for longer horizons\n",
    "        depth_low = max(3, base_depth - (h // 2))\n",
    "        depth_high = min(8, base_depth + 2 - (h // 2))\n",
    "\n",
    "        space = {\n",
    "            \"learning_rate\": {\"type\": \"float\", \"low\": lr_low, \"high\": lr_high, \"log\": True},\n",
    "            \"iterations\": {\"type\": \"int\", \"low\": int(iter_low), \"high\": int(iter_high)},\n",
    "            \"depth\": {\"type\": \"int\", \"low\": depth_low, \"high\": depth_high},\n",
    "            # Keep other parameters with a reasonably wide search space as they are less critical\n",
    "            \"l2_leaf_reg\": {\"type\": \"float\", \"low\": 1e-5, \"high\": 10.0, \"log\": True},\n",
    "            \"random_strength\": {\"type\": \"float\", \"low\": 1e-5, \"high\": 10.0, \"log\": True},\n",
    "            \"bagging_temperature\": {\"type\": \"float\", \"low\": 0.0, \"high\": 1.0},\n",
    "        }\n",
    "        targeted_spaces.append(space)\n",
    "\n",
    "    return targeted_spaces\n",
    "\n",
    "\n",
    "best_avg_catboost_params = {\n",
    "    \"learning_rate\": 0.0149,\n",
    "    \"iterations\": 528,\n",
    "    \"depth\": 5,\n",
    "}\n",
    "\n",
    "catboost_targeted_search_spaces = create_catboost_targeted_spaces(best_avg_catboost_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - HPO Study 3: CatBoost per-horizon\n",
    "\n",
    "# --- HPO CONFIGURATION ---\n",
    "TUNE_PER_HORIZON = True\n",
    "N_TRIALS = 200  # Total trials if False, trials per horizon if True\n",
    "\n",
    "# --- 2. Data Setup ---\n",
    "X_train_for_tuning = {\"linear\": X_train_linear, \"tree\": X_train_tree}\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --- 3. Run Study for CatBoostRegressor ---\n",
    "STUDY_NAME_CATBOOST = \"hpo_catboost_regressor\"\n",
    "if TUNE_PER_HORIZON:\n",
    "    per_horizon_studies = {}\n",
    "    for h in config.HORIZONS:\n",
    "        print(f\"--- Tuning CatBoostRegressor for HORIZON t+{h} ({N_TRIALS} trials) ---\")\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"CatBoostRegressor\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            custom_search_space=catboost_targeted_search_spaces[h - 1],\n",
    "        )\n",
    "        study = train.run_optuna_study(f\"{STUDY_NAME_CATBOOST}_h{h}\", objective, n_trials=N_TRIALS)\n",
    "        per_horizon_studies[h] = study\n",
    "    train.save_tuned_params(None, \"experiments/tuned_params_CatBoostRegressor.pkl\", per_horizon_studies)\n",
    "else:\n",
    "    print(f\"--- Tuning CatBoostRegressor for AVERAGE PERFORMANCE ({N_TRIALS} trials) ---\")\n",
    "    objective = train.create_optuna_objective(\n",
    "        model_name=\"CatBoostRegressor\",\n",
    "        X_train_dict=X_train_for_tuning,\n",
    "        y_train=y_train,\n",
    "        cv_splitter=tscv_splitter,\n",
    "        horizon_to_tune=None,\n",
    "        custom_search_space=catboost_targeted_search_spaces[h - 1],\n",
    "    )\n",
    "    study = train.run_optuna_study(STUDY_NAME_CATBOOST, objective, n_trials=N_TRIALS)\n",
    "    train.save_tuned_params(study, \"experiments/tuned_params_CatBoostRegressor.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a50cf9",
   "metadata": {},
   "source": [
    "#### **Preliminary evaluation of tuned base models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c69a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params_paths = {\n",
    "    \"CatBoostRegressor\": \"experiments/tuned_params_CatBoostRegressor_v2.pkl\",\n",
    "    \"LGBMRegressor\": \"./experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "}\n",
    "\n",
    "# Configuration for all models we want to evaluate in this run\n",
    "models_to_evaluate = {\n",
    "    # Standalone Tuned Tree Models\n",
    "    \"Tuned_CatBoost_v2_weighted_CV\": {\n",
    "        \"type\": \"single\",\n",
    "        \"tree_model_name\": \"CatBoostRegressor\",\n",
    "    },\n",
    "    \"Tuned_LGBM_v2_weighted_CV\": {\n",
    "        \"type\": \"single\",\n",
    "        \"tree_model_name\": \"LGBMRegressor\",\n",
    "    },\n",
    "    # # --- Tuned Simple Averaging Ensembles ---\n",
    "    # \"Avg_Ridge_TunedCatBoost\": {\n",
    "    #     \"type\": \"simple_averaging\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_name\": \"CatBoostRegressor\",\n",
    "    # },\n",
    "    # \"Avg_ElasticNet_TunedLGBM\": {\n",
    "    #     \"type\": \"simple_averaging\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_name\": \"LGBMRegressor\",\n",
    "    # },\n",
    "    # # --- Tuned Stacking Ensembles ---\n",
    "    # # CatBoost combinations\n",
    "    # \"Stack_Ridge_TunedCatBoost_RidgeMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_name\": \"CatBoostRegressor\",\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    # },\n",
    "    # \"Stack_ElasticNet_TunedCatBoost_RidgeMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_name\": \"CatBoostRegressor\",\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    # },\n",
    "    # \"Stack_ElasticNet_TunedCatBoost_ElasticNetMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_name\": \"CatBoostRegressor\",\n",
    "    #     \"meta_learner_class\": ElasticNetCV,\n",
    "    # },\n",
    "    # # LGBM combinations\n",
    "    # \"Stack_Ridge_TunedLGBM_RidgeMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_name\": \"LGBMRegressor\",\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    # },\n",
    "    # \"Stack_ElasticNet_TunedLGBM_RidgeMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_name\": \"LGBMRegressor\",\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    # },\n",
    "    # \"Stack_ElasticNet_TunedLGBM_ElasticNetMeta\": {\n",
    "    #     \"type\": \"stacking\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_name\": \"LGBMRegressor\",\n",
    "    #     \"meta_learner_class\": ElasticNetCV,\n",
    "    # },\n",
    "}\n",
    "\n",
    "# --- 3. Run the Evaluation ---\n",
    "print(\"\\n--- Evaluating Performance of Tuned Models and Ensembles ---\")\n",
    "tuned_evaluation_results = eval.evaluate_tuned_models(\n",
    "    models_to_evaluate=models_to_evaluate,\n",
    "    tuned_params_paths=tuned_params_paths,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "filename_of_tuned_models = \"experiments/evaluation_results_tuned_single_models_v2_weighted_CV.pkl\"\n",
    "joblib.dump(tuned_evaluation_results, filename_of_tuned_models)\n",
    "print(f\"Tuned model evaluation results saved to '{filename_of_tuned_models}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files_to_load = [\n",
    "    \"experiments/bakeoff_results_linear.pkl\",\n",
    "    \"experiments/bakeoff_results_tree.pkl\",\n",
    "    # \"experiments/bakeoff_results_ensemble.pkl\",\n",
    "    # \"experiments/evaluation_results_tuned_preliminary_ensemble_models.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_single_models_v1_mean.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_single_models_v2_weighted_CV.pkl\",\n",
    "]\n",
    "\n",
    "df_leaderboard_final = eval.generate_leaderboard_data(results_files=results_files_to_load)\n",
    "eval.display_leaderboard_stage1(df_leaderboard_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369c401",
   "metadata": {},
   "source": [
    "#### **LGBM - Per-horizon re-tune with weighted CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Cell 1: Setup & Configuration\n",
    "\n",
    "# --- 2. HPO CONFIGURATION ---\n",
    "NUM_RUNS_PER_HORIZON = 7\n",
    "N_TRIALS_PER_RUN = 100\n",
    "CV_WEIGHTS = np.array([0.10, 0.15, 0.20, 0.25, 0.30])\n",
    "\n",
    "# --- 3. Data & Splitter Setup ---\n",
    "X_train_for_tuning = {\"linear\": X_train_linear, \"tree\": X_train_tree}\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --- 4. Define Final Tuned LGBM Parameters (from single-model run) ---\n",
    "# This is our reference point for creating targeted search spaces.\n",
    "lgbm_single_model_best_params = {\n",
    "    1: {\n",
    "        \"learning_rate\": 0.0053,\n",
    "        \"n_estimators\": 852,\n",
    "        \"num_leaves\": 54,\n",
    "        \"max_depth\": 5,\n",
    "        \"lambda_l1\": 1.4e-05,\n",
    "        \"lambda_l2\": 2.0e-05,\n",
    "        \"feature_fraction\": 0.75,\n",
    "        \"bagging_fraction\": 0.50,\n",
    "        \"bagging_freq\": 4,\n",
    "        \"min_child_samples\": 24,\n",
    "    },\n",
    "    2: {\n",
    "        \"learning_rate\": 0.0053,\n",
    "        \"n_estimators\": 722,\n",
    "        \"num_leaves\": 20,\n",
    "        \"max_depth\": 4,\n",
    "        \"lambda_l1\": 0.122,\n",
    "        \"lambda_l2\": 8.4e-05,\n",
    "        \"feature_fraction\": 0.73,\n",
    "        \"bagging_fraction\": 0.51,\n",
    "        \"bagging_freq\": 3,\n",
    "        \"min_child_samples\": 33,\n",
    "    },\n",
    "    3: {\n",
    "        \"learning_rate\": 0.0060,\n",
    "        \"n_estimators\": 589,\n",
    "        \"num_leaves\": 33,\n",
    "        \"max_depth\": 3,\n",
    "        \"lambda_l1\": 1.7e-08,\n",
    "        \"lambda_l2\": 2.7e-05,\n",
    "        \"feature_fraction\": 0.69,\n",
    "        \"bagging_fraction\": 0.51,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"min_child_samples\": 42,\n",
    "    },\n",
    "    4: {\n",
    "        \"learning_rate\": 0.0100,\n",
    "        \"n_estimators\": 365,\n",
    "        \"num_leaves\": 26,\n",
    "        \"max_depth\": 3,\n",
    "        \"lambda_l1\": 7.1e-08,\n",
    "        \"lambda_l2\": 0.00015,\n",
    "        \"feature_fraction\": 0.56,\n",
    "        \"bagging_fraction\": 0.53,\n",
    "        \"bagging_freq\": 7,\n",
    "        \"min_child_samples\": 41,\n",
    "    },\n",
    "    5: {\n",
    "        \"learning_rate\": 0.0086,\n",
    "        \"n_estimators\": 490,\n",
    "        \"num_leaves\": 25,\n",
    "        \"max_depth\": 3,\n",
    "        \"lambda_l1\": 0.035,\n",
    "        \"lambda_l2\": 0.0033,\n",
    "        \"feature_fraction\": 0.50,\n",
    "        \"bagging_fraction\": 0.53,\n",
    "        \"bagging_freq\": 6,\n",
    "        \"min_child_samples\": 29,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5. Function to Generate Targeted Spaces ---\n",
    "def create_targeted_search_space(base_params: Dict, buffer: float = 0.3) -> Dict:\n",
    "    \"\"\"Creates a targeted Optuna search space around a set of base parameters.\"\"\"\n",
    "    space = {}\n",
    "    for param, value in base_params.items():\n",
    "        if isinstance(value, int) and param not in [\"bagging_freq\", \"max_depth\"]:\n",
    "            low = max(1, int(value * (1 - buffer)))\n",
    "            high = int(value * (1 + buffer))\n",
    "            space[param] = {\"type\": \"int\", \"low\": low, \"high\": high}\n",
    "        elif isinstance(value, float):\n",
    "            low = max(1e-9, value * (1 - buffer))\n",
    "            high = (\n",
    "                min(1.0, value * (1 + buffer))\n",
    "                if param in [\"feature_fraction\", \"bagging_fraction\"]\n",
    "                else value * (1 + buffer)\n",
    "            )\n",
    "            space[param] = {\n",
    "                \"type\": \"float\",\n",
    "                \"low\": low,\n",
    "                \"high\": high,\n",
    "                \"log\": param in [\"learning_rate\", \"lambda_l1\", \"lambda_l2\"],\n",
    "            }\n",
    "        # For categorical or fixed-range params, define them explicitly\n",
    "        elif param == \"max_depth\":\n",
    "            space[param] = {\"type\": \"int\", \"low\": max(2, value - 1), \"high\": value + 2}\n",
    "        elif param == \"bagging_freq\":\n",
    "            space[param] = {\"type\": \"int\", \"low\": max(1, value - 2), \"high\": value + 2}\n",
    "    return space\n",
    "\n",
    "\n",
    "# --- 6. Generate the Dictionary of Targeted Spaces ---\n",
    "lgbm_targeted_spaces = {h: create_targeted_search_space(lgbm_single_model_best_params[h]) for h in config.HORIZONS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Cell 2: Re-tune Single LGBM\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"  STARTING: Re-tuning Single LGBMRegressor with Weighted CV\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "STUDY_NAME_LGBM_WEIGHTED = \"hpo_lgbm_regressor_weighted_v3\"\n",
    "lgbm_weighted_studies = {}\n",
    "for h in config.HORIZONS:\n",
    "    for i in range(NUM_RUNS_PER_HORIZON):\n",
    "        run_name = f\"{STUDY_NAME_LGBM_WEIGHTED}_h{h}_run{i + 1}\"\n",
    "        print(f\"--- Running Study: {run_name} ({N_TRIALS_PER_RUN} trials) ---\")\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"LGBMRegressor\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            cv_weights=CV_WEIGHTS,\n",
    "            custom_search_space=lgbm_targeted_spaces[h],\n",
    "        )\n",
    "        train.run_optuna_study(run_name, objective, n_trials=N_TRIALS_PER_RUN)\n",
    "\n",
    "# Consolidate results for the re-tuned LGBM\n",
    "train.consolidate_best_params(\n",
    "    base_study_name=STUDY_NAME_LGBM_WEIGHTED,\n",
    "    horizons=config.HORIZONS,\n",
    "    num_runs=NUM_RUNS_PER_HORIZON,\n",
    "    output_file_path=\"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63a5ea",
   "metadata": {},
   "source": [
    "#### **LGBM - Per-horizon re-tune with weighted CV AND `linear_tree=True`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - HPO Study: LGBM with linear_tree=True\n",
    "\n",
    "# --- 1. Imports & Configuration ---\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "import utils.training as train\n",
    "\n",
    "# --- HPO CONFIGURATION ---\n",
    "NUM_RUNS_PER_HORIZON = 2\n",
    "N_TRIALS_PER_RUN = 190\n",
    "CV_WEIGHTS = np.array([0.10, 0.15, 0.20, 0.25, 0.30])\n",
    "X_train_for_tuning = {\"linear\": X_train_linear, \"tree\": X_train_tree}\n",
    "tscv_splitter = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --- 2. Define FINAL Targeted Search Spaces for linear_tree ---\n",
    "# Based on deep analysis of the initial run and fANOVA plots.\n",
    "\n",
    "lgbm_linear_tree_targeted_spaces = {\n",
    "    # --- Horizon 1: Focus on n_estimators and bagging ---\n",
    "    1: {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"low\": 0.0045, \"high\": 0.006, \"log\": True},\n",
    "        \"n_estimators\": {\"type\": \"int\", \"low\": 900, \"high\": 1450},\n",
    "        \"num_leaves\": {\"type\": \"int\", \"low\": 60, \"high\": 170},\n",
    "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 5},  # Constrained\n",
    "        \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 1e-4, \"log\": True},  # Unimportant, constrain\n",
    "        \"lambda_l2\": {\"type\": \"float\", \"low\": 5e-7, \"high\": 5e-5, \"log\": True},  # Unimportant, constrain\n",
    "        \"feature_fraction\": {\"type\": \"float\", \"low\": 0.92, \"high\": 1},\n",
    "        \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.61, \"high\": 0.7},  # Focused range\n",
    "        \"bagging_freq\": {\"type\": \"int\", \"low\": 1, \"high\": 3},\n",
    "        \"min_child_samples\": {\"type\": \"int\", \"low\": 70, \"high\": 120},\n",
    "    },\n",
    "    # --- Horizon 2: Similar to H1, but slightly simpler ---\n",
    "    2: {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"low\": 0.005, \"high\": 0.01, \"log\": True},\n",
    "        \"n_estimators\": {\"type\": \"int\", \"low\": 500, \"high\": 1100},\n",
    "        \"num_leaves\": {\"type\": \"int\", \"low\": 40, \"high\": 140},\n",
    "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 5},\n",
    "        \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 1e-4, \"log\": True},\n",
    "        \"lambda_l2\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 0.1, \"log\": True},\n",
    "        \"feature_fraction\": {\"type\": \"float\", \"low\": 0.6, \"high\": 0.8},\n",
    "        \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.42, \"high\": 0.62},\n",
    "        \"bagging_freq\": {\"type\": \"int\", \"low\": 3, \"high\": 7},\n",
    "        \"min_child_samples\": {\"type\": \"int\", \"low\": 60, \"high\": 100},\n",
    "    },\n",
    "    # --- Horizon 3: Focus on bagging_fraction and max_depth ---\n",
    "    3: {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"low\": 0.006, \"high\": 0.012, \"log\": True},\n",
    "        \"n_estimators\": {\"type\": \"int\", \"low\": 400, \"high\": 800},\n",
    "        \"num_leaves\": {\"type\": \"int\", \"low\": 20, \"high\": 50},\n",
    "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 4},\n",
    "        \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-6, \"high\": 0.1, \"log\": True},\n",
    "        \"lambda_l2\": {\"type\": \"float\", \"low\": 1.0, \"high\": 4.0, \"log\": True},  # Focus on higher value\n",
    "        \"feature_fraction\": {\"type\": \"float\", \"low\": 0.35, \"high\": 0.64},\n",
    "        \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.35, \"high\": 0.55},  # Focus on this key param\n",
    "        \"bagging_freq\": {\"type\": \"int\", \"low\": 2, \"high\": 5},\n",
    "        \"min_child_samples\": {\"type\": \"int\", \"low\": 80, \"high\": 120},\n",
    "    },\n",
    "    # --- Horizon 4: Return focus to n_estimators ---\n",
    "    4: {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"low\": 0.005, \"high\": 0.01, \"log\": True},\n",
    "        \"n_estimators\": {\"type\": \"int\", \"low\": 500, \"high\": 900},\n",
    "        \"num_leaves\": {\"type\": \"int\", \"low\": 80, \"high\": 150},\n",
    "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 6},\n",
    "        \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 0.1, \"log\": True},\n",
    "        \"lambda_l2\": {\"type\": \"float\", \"low\": 1e-8, \"high\": 1e-4, \"log\": True},\n",
    "        \"feature_fraction\": {\"type\": \"float\", \"low\": 0.4, \"high\": 0.6},\n",
    "        \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.3, \"high\": 0.5},\n",
    "        \"bagging_freq\": {\"type\": \"int\", \"low\": 1, \"high\": 5},\n",
    "        \"min_child_samples\": {\"type\": \"int\", \"low\": 70, \"high\": 110},\n",
    "    },\n",
    "    # --- Horizon 5: Focus on max_depth and bagging ---\n",
    "    5: {\n",
    "        \"learning_rate\": {\"type\": \"float\", \"low\": 0.007, \"high\": 0.015, \"log\": True},\n",
    "        \"n_estimators\": {\"type\": \"int\", \"low\": 400, \"high\": 700},\n",
    "        \"num_leaves\": {\"type\": \"int\", \"low\": 40, \"high\": 75},\n",
    "        \"max_depth\": {\"type\": \"int\", \"low\": 3, \"high\": 4},  # Focus on this key param\n",
    "        \"lambda_l1\": {\"type\": \"float\", \"low\": 1e-5, \"high\": 0.01, \"log\": True},\n",
    "        \"lambda_l2\": {\"type\": \"float\", \"low\": 0.1, \"high\": 1.0, \"log\": True},\n",
    "        \"feature_fraction\": {\"type\": \"float\", \"low\": 0.6, \"high\": 0.8},\n",
    "        \"bagging_fraction\": {\"type\": \"float\", \"low\": 0.35, \"high\": 0.52},  # Focus on this key param\n",
    "        \"bagging_freq\": {\"type\": \"int\", \"low\": 1, \"high\": 5},\n",
    "        \"min_child_samples\": {\"type\": \"int\", \"low\": 70, \"high\": 110},\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 3. Run the Tuning Study ---\n",
    "STUDY_NAME_LGBM_LINEAR_TREE = \"hpo_lgbm_regressor_weighted_CV_linear_tree_v2\"\n",
    "for h in [2, 3, 4, 5]:\n",
    "    for i in range(NUM_RUNS_PER_HORIZON):\n",
    "        run_name = f\"{STUDY_NAME_LGBM_LINEAR_TREE}_h{h}_run{i + 1}\"\n",
    "        print(f\"--- Running Study: {run_name} ({N_TRIALS_PER_RUN} trials) ---\")\n",
    "\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"LGBMRegressor\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            cv_weights=CV_WEIGHTS,\n",
    "            custom_search_space=lgbm_linear_tree_targeted_spaces[h],\n",
    "        )\n",
    "        train.run_optuna_study(run_name, objective, n_trials=N_TRIALS_PER_RUN)\n",
    "\n",
    "# --- 4. Consolidate Final Results ---\n",
    "filename_to_save_tuned_params = \"experiments/tuned_params_LGBMRegressor_v5.pkl\"\n",
    "train.consolidate_best_params(\n",
    "    base_study_name=STUDY_NAME_LGBM_LINEAR_TREE,\n",
    "    horizons=config.HORIZONS,\n",
    "    num_runs=NUM_RUNS_PER_HORIZON,\n",
    "    output_file_path=filename_to_save_tuned_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab9597",
   "metadata": {},
   "source": [
    "#### **Stacking ensemble 01 - Per-horizon with weighted CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Cell 3: Tune Stacking Ensemble (ElasticNet + LGBM + Ridge)\n",
    "\n",
    "NUM_RUNS_PER_HORIZON = 4\n",
    "N_TRIALS_PER_RUN = 50\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"  STARTING: Tuning Stack_ElasticNet_LGBM_RidgeMeta\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "STUDY_NAME_STACK_LGBM = \"hpo_stack_elasticnet_lgbm_ridge_v2\"\n",
    "stack_lgbm_studies = {}\n",
    "for h in config.HORIZONS:\n",
    "    for i in range(NUM_RUNS_PER_HORIZON):\n",
    "        run_name = f\"{STUDY_NAME_STACK_LGBM}_h{h}_run{i + 1}\"\n",
    "        print(f\"--- Running Study: {run_name} ({N_TRIALS_PER_RUN} trials) ---\")\n",
    "\n",
    "        # We pass the full model name to the objective factory.\n",
    "        # It will parse this to know it needs to build a StackingEnsemble\n",
    "        # with an internal LGBMRegressor.\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"Stacking_LGBM\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            cv_weights=CV_WEIGHTS,\n",
    "            # CRUCIAL: We reuse the same targeted search space for this horizon.\n",
    "            custom_search_space=lgbm_targeted_spaces[h],\n",
    "        )\n",
    "        train.run_optuna_study(run_name, objective, n_trials=N_TRIALS_PER_RUN)\n",
    "\n",
    "# Consolidate results for this ensemble\n",
    "train.consolidate_best_params(\n",
    "    base_study_name=STUDY_NAME_STACK_LGBM,\n",
    "    horizons=config.HORIZONS,\n",
    "    num_runs=NUM_RUNS_PER_HORIZON,\n",
    "    output_file_path=\"experiments/tuned_params_final_Stack_ElasticNet_LGBM_Ridge_v2.pkl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356b116",
   "metadata": {},
   "source": [
    "#### **Stacking ensemble 02 - Per-horizon with weighted CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Cell 4: Define CatBoost Targeted Spaces\n",
    "\n",
    "NUM_RUNS_PER_HORIZON = 2\n",
    "N_TRIALS_PER_RUN = 20\n",
    "\n",
    "# --- 1. Define Final Tuned CatBoost Parameters (from single-model run) ---\n",
    "catboost_single_model_best_params = {\n",
    "    1: {\n",
    "        \"learning_rate\": 0.0175,\n",
    "        \"iterations\": 605,\n",
    "        \"depth\": 5,\n",
    "        \"l2_leaf_reg\": 0.547,\n",
    "        \"random_strength\": 0.033,\n",
    "        \"bagging_temperature\": 0.356,\n",
    "    },\n",
    "    2: {\n",
    "        \"learning_rate\": 0.0101,\n",
    "        \"iterations\": 581,\n",
    "        \"depth\": 6,\n",
    "        \"l2_leaf_reg\": 0.227,\n",
    "        \"random_strength\": 0.040,\n",
    "        \"bagging_temperature\": 0.463,\n",
    "    },\n",
    "    3: {\n",
    "        \"learning_rate\": 0.0094,\n",
    "        \"iterations\": 594,\n",
    "        \"depth\": 4,\n",
    "        \"l2_leaf_reg\": 0.167,\n",
    "        \"random_strength\": 4.037,\n",
    "        \"bagging_temperature\": 0.154,\n",
    "    },\n",
    "    4: {\n",
    "        \"learning_rate\": 0.0137,\n",
    "        \"iterations\": 438,\n",
    "        \"depth\": 4,\n",
    "        \"l2_leaf_reg\": 0.184,\n",
    "        \"random_strength\": 5.495,\n",
    "        \"bagging_temperature\": 0.946,\n",
    "    },\n",
    "    5: {\n",
    "        \"learning_rate\": 0.0203,\n",
    "        \"iterations\": 349,\n",
    "        \"depth\": 3,\n",
    "        \"l2_leaf_reg\": 0.974,\n",
    "        \"random_strength\": 9.870,\n",
    "        \"bagging_temperature\": 0.764,\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 2. Generate the Dictionary of Targeted Spaces for CatBoost ---\n",
    "# We can reuse the same create_targeted_search_space function from the previous cell.\n",
    "catboost_targeted_spaces = {\n",
    "    h: create_targeted_search_space(catboost_single_model_best_params[h]) for h in config.HORIZONS\n",
    "}\n",
    "\n",
    "# sandbox_notebook.ipynb - Cell 5: Tune Stacking Ensemble (ElasticNet + CatBoost + Ridge)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"  STARTING: Tuning Stack_ElasticNet_CatBoost_RidgeMeta\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "STUDY_NAME_STACK_CATBOOST = \"hpo_stack_elasticnet_catboost_ridge\"\n",
    "stack_catboost_studies = {}\n",
    "for h in [2, 3, 4, 5]:\n",
    "    for i in range(NUM_RUNS_PER_HORIZON):\n",
    "        run_name = f\"{STUDY_NAME_STACK_CATBOOST}_h{h}_run{i + 1}\"\n",
    "        print(f\"--- Running Study: {run_name} ({N_TRIALS_PER_RUN} trials) ---\")\n",
    "\n",
    "        # Pass the full model name for the objective factory to parse\n",
    "        objective = train.create_optuna_objective(\n",
    "            model_name=\"Stacking_CatBoost\",\n",
    "            X_train_dict=X_train_for_tuning,\n",
    "            y_train=y_train,\n",
    "            cv_splitter=tscv_splitter,\n",
    "            horizon_to_tune=h,\n",
    "            cv_weights=CV_WEIGHTS,\n",
    "            # Use the targeted search space for this horizon\n",
    "            custom_search_space=catboost_targeted_spaces[h],\n",
    "        )\n",
    "        train.run_optuna_study(run_name, objective, n_trials=N_TRIALS_PER_RUN)\n",
    "\n",
    "# Consolidate results for this ensemble\n",
    "train.consolidate_best_params(\n",
    "    base_study_name=STUDY_NAME_STACK_CATBOOST,\n",
    "    horizons=config.HORIZONS,\n",
    "    num_runs=NUM_RUNS_PER_HORIZON,\n",
    "    output_file_path=\"experiments/tuned_params_final_Stack_ElasticNet_CatBoost_Ridge.pkl\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd46d9",
   "metadata": {},
   "source": [
    "#### **MODELS EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb664096",
   "metadata": {},
   "source": [
    "##### **User guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a9dc4",
   "metadata": {},
   "source": [
    "| Parameter              | Data Type         | Description                                                                                             | Example                                             |\n",
    "| :--------------------- | :---------------- | :------------------------------------------------------------------------------------------------------ | :-------------------------------------------------- |\n",
    "| **`type`** (Required)  | `str`             | The category of model. Determines the logic used by `evaluate_models`.                                  | `\"simple_averaging_tuned\"`, `\"stacking_tuned\"`, `\"single_tuned\"`, `\"oob\"` |\n",
    "| **`model_class`**      | `class`           | The scikit-learn compatible class for **OOB** (untuned) models.                                         | `LGBMRegressor`                                     |\n",
    "| **`feature_set`**      | `str`             | **For OOB models only.** Specifies which feature set to use.                                            | `\"linear\"` or `\"tree\"`                              |\n",
    "| **`linear_model_class`** | `class`         | **For ensembles.** The class for the linear base model.                                                 | `RidgeCV`, `ElasticNetCV`                           |\n",
    "| **`tree_model_class`**   | `class`         | **For tuned models.** The class for the tree-based model (for parameter mapping).                       | `CatBoostRegressor`, `LGBMRegressor`                |\n",
    "| **`meta_learner_class`** | `class`         | **For stacking ensembles.** The class for the meta-learner.                                             | `RidgeCV`                                           |\n",
    "| **`params_path`**      | `str`             | **For tuned models.** The file path to the `.pkl` file containing the per-horizon tuned hyperparameters. | `\"experiments/my_params.pkl\"`                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62edea42",
   "metadata": {},
   "source": [
    "\n",
    "**Examples:**\n",
    "\n",
    "**1. To evaluate a new, untuned `XGBRegressor`:**\n",
    "```python\n",
    "\"XGBRegressor_OOB\": {\n",
    "    \"type\": \"oob\",\n",
    "    \"model_class\": XGBRegressor,\n",
    "    \"feature_set\": \"tree\" # Use the full feature set\n",
    "},\n",
    "```\n",
    "\n",
    "**2. To evaluate your `Stack_ElasticNet_LGBM_Ridge_Tuned` but using the `v1` parameters instead:**\n",
    "```python\n",
    "\"Stack_ElasticNet_LGBM_Ridge_Tuned_v1\": {\n",
    "    \"type\": \"stacking_tuned\",\n",
    "    \"linear_model_class\": ElasticNetCV,\n",
    "    \"tree_model_class\": LGBMRegressor,\n",
    "    \"meta_learner_class\": RidgeCV,\n",
    "    \"params_path\": \"experiments/tuned_params_final_Stack_ElasticNet_LGBM_Ridge.pkl\" # Note the changed file path\n",
    "},\n",
    "```\n",
    "\n",
    "**3. To evaluate a single, tuned `CatBoostRegressor` (using `v1` params):**\n",
    "```python\n",
    "\"Tuned_CatBoost_v1\": {\n",
    "    \"type\": \"single_tuned\",\n",
    "    \"tree_model_class\": CatBoostRegressor,\n",
    "    \"params_path\": \"experiments/tuned_params_CatBoostRegressor_v1.pkl\"\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb5a15",
   "metadata": {},
   "source": [
    "##### **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_evaluate = {\n",
    "    # \"Tuned_LGBM_v1_mean\": {\n",
    "    #     \"type\": \"single_tuned\",\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"./experiments/tuned_params_LGBMRegressor_v1.pkl\",\n",
    "    # },\n",
    "    # \"Tuned_LGBM_v2_weighted\": {\n",
    "    #     \"type\": \"single_tuned\",\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"./experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    # },\n",
    "    \"Tuned_LGBM_v3_linear_tree\": {\n",
    "        \"type\": \"single_tuned\",\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"./experiments/tuned_params_LGBMRegressor_v5.pkl\",\n",
    "    },\n",
    "    # \"Avg_Ridge_LGBM_v1_mean\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_LGBMRegressor_v1.pkl\",\n",
    "    # },\n",
    "    # \"Avg_Ridge_LGBM_v2_weighted\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    # },\n",
    "    # --- 1. Tuned Simple Averaging Ensembles ---\n",
    "    # We will test all four logical combinations of our best linear and tuned tree models.\n",
    "    # \"Avg_Ridge_TunedCatBoost\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_class\": CatBoostRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_CatBoostRegressor_v2.pkl\",  # Using CatBoost v2\n",
    "    # },\n",
    "    # \"Avg_ElasticNet_TunedCatBoost\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_class\": CatBoostRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_CatBoostRegressor_v2.pkl\",\n",
    "    # },\n",
    "    # \"Avg_Ridge_TunedLGBM\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",  # Using LGBM v3\n",
    "    # },\n",
    "    # \"Avg_ElasticNet_TunedLGBM\": {\n",
    "    #     \"type\": \"simple_averaging_tuned\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    # },\n",
    "    # # --- 2. Holistically Tuned Stacking Ensembles ---\n",
    "    # # These models were tuned as a complete unit.\n",
    "    # \"Stack_ElasticNet_CatBoost_Ridge_Tuned\": {\n",
    "    #     \"type\": \"stacking_tuned\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_class\": CatBoostRegressor,\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    #     \"params_path\": \"experiments/tuned_params_final_Stack_ElasticNet_CatBoost_Ridge.pkl\",\n",
    "    # },\n",
    "    # \"Stack_ElasticNet_LGBM_Ridge_Tuned\": {\n",
    "    #     \"type\": \"stacking_tuned\",\n",
    "    #     \"linear_model_class\": ElasticNetCV,\n",
    "    #     \"tree_model_class\": LGBMRegressor,\n",
    "    #     \"meta_learner_class\": RidgeCV,\n",
    "    #     \"params_path\": \"experiments/tuned_params_final_Stack_ElasticNet_LGBM_Ridge_v2.pkl\",  # Using LGBM Stacker v2\n",
    "    # },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_of_evaluated_model_yes_long_name_i_know = \"experiments/evaluation_results_tuned_LGBM_v3_linear_tree.pkl\"\n",
    "\n",
    "print(\"\\n--- Running Final Evaluation of Tuned Ensembles ---\")\n",
    "final_tuned_ensemble_results = eval.evaluate_models(\n",
    "    models_to_evaluate=models_to_evaluate,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(final_tuned_ensemble_results, filename_of_evaluated_model_yes_long_name_i_know)\n",
    "print(f\"Tuned ensemble evaluation results saved to '{filename_of_evaluated_model_yes_long_name_i_know}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98d666",
   "metadata": {},
   "source": [
    "##### **Display leaderboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_files_to_load = [\n",
    "    \"experiments/bakeoff_results_linear.pkl\",\n",
    "    # \"experiments/bakeoff_results_tree.pkl\",\n",
    "    # \"experiments/bakeoff_results_ensemble.pkl\",\n",
    "    # \"experiments/evaluation_results_tuned_preliminary_ensemble_models.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_single_models_v1_mean.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_single_models_v2_weighted_CV.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_ensembles_test.pkl\",\n",
    "    \"experiments/evaluation_results_tuned_LGBM_v3.pkl\",\n",
    "]\n",
    "\n",
    "df_leaderboard_final = eval.generate_leaderboard_data(results_files=results_files_to_load)\n",
    "eval.display_leaderboard(df_leaderboard_final, title=\"Tuned Ensemble Leaderboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fb659",
   "metadata": {},
   "source": [
    "#### **Check test set, because why not...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4052a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Make sure all our custom classes and helper functions are available\n",
    "from utils.training import StackingEnsemble, _calculate_mape\n",
    "from utils.evaluations import _get_model_instance\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LinearRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "\n",
    "# Paths to the BEST versions of our tuned parameters\n",
    "final_params_paths = {\n",
    "    \"LGBMRegressor\": \"experiments/tuned_params_LGBMRegressor_v5.pkl\",\n",
    "    \"CatBoostRegressor\": \"experiments/tuned_params_CatBoostRegressor_v2.pkl\",\n",
    "    \"Stack_ElasticNet_LGBM_Ridge\": \"experiments/tuned_params_final_Stack_ElasticNet_LGBM_Ridge_v2.pkl\",\n",
    "}\n",
    "\n",
    "# Load all parameter dictionaries into memory\n",
    "loaded_params = {name: joblib.load(path) for name, path in final_params_paths.items()}\n",
    "\n",
    "# --- 3. Final Evaluation Loop ---\n",
    "\n",
    "console = Console()\n",
    "final_test_results = {}\n",
    "\n",
    "# A list of models to train and evaluate\n",
    "models_to_test = [\n",
    "    \"RidgeCV\",\n",
    "    \"LinearRegression\",\n",
    "    \"Tuned_LGBM\",\n",
    "    \"Tuned_CatBoost\",\n",
    "    \"Avg_Ridge_TunedLGBM\",\n",
    "    \"Stack_ElasticNet_LGBM_Ridge_Tuned\",\n",
    "]\n",
    "\n",
    "for model_name in tqdm(models_to_test, desc=\"Final Test Set Evaluation\"):\n",
    "    horizon_predictions = {}\n",
    "\n",
    "    for h in tqdm(config.HORIZONS, desc=f\"Training {model_name}\", leave=False):\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        y_train_horizon = y_train[f\"target_temp_{horizon_name}\"]\n",
    "\n",
    "        # --- Model-specific training on 100% of the training data ---\n",
    "\n",
    "        if model_name == \"RidgeCV\":\n",
    "            model = _get_model_instance(RidgeCV).fit(X_train_linear, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_linear)\n",
    "\n",
    "        elif model_name == \"LinearRegression\":\n",
    "            model = _get_model_instance(LinearRegression).fit(X_train_linear, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_linear)\n",
    "\n",
    "        elif model_name == \"Tuned_LGBM\":\n",
    "            params = loaded_params[\"LGBMRegressor\"][horizon_name]\n",
    "            model = _get_model_instance(LGBMRegressor, params=params).fit(X_train_tree, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_tree)\n",
    "\n",
    "        elif model_name == \"Tuned_CatBoost\":\n",
    "            params = loaded_params[\"CatBoostRegressor\"][horizon_name]\n",
    "            model = _get_model_instance(CatBoostRegressor, params=params).fit(X_train_tree, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_tree)\n",
    "\n",
    "        elif model_name == \"Avg_Ridge_TunedLGBM\":\n",
    "            # Train base models separately\n",
    "            linear_base = _get_model_instance(RidgeCV).fit(X_train_linear, y_train_horizon)\n",
    "            params_tree = loaded_params[\"LGBMRegressor\"][horizon_name]\n",
    "            tree_base = _get_model_instance(LGBMRegressor, params=params_tree).fit(X_train_tree, y_train_horizon)\n",
    "\n",
    "            # Predict and average\n",
    "            pred_linear = linear_base.predict(X_test_linear)\n",
    "            pred_tree = tree_base.predict(X_test_tree)\n",
    "            y_pred_test = (pred_linear + pred_tree) / 2.0\n",
    "\n",
    "        elif model_name == \"Stack_ElasticNet_LGBM_Ridge_Tuned\":\n",
    "            params_tree = loaded_params[\"Stack_ElasticNet_LGBM_Ridge\"][horizon_name]\n",
    "            tree_model_component = _get_model_instance(LGBMRegressor, params=params_tree)\n",
    "\n",
    "            model = StackingEnsemble(\n",
    "                base_model_linear=_get_model_instance(ElasticNetCV),\n",
    "                base_model_tree=tree_model_component,\n",
    "                meta_learner=_get_model_instance(RidgeCV),\n",
    "            )\n",
    "            model.fit({\"linear\": X_train_linear, \"tree\": X_train_tree}, y_train_horizon)\n",
    "            y_pred_test = model.predict({\"linear\": X_test_linear, \"tree\": X_test_tree})\n",
    "\n",
    "        horizon_predictions[horizon_name] = y_pred_test\n",
    "\n",
    "    final_test_results[model_name] = horizon_predictions\n",
    "\n",
    "# --- 4. Calculate and Display Metrics ---\n",
    "\n",
    "results_summary = []\n",
    "for model_name, horizon_preds in final_test_results.items():\n",
    "    avg_metrics = {\"r2\": [], \"rmse\": [], \"mae\": [], \"mape\": []}\n",
    "    for horizon_name, y_pred in horizon_preds.items():\n",
    "        y_true = y_test[f\"target_temp_{horizon_name}\"]\n",
    "        avg_metrics[\"r2\"].append(r2_score(y_true, y_pred))\n",
    "        avg_metrics[\"rmse\"].append(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "        avg_metrics[\"mae\"].append(mean_absolute_error(y_true, y_pred))\n",
    "        avg_metrics[\"mape\"].append(_calculate_mape(y_true.values, y_pred))\n",
    "\n",
    "    results_summary.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Avg R²\": np.mean(avg_metrics[\"r2\"]),\n",
    "        \"Avg RMSE\": np.mean(avg_metrics[\"rmse\"]),\n",
    "        \"Avg MAE\": np.mean(avg_metrics[\"mae\"]),\n",
    "        \"Avg MAPE (%)\": np.mean(avg_metrics[\"mape\"]),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(results_summary).sort_values(\"Avg R²\", ascending=False)\n",
    "\n",
    "# Display using a simple Rich table\n",
    "summary_table = Table(\n",
    "    title=\"[bold]Final Model Performance on Held-Out Test Set[/bold]\", show_header=True, header_style=\"bold blue\"\n",
    ")\n",
    "summary_table.add_column(\"Model\", style=\"cyan\")\n",
    "summary_table.add_column(\"Avg R² ↑\", style=\"green\")\n",
    "summary_table.add_column(\"Avg RMSE ↓\", style=\"yellow\")\n",
    "summary_table.add_column(\"Avg MAE ↓\", style=\"yellow\")\n",
    "summary_table.add_column(\"Avg MAPE (%) ↓\", style=\"yellow\")\n",
    "\n",
    "for _, row in df_summary.iterrows():\n",
    "    summary_table.add_row(\n",
    "        row[\"Model\"],\n",
    "        f\"{row['Avg R²']:.4f}\",\n",
    "        f\"{row['Avg RMSE']:.4f}\",\n",
    "        f\"{row['Avg MAE']:.4f}\",\n",
    "        f\"{row['Avg MAPE (%)']:.2f}%\",\n",
    "    )\n",
    "\n",
    "console.print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d2727",
   "metadata": {},
   "source": [
    "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       </span><span style=\"font-weight: bold; font-style: italic\">Final Model Performance on Held-Out Test Set</span><span style=\"font-style: italic\">                       </span>\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
    "┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Model                             </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Avg R² ↑ </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Avg RMSE ↓ </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Avg MAE ↓ </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Avg MAPE (%) ↓ </span>┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> Avg_Ridge_TunedLGBM               </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.6198   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9660     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7695    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.69%          </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> Stack_ElasticNet_LGBM_Ridge_Tuned </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.6175   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9687     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7715    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.69%          </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> LinearRegression                  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.6137   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9740     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7746    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.71%          </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> RidgeCV                           </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.6137   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9740     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7746    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.71%          </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> Tuned_LGBM                        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.6031   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9871     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7888    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.74%          </span>│\n",
    "│<span style=\"color: #008080; text-decoration-color: #008080\"> Tuned_CatBoost                    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> 0.5958   </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.9955     </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.7974    </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.77%          </span>│\n",
    "└───────────────────────────────────┴──────────┴────────────┴───────────┴────────────────┘\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Final Test Set Performance Comparison\n",
    "\n",
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Make sure all our custom classes and helper functions are available\n",
    "# Ensure this path is correct relative to your sandbox notebook\n",
    "from utils.training import StackingEnsemble, _calculate_mape\n",
    "from utils.evaluations import _get_model_instance\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --- 2. Configuration for this Experiment ---\n",
    "\n",
    "# This dictionary is your control panel for this specific test.\n",
    "# It compares models tuned with a standard mean objective vs. our weighted objective.\n",
    "models_to_evaluate = {\n",
    "    # CatBoost Single Models\n",
    "    \"CatBoost_v1_mean\": {\n",
    "        \"type\": \"single_tuned\",\n",
    "        \"tree_model_class\": CatBoostRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_CatBoostRegressor_v1.pkl\",\n",
    "    },\n",
    "    \"CatBoost_v2_weighted\": {\n",
    "        \"type\": \"single_tuned\",\n",
    "        \"tree_model_class\": CatBoostRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_CatBoostRegressor_v2.pkl\",\n",
    "    },\n",
    "    # LGBM Single Models\n",
    "    \"LGBM_v1_mean\": {\n",
    "        \"type\": \"single_tuned\",\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v1.pkl\",\n",
    "    },\n",
    "    \"LGBM_v2_weighted\": {\n",
    "        \"type\": \"single_tuned\",\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    },\n",
    "    # Simple Averaging Ensembles\n",
    "    \"Avg_Ridge_LGBM_v1_mean\": {\n",
    "        \"type\": \"simple_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v1.pkl\",\n",
    "    },\n",
    "    \"Avg_Ridge_LGBM_v2_weighted\": {\n",
    "        \"type\": \"simple_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    },\n",
    "    # Single models\n",
    "    \"RidgeCV\": {\n",
    "        \"type\": \"oob\",\n",
    "        \"model_class\": RidgeCV,\n",
    "        \"feature_set\": \"linear\",\n",
    "    },\n",
    "    \"ElasticNetCV\": {\n",
    "        \"type\": \"oob\",\n",
    "        \"model_class\": ElasticNetCV,\n",
    "        \"feature_set\": \"linear\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 3. Final Evaluation Loop ---\n",
    "\n",
    "console = Console()\n",
    "final_test_results = {}\n",
    "\n",
    "for model_name, model_config in tqdm(models_to_evaluate.items(), desc=\"Final Test Set Evaluation\"):\n",
    "    horizon_predictions = {}\n",
    "    model_type = model_config.get(\"type\")\n",
    "\n",
    "    if model_type != \"oob\":\n",
    "        # Load parameters once per model\n",
    "        params_path = model_config.get(\"params_path\")\n",
    "        if not params_path or not os.path.exists(params_path):\n",
    "            print(f\"Skipping {model_name}: Param file not found at {params_path}\")\n",
    "            continue\n",
    "        loaded_params = joblib.load(params_path)\n",
    "\n",
    "    for h in tqdm(config.HORIZONS, desc=f\"Training {model_name}\", leave=False):\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        y_train_horizon = y_train[f\"target_temp_{horizon_name}\"]\n",
    "\n",
    "        params_for_horizon = loaded_params[horizon_name]\n",
    "\n",
    "        # --- Model-specific training on 100% of the training data ---\n",
    "\n",
    "        if model_type == \"single_tuned\":\n",
    "            model = _get_model_instance(model_config[\"tree_model_class\"], params=params_for_horizon)\n",
    "            model.fit(X_train_tree, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_tree)\n",
    "\n",
    "        elif model_type == \"simple_averaging_tuned\":\n",
    "            linear_base = _get_model_instance(model_config[\"linear_model_class\"]).fit(X_train_linear, y_train_horizon)\n",
    "            tree_base = _get_model_instance(model_config[\"tree_model_class\"], params=params_for_horizon).fit(\n",
    "                X_train_tree, y_train_horizon\n",
    "            )\n",
    "\n",
    "            pred_linear = linear_base.predict(X_test_linear)\n",
    "            pred_tree = tree_base.predict(X_test_tree)\n",
    "            y_pred_test = (pred_linear + pred_tree) / 2.0\n",
    "\n",
    "        elif model_name == \"RidgeCV\":\n",
    "            model = _get_model_instance(RidgeCV).fit(X_train_linear, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_linear)\n",
    "\n",
    "        elif model_name == \"ElasticNetCV\":\n",
    "            model = _get_model_instance(ElasticNetCV).fit(X_train_linear, y_train_horizon)\n",
    "            y_pred_test = model.predict(X_test_linear)\n",
    "\n",
    "        horizon_predictions[horizon_name] = y_pred_test\n",
    "\n",
    "    final_test_results[model_name] = horizon_predictions\n",
    "\n",
    "# --- 4. Calculate and Display Metrics (Enhanced Table) ---\n",
    "\n",
    "results_summary = []\n",
    "for model_name, horizon_preds in final_test_results.items():\n",
    "    per_horizon_r2 = {}\n",
    "    avg_metrics = {\"r2\": [], \"rmse\": [], \"mae\": [], \"mape\": []}\n",
    "\n",
    "    for h in config.HORIZONS:\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        y_pred = horizon_preds[horizon_name]\n",
    "        y_true = y_test[f\"target_temp_{horizon_name}\"]\n",
    "\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        per_horizon_r2[f\"R² (t+{h})\"] = r2\n",
    "\n",
    "        avg_metrics[\"r2\"].append(r2)\n",
    "        avg_metrics[\"rmse\"].append(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "        avg_metrics[\"mae\"].append(mean_absolute_error(y_true, y_pred))\n",
    "        avg_metrics[\"mape\"].append(_calculate_mape(y_true.values, y_pred))\n",
    "\n",
    "    summary_row = {\n",
    "        \"Model\": model_name,\n",
    "        \"Avg R²\": np.mean(avg_metrics[\"r2\"]),\n",
    "        \"Avg RMSE\": np.mean(avg_metrics[\"rmse\"]),\n",
    "        \"Avg MAE\": np.mean(avg_metrics[\"mae\"]),\n",
    "        \"Avg MAPE (%)\": np.mean(avg_metrics[\"mape\"]),\n",
    "        **per_horizon_r2,  # Unpack the per-horizon R² scores into the dictionary\n",
    "    }\n",
    "    results_summary.append(summary_row)\n",
    "\n",
    "df_summary = pd.DataFrame(results_summary).sort_values(\"Avg R²\", ascending=False)\n",
    "\n",
    "# Display using a Rich table with per-horizon metrics\n",
    "summary_table = Table(\n",
    "    title=\"[bold]Test Set Performance: Weighted CV vs. Mean CV Tuning Objective[/bold]\",\n",
    "    show_header=True,\n",
    "    header_style=\"bold blue\",\n",
    "    show_lines=True,\n",
    ")\n",
    "summary_table.add_column(\"Model\", style=\"cyan\", width=30)\n",
    "summary_table.add_column(\"Avg R² ↑\", style=\"bold green\", justify=\"center\")\n",
    "\n",
    "# Add per-horizon R² columns\n",
    "for h in config.HORIZONS:\n",
    "    summary_table.add_column(f\"R² (t+{h})\", justify=\"center\")\n",
    "\n",
    "# Add other average metrics\n",
    "summary_table.add_column(\"Avg RMSE ↓\", style=\"yellow\", justify=\"center\")\n",
    "summary_table.add_column(\"Avg MAE ↓\", style=\"yellow\", justify=\"center\")\n",
    "\n",
    "for _, row in df_summary.iterrows():\n",
    "    table_row_data = [\n",
    "        row[\"Model\"],\n",
    "        f\"{row['Avg R²']:.4f}\",\n",
    "        f\"{row['R² (t+1)']:.4f}\",\n",
    "        f\"{row['R² (t+2)']:.4f}\",\n",
    "        f\"{row['R² (t+3)']:.4f}\",\n",
    "        f\"{row['R² (t+4)']:.4f}\",\n",
    "        f\"{row['R² (t+5)']:.4f}\",\n",
    "        f\"{row['Avg RMSE']:.4f}\",\n",
    "        f\"{row['Avg MAE']:.4f}\",\n",
    "    ]\n",
    "    summary_table.add_row(*table_row_data)\n",
    "\n",
    "console.print(summary_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce682cd",
   "metadata": {},
   "source": [
    "#### **FLAML test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - AutoML Comparison: FLAML\n",
    "\n",
    "# --- 1. Imports ---\n",
    "from flaml import AutoML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "from utils.evaluations import _calculate_mape\n",
    "\n",
    "# --- 2. Reusable AutoML Evaluation Function (for both FLAML & AutoGluon) ---\n",
    "\n",
    "\n",
    "def evaluate_and_display_automl(\n",
    "    predictor_dict: dict,\n",
    "    model_family_name: str,\n",
    "    X_test_linear: pd.DataFrame,\n",
    "    X_test_tree: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    console: Console,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of trained per-horizon AutoML predictors, evaluates them on the\n",
    "    test set, and displays the results in our standard rich table format.\n",
    "    \"\"\"\n",
    "    final_test_results = {}\n",
    "\n",
    "    # --- A. Generate Predictions ---\n",
    "    for horizon_name, predictor in predictor_dict.items():\n",
    "        # AutoML libraries typically use the tree-based feature set\n",
    "        y_pred_test = predictor.predict(X_test_tree)\n",
    "        final_test_results[horizon_name] = y_pred_test\n",
    "\n",
    "    # --- B. Calculate Metrics ---\n",
    "    results_summary = []\n",
    "    per_horizon_r2 = {}\n",
    "    avg_metrics = {\"r2\": [], \"rmse\": [], \"mae\": [], \"mape\": []}\n",
    "\n",
    "    for h in config.HORIZONS:\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        y_pred = final_test_results[horizon_name]\n",
    "        y_true = y_test[f\"target_temp_{horizon_name}\"]\n",
    "\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        per_horizon_r2[f\"R² ({horizon_name})\"] = r2\n",
    "        avg_metrics[\"r2\"].append(r2)\n",
    "        avg_metrics[\"rmse\"].append(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "        avg_metrics[\"mae\"].append(mean_absolute_error(y_true, y_pred))\n",
    "        avg_metrics[\"mape\"].append(_calculate_mape(y_true.values, y_pred))\n",
    "\n",
    "    summary_row = {\n",
    "        \"Model\": model_family_name,\n",
    "        \"Avg R²\": np.mean(avg_metrics[\"r2\"]),\n",
    "        \"Avg RMSE\": np.mean(avg_metrics[\"rmse\"]),\n",
    "        \"Avg MAE\": np.mean(avg_metrics[\"mae\"]),\n",
    "        \"Avg MAPE (%)\": np.mean(avg_metrics[\"mape\"]),\n",
    "        **per_horizon_r2,\n",
    "    }\n",
    "    results_summary.append(summary_row)\n",
    "    df_summary = pd.DataFrame(results_summary)\n",
    "\n",
    "    # --- C. Display Table ---\n",
    "    summary_table = Table(\n",
    "        title=f\"[bold]Test Set Performance: {model_family_name}[/bold]\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold blue\",\n",
    "        show_lines=True,\n",
    "    )\n",
    "    summary_table.add_column(\"Model\", style=\"cyan\", width=30)\n",
    "    summary_table.add_column(\"Avg R² ↑\", style=\"bold green\", justify=\"center\")\n",
    "    for h in config.HORIZONS:\n",
    "        summary_table.add_column(f\"R² (t+{h})\", justify=\"center\")\n",
    "    summary_table.add_column(\"Avg RMSE ↓\", style=\"yellow\", justify=\"center\")\n",
    "    summary_table.add_column(\"Avg MAE ↓\", style=\"yellow\", justify=\"center\")\n",
    "\n",
    "    for _, row in df_summary.iterrows():\n",
    "        table_row_data = [\n",
    "            row[\"Model\"],\n",
    "            f\"{row['Avg R²']:.4f}\",\n",
    "            f\"{row['R² (t+1)']:.4f}\",\n",
    "            f\"{row['R² (t+2)']:.4f}\",\n",
    "            f\"{row['R² (t+3)']:.4f}\",\n",
    "            f\"{row['R² (t+4)']:.4f}\",\n",
    "            f\"{row['R² (t+5)']:.4f}\",\n",
    "            f\"{row['Avg RMSE']:.4f}\",\n",
    "            f\"{row['Avg MAE']:.4f}\",\n",
    "        ]\n",
    "        summary_table.add_row(*table_row_data)\n",
    "\n",
    "    console.print(summary_table)\n",
    "\n",
    "    return final_test_results  # Return predictions for potential further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc7ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. FLAML Experiment ---\n",
    "\n",
    "console = Console()\n",
    "flaml_predictors = {}\n",
    "TIME_BUDGET_PER_HORIZON = 600\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"  STARTING: FLAML Evaluation (Budget: {TIME_BUDGET_PER_HORIZON}s per horizon)\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for h in tqdm(config.HORIZONS, desc=\"FLAML Horizon Evaluation\"):\n",
    "    horizon_name = f\"t+{h}\"\n",
    "    y_train_horizon = y_train[f\"target_temp_{horizon_name}\"]\n",
    "\n",
    "    print(f\"\\n--- Training FLAML for {horizon_name} ---\")\n",
    "\n",
    "    # Initialize AutoML for this horizon\n",
    "    automl = AutoML()\n",
    "\n",
    "    # Define settings\n",
    "    automl_settings = {\n",
    "        \"time_budget\": TIME_BUDGET_PER_HORIZON,\n",
    "        \"metric\": \"r2\",\n",
    "        \"task\": \"regression\",\n",
    "        \"log_file_name\": f\"experiments/flaml_{horizon_name}.log\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"seed\": config.GLOBAL_RANDOM_SEED,\n",
    "        # \"estimator_list\": [\"lgbm\", \"xgboost\", \"catboost\", \"rf\", \"extra_tree\"],  # Focus on tree models\n",
    "    }\n",
    "\n",
    "    # Train FLAML on the full training data for this horizon\n",
    "    automl.fit(X_train=X_train_tree, y_train=y_train_horizon, **automl_settings)\n",
    "\n",
    "    # Store the trained predictor\n",
    "    flaml_predictors[horizon_name] = automl\n",
    "\n",
    "    print(f\"--- Best model for {horizon_name}: {automl.best_estimator} ---\")\n",
    "    print(f\"Best R2 score on internal validation: {automl.best_result['val_loss']:.4f}\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(f\"\\n--- FLAML Training Complete. Total time: {(total_end_time - total_start_time) / 60:.2f} minutes ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Evaluate and Display FLAML's Test Set Performance ---\n",
    "flaml_test_predictions = evaluate_and_display_automl(\n",
    "    predictor_dict=flaml_predictors,\n",
    "    model_family_name=f\"FLAML (Budget: {TIME_BUDGET_PER_HORIZON}s/horizon)\",\n",
    "    X_test_linear=X_test_linear,  # Not used by FLAML but required by function signature\n",
    "    X_test_tree=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    console=console,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb9547",
   "metadata": {},
   "source": [
    "#### **Autogluon test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d23d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - AutoML Comparison: AutoGluon\n",
    "\n",
    "# --- 1. Imports ---\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- 2. AutoGluon Experiment ---\n",
    "\n",
    "console = Console()\n",
    "autogluon_predictors = {}\n",
    "TIME_LIMIT_PER_HORIZON = 1000\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"  STARTING: AutoGluon Evaluation (Budget: {TIME_LIMIT_PER_HORIZON}s per horizon)\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "# AutoGluon requires the target column to be part of the training DataFrame.\n",
    "# We will create temporary DataFrames for each horizon.\n",
    "train_data_ag = X_train_tree.copy()\n",
    "for col in y_train.columns:\n",
    "    train_data_ag[col] = y_train[col]\n",
    "\n",
    "\n",
    "for h in tqdm(config.HORIZONS, desc=\"AutoGluon Horizon Evaluation\"):\n",
    "    horizon_name = f\"t+{h}\"\n",
    "    target_col = f\"target_temp_{horizon_name}\"\n",
    "\n",
    "    # Define a unique path for this predictor's artifacts\n",
    "    predictor_path = f\"experiments/autogluon/h{h}\"\n",
    "    os.makedirs(predictor_path, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n--- Training AutoGluon for {horizon_name} ---\")\n",
    "\n",
    "    # Initialize TabularPredictor for this horizon\n",
    "    predictor = TabularPredictor(\n",
    "        label=target_col,\n",
    "        path=predictor_path,\n",
    "        eval_metric=\"r2\",\n",
    "    )\n",
    "\n",
    "    # Train the predictor on the full training data\n",
    "    # We drop the other target columns to prevent data leakage\n",
    "    predictor.fit(\n",
    "        train_data=train_data_ag.drop(columns=[c for c in target_cols if c != target_col]),\n",
    "        time_limit=TIME_LIMIT_PER_HORIZON,\n",
    "        presets=\"best_quality\",\n",
    "        # Exclude models that are known to be slow or less effective for this data type\n",
    "        excluded_model_types=[\"FASTAI\", \"KNN\"],\n",
    "    )\n",
    "\n",
    "    # Store the trained predictor object\n",
    "    autogluon_predictors[horizon_name] = predictor\n",
    "\n",
    "    print(f\"--- AutoGluon training for {horizon_name} complete. ---\")\n",
    "    # You can view the detailed leaderboard for this horizon's predictor:\n",
    "    predictor.leaderboard(train_data_ag, silent=True)\n",
    "\n",
    "\n",
    "total_end_time = time.time()\n",
    "print(f\"\\n--- AutoGluon Training Complete. Total time: {(total_end_time - total_start_time) / 60:.2f} minutes ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44fa029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Evaluate and Display AutoGluon's Test Set Performance ---\n",
    "autogluon_test_predictions = evaluate_and_display_automl(\n",
    "    predictor_dict=autogluon_predictors,\n",
    "    model_family_name=f\"AutoGluon (Preset: 'best_quality', Budget: {TIME_LIMIT_PER_HORIZON}s/horizon)\",\n",
    "    X_test_linear=X_test_linear,  # Not used, but required by function signature\n",
    "    X_test_tree=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    console=console,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7c597",
   "metadata": {},
   "source": [
    "#### **Check extrapolation limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Diagnostic: Extrapolation Failure\n",
    "\n",
    "# --- 1. Imports ---\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from lightgbm import LGBMRegressor\n",
    "import utils.analysis_helpers as ah\n",
    "\n",
    "# --- 2. Train Diagnostic Models ---\n",
    "# We train on 100% of the training data for the t+1 horizon, as this has the strongest signal.\n",
    "console.print(\"[bold]Training diagnostic models on full training set for t+1 horizon...[/bold]\")\n",
    "\n",
    "# Train RidgeCV\n",
    "model_ridge_diag = RidgeCV(cv=TimeSeriesSplit(n_splits=5)).fit(X_train_linear, y_train[\"target_temp_t+1\"])\n",
    "\n",
    "# Train OOB LGBM\n",
    "model_lgbm_diag = LGBMRegressor(random_state=config.GLOBAL_RANDOM_SEED, n_jobs=-1, verbosity=-1).fit(\n",
    "    X_train_tree, y_train[\"target_temp_t+1\"]\n",
    ")\n",
    "\n",
    "# --- 3. Generate Predictions on the Test Set ---\n",
    "y_pred_ridge_test = pd.Series(model_ridge_diag.predict(X_test_linear), index=X_test_linear.index)\n",
    "y_pred_lgbm_test = pd.Series(model_lgbm_diag.predict(X_test_tree), index=X_test_tree.index)\n",
    "\n",
    "# --- 4. Display the Quantitative Summary ---\n",
    "ah.display_extrapolation_summary(\n",
    "    y_train=y_train[\"target_temp_t+1\"],\n",
    "    y_test=y_test[\"target_temp_t+1\"],\n",
    "    y_pred_linear=y_pred_ridge_test,\n",
    "    y_pred_tree=y_pred_lgbm_test,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_diag = ah.plot_extrapolation_diagnostics(\n",
    "    y_train=y_train[\"target_temp_t+1\"],\n",
    "    y_test=y_test[\"target_temp_t+1\"],\n",
    "    y_pred_linear=y_pred_ridge_test,\n",
    "    y_pred_tree=y_pred_lgbm_test,\n",
    ")\n",
    "\n",
    "fig_diag.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95b38f",
   "metadata": {},
   "source": [
    "#### **Horizon-weighted average ensembles training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Final Refinement: Horizon-Weighted Averaging\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# We will test our best linear and best tuned tree models\n",
    "models_to_evaluate_weighted = {\n",
    "    \"WeightedAvg_Ridge_TunedLGBM\": {\n",
    "        \"type\": \"weighted_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"./experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    },\n",
    "    # \"WeightedAvg_Ridge_TunedCatBoost\": {\n",
    "    #     \"type\": \"weighted_averaging_tuned\",\n",
    "    #     \"linear_model_class\": RidgeCV,\n",
    "    #     \"tree_model_class\": CatBoostRegressor,\n",
    "    #     \"params_path\": \"experiments/tuned_params_final_CatBoostRegressor.pkl\",\n",
    "    # },\n",
    "}\n",
    "\n",
    "# --- 2. Run the Evaluation ---\n",
    "print(\"\\n--- Searching for Optimal Ensemble Weights ---\")\n",
    "weighted_avg_results = eval.evaluate_models(\n",
    "    models_to_evaluate=models_to_evaluate_weighted,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(weighted_avg_results, \"experiments/evaluation_results_tuned_ensemble_weighted_avg.pkl\")\n",
    "print(\"Weighted average ensemble results saved to 'experiments/evaluation_results_tuned_ensemble_weighted_avg.pkl'\")\n",
    "\n",
    "# --- 3. Analyze the Optimal Weights Found ---\n",
    "# This part can be run in the presentation notebook later, but it's good to check here.\n",
    "console = Console()\n",
    "weights_table = Table(title=\"[bold]Optimal Blending Weight for Linear Model (RidgeCV)[/bold]\")\n",
    "weights_table.add_column(\"Horizon\", style=\"cyan\")\n",
    "weights_table.add_column(\"Avg. Optimal Weight\", justify=\"center\", style=\"green\")\n",
    "\n",
    "for model_name, model_results in weighted_avg_results.items():\n",
    "    for h in config.HORIZONS:\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        avg_weight = np.mean(model_results[horizon_name][\"scores\"][\"optimal_weights\"])\n",
    "        weights_table.add_row(f\"({model_name}) {horizon_name}\", f\"{avg_weight:.2f}\")\n",
    "\n",
    "console.print(weights_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46c7df",
   "metadata": {},
   "source": [
    "#### **Top 2 models snapshot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f774d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSTIC_ARTIFACT_PATH = \"experiments/chosen_models_diagnostic_data.pkl\"\n",
    "\n",
    "# Define our two champion models for final diagnostics\n",
    "final_models_to_diagnose = {\n",
    "    \"WeightedAvg_Ridge_TunedLGBM\": {\n",
    "        \"type\": \"weighted_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "        \"horizon_weights\": {1: 0.40, 2: 0.39, 3: 0.53, 4: 0.50, 5: 0.58},\n",
    "    },\n",
    "    \"RidgeCV\": {\n",
    "        \"type\": \"oob\",\n",
    "        \"model_class\": RidgeCV,\n",
    "        \"feature_set\": \"linear\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d116e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Generate or Load the Data ---\n",
    "# This function will either run the full CV process or load from the cache.\n",
    "diagnostic_data = diag.generate_diagnostic_data(\n",
    "    models_to_run=final_models_to_diagnose,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons_to_analyze=config.HORIZONS,\n",
    "    artifact_path=DIAGNOSTIC_ARTIFACT_PATH,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4bf5e",
   "metadata": {},
   "source": [
    "#### **Generate feature permuation importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_VS_PERM_ARTIFACT_PATH = \"assets/saved_results/shap_vs_perm_data.pkl\"\n",
    "\n",
    "champion_model_config = {\n",
    "    \"WeightedAvg_Ridge_TunedLGBM\": {\n",
    "        \"type\": \"weighted_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "        \"horizon_weights\": {1: 0.40, 2: 0.39, 3: 0.53, 4: 0.50, 5: 0.58},\n",
    "    }\n",
    "}\n",
    "# Extract the single config dictionary needed by the function\n",
    "champion_config_for_function = list(champion_model_config.values())[0]\n",
    "\n",
    "\n",
    "# --- 2. Generate or Load the Data ---\n",
    "# This function will either run the full CV and SHAP process or load from the cache.\n",
    "shap_vs_perm_data = diag.generate_shap_vs_perm_data(\n",
    "    champion_model_config=champion_config_for_function,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons_to_analyze=[1, 2, 3, 4, 5],\n",
    "    artifact_path=SHAP_VS_PERM_ARTIFACT_PATH,\n",
    "    n_repeats=20,\n",
    ")\n",
    "\n",
    "print(\"\\n--- SHAP vs. Permutation Data Generation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a743b",
   "metadata": {},
   "source": [
    "#### **Test enhancements and all**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox_notebook.ipynb - Final Enhancement Experiments\n",
    "\n",
    "# --- 1. Base Champion Config ---\n",
    "base_champion_config = {\n",
    "    \"type\": \"weighted_averaging_tuned\",\n",
    "    \"linear_model_class\": RidgeCV,\n",
    "    \"tree_model_class\": LGBMRegressor,\n",
    "    \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    \"horizon_weights\": {1: 0.40, 2: 0.39, 3: 0.53, 4: 0.50, 5: 0.58},\n",
    "}\n",
    "\n",
    "# --- 2. Enhancement Experiment Configurations ---\n",
    "enhancement_configs_to_run = {\n",
    "    \"Ensemble + Isotonic Calibration\": {\n",
    "        \"type\": \"calibration\",\n",
    "    },\n",
    "    \"Ensemble + Residual Stacking (Tuned LGBM)\": {\n",
    "        \"type\": \"residual_stacking\",\n",
    "        \"error_model_class\": LGBMRegressor,\n",
    "        \"error_model_feature_set\": \"tree\",\n",
    "        # CRITICAL: Use the best tuned params for the error model\n",
    "        \"error_model_params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    },\n",
    "    \"Ensemble + Residual Stacking (Ridge)\": {\n",
    "        \"type\": \"residual_stacking\",\n",
    "        \"error_model_class\": RidgeCV,\n",
    "        \"error_model_feature_set\": \"linear\",\n",
    "    },\n",
    "    # NEW: The combined \"kitchen sink\" approach\n",
    "    \"Ensemble + Residual Stacking (Tuned LGBM) + Calibration\": {\n",
    "        \"type\": \"residual_stacking_calibration\",\n",
    "        \"error_model_class\": LGBMRegressor,\n",
    "        \"error_model_feature_set\": \"tree\",\n",
    "        \"error_model_params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 3. Run the Evaluation ---\n",
    "print(\"\\n--- Running Final Evaluation of Ensemble Model Enhancements ---\")\n",
    "enhancement_results = eval.evaluate_enhancements(\n",
    "    base_model_config=base_champion_config,\n",
    "    enhancement_configs=enhancement_configs_to_run,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    cv_splitter=tscv_splitter,\n",
    "    horizons=config.HORIZONS,\n",
    ")\n",
    "joblib.dump(enhancement_results, \"experiments/evaluation_results_ensemble_enhancements.pkl\")\n",
    "print(\"Final enhancement evaluation results saved to 'experiments/evaluation_results_ensemble_enhancements.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847d4ce",
   "metadata": {},
   "source": [
    "#### **Test set, FINALLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a6ef8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Training final models on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">% of training data</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTraining final models on \u001b[0m\u001b[1;36m100\u001b[0m\u001b[1m% of training data\u001b[0m\u001b[1;33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd1f71d90c4134a1ae94691fb2d2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Final Models:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">All final models trained and saved to </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'assets/models'</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mAll final models trained and saved to \u001b[0m\u001b[1;32m'assets/models'\u001b[0m\u001b[1;32m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_models_to_train = {\n",
    "    \"WeightedAvg_Ridge_TunedLGBM\": {\n",
    "        \"type\": \"weighted_averaging_tuned\",\n",
    "        \"linear_model_class\": RidgeCV,\n",
    "        \"tree_model_class\": LGBMRegressor,\n",
    "        \"params_path\": \"experiments/tuned_params_LGBMRegressor_v3.pkl\",\n",
    "        \"horizon_weights\": {1: 0.40, 2: 0.39, 3: 0.53, 4: 0.50, 5: 0.58},\n",
    "    },\n",
    "    \"RidgeCV\": {\n",
    "        \"type\": \"oob\",\n",
    "        \"model_class\": RidgeCV,\n",
    "        \"feature_set\": \"linear\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- 2. Run Final Training ---\n",
    "train.train_and_save_final_models(\n",
    "    models_to_train=final_models_to_train,\n",
    "    X_train_linear=X_train_linear,\n",
    "    X_train_tree=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    horizons=config.HORIZONS,\n",
    "    save_dir=\"assets/models\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d1cee",
   "metadata": {},
   "source": [
    "#### **Lagged rolling average benchmarks...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db6030c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "\n",
    "def test_lagged_average_baseline(\n",
    "    y_true_all_horizons: pd.DataFrame, full_temp_series: pd.Series, horizons: list, windows: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the R² score for a time-lagged rolling average baseline\n",
    "    on a test set for multiple horizons and window sizes, ensuring no data leakage.\n",
    "\n",
    "    Args:\n",
    "        y_true_all_horizons (pd.DataFrame): DataFrame containing the true target values\n",
    "                                            for all horizons on the test set.\n",
    "        full_temp_series (pd.Series): The complete, original 'temp' series spanning\n",
    "                                      both training and test periods.\n",
    "        horizons (list): A list of integer horizons to test (e.g., [1, 5]).\n",
    "        windows (list): A list of integer window sizes for the rolling average (e.g., [7, 30]).\n",
    "    \"\"\"\n",
    "    console = Console()\n",
    "    results_table = Table(\n",
    "        title=\"[bold]Time-Lagged Average Baseline Performance (R² on Test Set)[/bold]\",\n",
    "        show_header=True,\n",
    "        header_style=\"bold magenta\",\n",
    "    )\n",
    "    results_table.add_column(\"Horizon\", style=\"cyan\")\n",
    "    for w in windows:\n",
    "        results_table.add_column(f\"Window = {w} days\", justify=\"center\")\n",
    "\n",
    "    # The test set starts on the first index of our y_true DataFrame\n",
    "    test_start_date = y_true_all_horizons.index.min()\n",
    "\n",
    "    # To prevent any leakage, we only use data *before* the test set begins\n",
    "    # to calculate the rolling means that will be used for predictions.\n",
    "    temp_series_for_rolling = full_temp_series.loc[: test_start_date - pd.Timedelta(days=1)]\n",
    "\n",
    "    for h in horizons:\n",
    "        horizon_name = f\"t+{h}\"\n",
    "        y_true_horizon = y_true_all_horizons[f\"target_temp_{horizon_name}\"]\n",
    "        row_scores = [horizon_name]\n",
    "\n",
    "        for w in windows:\n",
    "            # --- CRITICAL LEAKAGE PREVENTION ---\n",
    "            # 1. Calculate the rolling mean on historical data ONLY.\n",
    "            rolling_mean = temp_series_for_rolling.rolling(window=w).mean()\n",
    "\n",
    "            # 2. To predict for a day `d` in the test set, we need the rolling average\n",
    "            #    calculated up to day `d-h`. We achieve this by shifting the\n",
    "            #    historical rolling mean series forward by `h` days.\n",
    "            #    This correctly simulates that for a t+5 forecast, we only have data\n",
    "            #    available up to day t.\n",
    "\n",
    "            # Create a shifted index to align predictions with the test set dates\n",
    "            shifted_index = rolling_mean.index + pd.Timedelta(days=h)\n",
    "\n",
    "            # Create the prediction series with the correctly shifted index\n",
    "            y_pred_series = pd.Series(rolling_mean.values, index=shifted_index)\n",
    "\n",
    "            # 3. Align the predictions with the true values for this horizon\n",
    "            y_pred_aligned = y_pred_series.reindex(y_true_horizon.index).ffill().bfill()\n",
    "\n",
    "            # --- Calculate Score ---\n",
    "            score = r2_score(y_true_horizon, y_pred_aligned)\n",
    "            row_scores.append(f\"{score:.4f}\")\n",
    "\n",
    "        results_table.add_row(*row_scores)\n",
    "\n",
    "    console.print(results_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "576e696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                             </span><span style=\"font-weight: bold; font-style: italic\">Time-Lagged Average Baseline Performance (R² on Test Set)</span><span style=\"font-style: italic\">                             </span>\n",
       "┏━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">  Window = 1  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">  Window = 3  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">  Window = 7  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Window = 14  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Window = 28  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Window = 30 </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Window = 365 </span>┃\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Horizon </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    days     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">     days     </span>┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> t+1     </span>│   -0.0002    │   -0.0147    │   -0.2848    │   -0.0703    │   -0.0098    │   -0.0091   │   -0.0022    │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> t+2     </span>│   -0.0002    │   -0.0152    │   -0.2869    │   -0.0716    │   -0.0103    │   -0.0097   │   -0.0020    │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> t+3     </span>│    0.0002    │   -0.0152    │   -0.2891    │   -0.0729    │   -0.0109    │   -0.0103   │   -0.0017    │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> t+4     </span>│    0.0008    │   -0.0153    │   -0.2920    │   -0.0745    │   -0.0115    │   -0.0110   │   -0.0015    │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> t+5     </span>│    0.0022    │   -0.0148    │   -0.2950    │   -0.0764    │   -0.0125    │   -0.0119   │   -0.0013    │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                             \u001b[0m\u001b[1;3mTime-Lagged Average Baseline Performance (R² on Test Set)\u001b[0m\u001b[3m                             \u001b[0m\n",
       "┏━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m         \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m Window = 1 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m Window = 3 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m Window = 7 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mWindow = 14 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mWindow = 28 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mWindow = 30\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mWindow = 365\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35mHorizon\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m   days    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m    days    \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mt+1    \u001b[0m\u001b[36m \u001b[0m│   -0.0002    │   -0.0147    │   -0.2848    │   -0.0703    │   -0.0098    │   -0.0091   │   -0.0022    │\n",
       "│\u001b[36m \u001b[0m\u001b[36mt+2    \u001b[0m\u001b[36m \u001b[0m│   -0.0002    │   -0.0152    │   -0.2869    │   -0.0716    │   -0.0103    │   -0.0097   │   -0.0020    │\n",
       "│\u001b[36m \u001b[0m\u001b[36mt+3    \u001b[0m\u001b[36m \u001b[0m│    0.0002    │   -0.0152    │   -0.2891    │   -0.0729    │   -0.0109    │   -0.0103   │   -0.0017    │\n",
       "│\u001b[36m \u001b[0m\u001b[36mt+4    \u001b[0m\u001b[36m \u001b[0m│    0.0008    │   -0.0153    │   -0.2920    │   -0.0745    │   -0.0115    │   -0.0110   │   -0.0015    │\n",
       "│\u001b[36m \u001b[0m\u001b[36mt+5    \u001b[0m\u001b[36m \u001b[0m│    0.0022    │   -0.0148    │   -0.2950    │   -0.0764    │   -0.0125    │   -0.0119   │   -0.0013    │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_temp_series_from_ready = df_model_ready[\"temp\"]\n",
    "\n",
    "horizons_to_test = [\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "]\n",
    "windows_to_test = [\n",
    "    1,\n",
    "    3,\n",
    "    7,\n",
    "    14,\n",
    "    28,\n",
    "    30,\n",
    "    365,\n",
    "]\n",
    "\n",
    "# Run the test\n",
    "test_lagged_average_baseline(\n",
    "    y_true_all_horizons=y_test,\n",
    "    full_temp_series=full_temp_series_from_ready,\n",
    "    horizons=horizons_to_test,\n",
    "    windows=windows_to_test,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
